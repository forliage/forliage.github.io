---
title: "ads09:贪心算法 (Greedy Algorithms)"
description: ""
pubDate: "2025-08-22"
heroImage: ""
---

# ads09:贪心算法 (Greedy Algorithms)

### **第一部分：贪心算法的核心思想**

在我们深入代码和证明之前，我们首先要理解，什么是贪心算法？它试图解决什么样的问题？

#### **1\. 优化问题 (Optimization Problems)**

我们生活和学习中遇到的很多问题，本质上都是**优化问题**。比如，如何规划回家的路线才能时间最短？如何在预算内选择商品才能价值最大？

一个标准的优化问题通常包含两个核心要素：

*   **约束 (Constraints)**：解决问题时必须满足的条件。例如，你的总花费不能超过预算，你选择的路线必须能从起点到达终点。
*   **优化函数 (Optimization Function)**：一个需要被最大化或最小化的目标。例如，最短的时间，最大的价值。

所有满足约束条件的解，我们称之为**可行解 (Feasible Solutions)**。而在所有可行解中，那个能让优化函数达到最优值（最大或最小）的解，就是我们梦寐以求的**最优解 (Optimal Solution)**。

#### **2\. 贪心方法 (The Greedy Method)**

面对一个复杂的优化问题，我们该如何下手？

贪心算法提供了一种非常直观的思路：**“目光短浅”，只看眼前**。它在每一步决策时，都采取当前状态下看起来最好或最优的选择，而不从整体最优上进行考虑。这个“当前最好”的判断标准，我们称之为**贪心准则 (Greedy Criterion)**。

贪心算法有一个非常关键的特性：**一旦做出选择，绝不反悔**。也就是说，一个阶段做出的决策，在后面的阶段是**不会被改变 (not changed)** 的。这要求我们每一步的选择都必须保证当前解仍然是**可行 (assure feasibility)** 的。

想象一下你在一个岔路口，每条路都标着走到下一个路口的距离。如果你使用贪心策略，你每次都会选择那条距离最短的路，期望最终能最快到达终点。

#### **3\. 贪心算法的“双刃剑”**

现在，请思考一个问题：这种“只看眼前”的策略，一定能得到全局最优解吗？

答案是：**不一定**。

**注意 (Note):**

*   **什么时候贪心算法有效？** 仅当问题的结构特殊，保证了**局部最优 (local optimum)** 的选择能够推导出**全局最优 (global optimum)** 时，贪心算法才能给出正确的解。
*   **什么时候贪心算法无效？** 在大多数情况下，贪心算法**不能保证 (does not guarantee)** 得到最优解。然而，它通常能快速地给出一个**接近最优解的近似解**。这种情况下，贪心算法常作为一种**启发式算法 (heuristics)** 使用。当寻找精确最优解非常耗时（例如指数级时间复杂度）时，贪心算法就显得非常有价值。

总结一下，贪心算法就像一个“急功近利”的决策者，它简单、高效，但在某些问题上会因为缺乏远见而犯错。我们的任务，就是要学会甄别哪些问题可以用贪心解决，并学会证明其正确性。

### **第二部分：经典案例一：活动选择问题 (Activity Selection Problem)**

让我们通过一个经典的例子来感受贪心算法的魅力与挑战。

#### **1\. 问题描述**

假设我们有一个资源，比如一间教室。现在有 $n$ 个活动申请使用这个教室，每个活动 $a\_i$ 都有一个开始时间 $s\_i$ 和一个结束时间 $f\_i$。活动占用的时间区间是左闭右开的 $\[s\_i, f\_i)$。

如果两个活动 $a\_i$ 和 $a\_j$ 的时间区间不重叠，我们就说它们是**兼容的 (compatible)**。数学上讲，即 $s\_i \\ge f\_j$ 或者 $s\_j \\ge f\_i$。

我们的目标是：从这 $n$ 个活动中，选出一个由**互相兼容**的活动组成的最大子集。也就是说，我们要安排尽可能多的活动。

为了方便处理，我们首先假设所有活动已经按照**结束时间的非递减顺序**排好序，即 $f\_1 \\le f\_2 \\le \\dots \\le f\_n$。

**【示例】**

下面是一个活动安排的例子：

活动 $i$

1

2

3

4

5

6

7

8

9

10

11

开始时间 $s\_i$

1

3

0

5

3

5

6

8

8

2

12

结束时间 $f\_i$

4

5

6

7

9

9

10

11

12

14

16

我们可以用图形化来表示这些活动的时间区间：

gantt title 活动时间区间 dateFormat HH axisFormat %H section 活动1 A1: 01, 3h section 活动2 A2: 03, 2h section 活动3 A3: 00, 6h section 活动4 A4: 05, 2h section 活动5 A5: 03, 6h section 活动6 A6: 05, 4h section 活动7 A7: 06, 4h section 活动8 A8: 08, 3h section 活动9 A9: 08, 4h section 活动10 A10: 02, 12h section 活动11 A11: 12, 4h

**思考一下：** 面对这个问题，我们可以有哪些“贪心”的思路？换句话说，我们的贪心准则可以是什么？

#### **2\. 探索不同的贪心策略**

在解决这个问题之前，我们先提一下，这个问题是可以用**动态规划 (Dynamic Programming)** 解决的。

如果我们定义 $S\_{ij}$ 为在活动 $a\_i$ 结束之后开始，且在活动 $a\_j$ 开始之前结束的活动集合。令 $c\_{ij}$ 为集合 $S\_{ij}$ 的最优解的大小。我们可以得到递推式： $$c\_{ij} = \\max\_{a\_k \\in S\_{ij}} {c\_{ik} + c\_{kj} + 1}$$ 如果 $S\_{ij} = \\emptyset$，则 $c\_{ij}=0$。 这个 DP 解法的时间复杂度大约是 $O(N^3)$，经过优化可以达到 $O(N^2)$。

**那么，我们能用更快的贪心算法吗？我们来试试看！**

##### **贪心策略 1：选择开始时间最早的活动**

*   **准则：** 每次都从还未安排的活动中，选择一个开始时间最早的。
*   **分析：** 这个策略看似合理，因为它能让教室尽早被利用。但我们来看一个反例：

gantt title 反例：最早开始 dateFormat X axisFormat %s section 贪心选择 选择的活动 : 0, 10 section 其它活动 未选择1 : 1, 2 未选择2 : 3, 4 未选择3 : 5, 6

在这个例子中，贪心策略会首先选择那个持续时间很长的活动 (0-10)，因为它开始得最早。但这样一来，其他所有活动都无法被安排了。而最优解是选择后面三个短的活动。所以，**策略 1 失败**。

##### **贪心策略 2：选择持续时间最短的活动**

*   **准则：** 每次都从还未安排的活动中，选择一个持续时间最短的。
*   **分析：** 这个策略试图留下更多的时间给其他活动。我们来看一个反例：

gantt title 反例：最短持续 dateFormat X axisFormat %s section 其它活动 未选择1 : 0, 5 未选择2 : 7, 12 section 贪心选择 选择的活动 : 4, 8

在这个例子中，最短的活动是中间那个 (4-8)。如果我们选了它，左右两边的活动就都不能选了，总共只能选1个。但最优解是选择左右两个活动，总共可以选2个。所以，**策略 2 失败**。

##### **贪心策略 3：选择冲突最少的活动**

*   **准则：** 每次都选择与剩余活动冲突最少的那个。
*   **分析：** 这个策略的思路是最大化未来的选择空间。但它的计算量很大，而且同样不是最优的。看下面的反例：

gantt title 反例：最少冲突 dateFormat X axisFormat %s section (d) 活动d1: 3, 5 活动d2: 4, 6 活动d3: 7, 9 活动d4: 8, 10 section (c) 活动c1: 0, 2 活动c2: 12, 14 section (b) 活动b1: 2, 12 section (a) 活动a1: 6, 7

在这个复杂的例子中，中间的活动(b1)可能会与很多活动冲突，但选择它可能导致更多的活动被安排。而那些冲突少的边缘活动，可能并不是最优解的一部分。这个策略也**不保证最优**。

##### **贪心策略 4：选择结束时间最早的活动**

*   **准则：** 每次都从还未安排的活动中，选择一个**结束时间最早**的。
*   **分析：** 这个策略的直觉是什么？**让资源尽快被释放出来 (Resource become free as soon as possible)**，这样就可以给后续的活动留下更多的时间窗口。

我们来模拟一下这个策略：

1.  首先，将所有活动按结束时间升序排序。
2.  选择第一个活动（结束时间最早的那个）加入解集。
3.  从剩下的活动中，剔除掉所有与刚选入的活动冲突的活动。
4.  重复步骤2和3，直到没有活动可选。

我们来试试这个策略：

gantt title 策略4：最早结束 dateFormat X axisFormat %s section A 选择 1: 0, 3 section B (冲突) : 1, 4 section C 选择 2: 4, 6 section D (冲突): 5, 8 section E 选择 3: 7, 9

在这个例子中，我们首先选择 (0, 3)。然后所有在 3 之前结束的、并且与 (0, 3) 冲突的活动都被排除。接下来，在剩下的活动中，我们选择结束时间最早的 (4, 6)。然后排除与它冲突的。最后选择 (7, 9)。这个策略看起来非常有前途！

#### **3\. 正确性证明**

直觉上感觉正确还不够，我们必须用数学来严格证明。证明一个贪心算法的正确性，通常分两步：**证明贪心选择性质**和**证明最优子结构**。

**【定理】** 对于任意非空的活动集合 $S\_k$，令 $a\_m$ 是 $S\_k$ 中结束时间最早的活动。那么 $a\_m$ 必定存在于 $S\_k$ 的某个最大兼容活动子集中。

这就是**贪心选择性质 (Greedy-choice property)**。它说明我们做出的第一个贪心选择，一定是通往最优解的正确一步。

**【证明】**

1.  假设 $A\_k$ 是 $S\_k$ 的一个最大兼容活动子集（即一个最优解）。
2.  令 $a\_j$ 是 $A\_k$ 中结束时间最早的活动。
3.  **情况一：** 如果 $a\_j$ 恰好就是 $a\_m$（我们贪心选择的活动），那么定理得证，我们的贪心选择就在这个最优解里。
4.  **情况二：** 如果 $a\_j \\neq a\_m$。根据我们对 $a\_m$ 的定义，它是所有活动中结束时间最早的，所以 $f\_m \\le f\_j$。
5.  现在，我们构造一个新的解 $A\_k' = (A\_k - {a\_j}) \\cup {a\_m}$。也就是说，我们把 $A\_k$ 中的 $a\_j$ 换成 $a\_m$。
6.  由于 $a\_j$ 是 $A\_k$ 中结束最早的活动，所以 $A\_k$ 中其他所有活动的开始时间都 $\\ge f\_j$。
7.  因为 $f\_m \\le f\_j$，所以 $A\_k$ 中其他所有活动的开始时间也都 $\\ge f\_m$。这意味着 $a\_m$ 与 $A\_k - {a\_j}$ 中的所有活动都是兼容的。
8.  因此，$A\_k'$ 是一个合法的兼容活动子集，并且它的大小和 $A\_k$ 相同（$|A\_k'| = |A\_k|$），所以 $A\_k'$ 也是一个最优解。
9.  在这个新的最优解 $A\_k'$ 中，包含了我们的贪心选择 $a\_m$。

**证明完毕**。这个证明方法叫做**剪切-粘贴法 (cut-and-paste) 或交换论证 (exchange argument)**，是证明贪心算法正确性的常用技巧。

接下来是**最优子结构 (Optimal substructure)**。 在我们做出了贪心选择 $a\_m$ 之后，原问题就缩减成了一个子问题：在所有与 $a\_m$ 兼容的活动中，寻找一个最大兼容活动子集。最优子结构性质表明：**原问题的最优解 = 贪心选择 + 子问题的最优解**。这个性质在本问题中是显而易见的，证明可以省略。

#### **4\. 算法实现**

**伪代码：**

```
GreedyActivitySelector(s, f):
  n = s.length
  // 假设活动已按结束时间f升序排序
  A = {a_1}  // 直接选择第一个活动
  last_finish_time = f[1]

  for i = 2 to n:
    if s[i] >= last_finish_time: // 如果当前活动与上一个选择的活动兼容
      A = A U {a_i}
      last_finish_time = f[i]
  
  return A
```

**C++ 实现:**

```cpp
#include <iostream>
#include <vector>
#include <algorithm>

struct Activity {
    int id;
    int start;
    int finish;
};

// 比较函数，用于按结束时间排序
bool compareActivities(const Activity& a, const Activity& b) {
    return a.finish < b.finish;
}

void printActivities(const std::vector<int>& activities) {
    std::cout << "Selected activities: ";
    for (int id : activities) {
        std::cout << id << " ";
    }
    std::cout << std::endl;
}

// 贪心算法求解活动选择问题
std::vector<int> selectActivities(std::vector<Activity>& acts) {
    // 1. 按结束时间升序排序
    std::sort(acts.begin(), acts.end(), compareActivities);

    std::vector<int> result;
    if (acts.empty()) {
        return result;
    }

    // 2. 选择第一个活动
    result.push_back(acts[0].id);
    int last_finish_time = acts[0].finish;

    // 3. 遍历剩余活动
    for (size_t i = 1; i < acts.size(); ++i) {
        // 如果当前活动的开始时间晚于或等于上一个选定活动的结束时间
        if (acts[i].start >= last_finish_time) {
            result.push_back(acts[i].id);
            last_finish_time = acts[i].finish;
        }
    }
    return result;
}

int main() {
    std::vector<Activity> activities = {
        {1, 1, 4}, {2, 3, 5}, {3, 0, 6}, {4, 5, 7}, {5, 3, 9},
        {6, 5, 9}, {7, 6, 10}, {8, 8, 11}, {9, 8, 12}, {10, 2, 14}, {11, 12, 16}
    };
    
    auto selected_ids = selectActivities(activities);
    printActivities(selected_ids); // 示例输出可能是: 1 4 8 11

    return 0;
}
```

**时间复杂度分析：**

*   排序需要 $O(N \\log N)$ 时间。
*   贪心选择过程只需要一次线性扫描，时间为 $O(N)$。
*   因此，总时间复杂度为 $O(N \\log N)$。

#### **5\. 拓展：带权重的活动选择问题**

现在，我们给问题增加点难度。如果每个活动 $a\_i$ 都有一个权重 $w\_j$ (比如参加这个活动能获得的收益)，我们的目标不再是最大化活动数量，而是**最大化总权重**。

我们再来看一下动态规划的解法。 令 $c\_{1,j}$ 为考虑前 $j$ 个活动（已按结束时间排序）所能获得的最大权重。对于活动 $j$，我们有两种选择：

1.  不选择活动 $j$：最大权重为 $c\_{1, j-1}$。
2.  选择活动 $j$：我们需要找到在 $j$ 开始之前就结束的、与 $j$ 兼容的最后一个活动 $k(j)$，然后最大权重为 $c\_{1, k(j)} + w\_j$。

递推公式为： $$c\_{1,j} = \\begin{cases} 1 & \\text{if } j=1 \\\\ \\max{c\_{1,j-1}, c\_{1,k(j)} + w\_j} & \\text{if } j>1 \\end{cases}$$

**Q1: 这个 DP 解法在带权重的情况下还正确吗？** **A1:** 是的，完全正确。DP 的本质就是通过穷举所有选择（选或不选）并记录子问题的最优解来构建全局最优解。权重只是改变了我们计算“价值”的方式，而没有改变问题的结构。

**Q2: 我们之前的“最早结束时间”贪心策略还正确吗？** **A2:** 不正确了！

**反例：** 假设有三个活动：

*   $a\_1$: `[0, 5)`, weight = 2
*   $a\_2$: `[4, 8)`, weight = 10
*   $a\_3$: `[6, 10)`, weight = 2

按照“最早结束时间”贪心策略：

1.  选择 $a\_1$ (结束最早)。总权重 = 2。
2.  $a\_2$ 与 $a\_1$ 冲突，跳过。
3.  选择 $a\_3$ (与 $a\_1$ 兼容)。总权重 = 2 + 2 = 4。 贪心解的总权重为 4。

但最优解是只选择 $a\_2$，总权重为 10。

**结论**：带权重的活动选择问题是一个更复杂的问题，简单的贪心策略失效了，我们需要使用动态规划来求解。这也深刻地揭示了贪心算法的局限性。

### **第三部分：贪心策略的设计原则**

通过活动选择的例子，我们可以总结出设计和证明一个贪心算法的通用步骤：

1.  **问题转化**：将优化问题描述成这样一个过程：每做一次选择，留下一个规模更小的子问题。
2.  **证明贪心选择性质**：证明你设计的贪心准则所做出的第一个选择，一定包含在某个最优解之中。这是最关键的一步，通常使用“交换论证”。如果这一步无法证明，那你的贪心策略很可能是错的。
3.  **证明最优子结构**：证明原问题的最优解等于你做出的贪心选择，加上对产生的子问题应用最优解。

一个重要的观察是：**几乎每一个正确的贪心算法背后，都隐藏着一个更复杂、更笨重的动态规划解法。** 贪心算法之所以能成立，是因为问题的特殊结构使得我们不需要像 DP 那样考虑所有选择，只需要“贪心”地走一条路即可。

### **第四部分：经典案例二：霍夫曼编码 (Huffman Codes)**

现在我们来看一个贪心算法在现实世界中大放异彩的应用——数据压缩。

#### **1\. 问题背景：文件压缩**

【**示例**】假设我们有一段长为1000的文本，只由 `a, u, x, z` 四个字符组成。如果使用标准的 ASCII 编码，每个字符占 8 位（1字节），那么存储这段文本需要 $1000 \\times 8 = 8000$ 位。

我们发现，其实只有4个不同的字符，我们可以设计一套更短的编码。比如：

*   `a = 00`
*   `u = 01`
*   `x = 10`
*   `z = 11`

这种每个字符编码长度相同的称为**定长编码**。用这种编码，总长度为 $1000 \\times 2 = 2000$ 位，大大节省了空间。

我们还能做得更好吗？假设在这1000个字符中，它们的出现**频率 (frequency)** 不同：

*   `f(a) = 400`
*   `f(x) = 300`
*   `f(u) = 200`
*   `f(z) = 100` （为方便说明，这里修改了原例中的频数）

直觉告诉我们，应该给出现频率高的字符更短的编码，给出现频率低的字符更长的编码。这就是**变长编码**的思想。

例如，我们可以设计这样一套编码：

*   `a = 0`
*   `x = 10`
*   `u = 110`
*   `z = 111`

我们来解码一个字符串试试，比如 `010110111`。它可以被唯一地解码为 `a` `x` `u` `z`。

#### **2\. 前缀码与编码树**

为什么上面这套编码可以被唯一解码？因为它满足一个重要性质：**前缀码 (Prefix Code)**。即任何一个字符的编码都不是另一个字符编码的前缀。

*   `a=0` 不是 `x, u, z` 的前缀。
*   `x=10` 不是 `u, z` 的前缀。
*   `u=110` 不是 `z` 的前缀。

这种性质保证了我们在解码时不会有歧义。

我们可以用二叉树来表示前缀码，这棵树也叫**编码树 (trie)**。

*   从根节点出发，向左走代表`0`，向右走代表`1`。
*   每个字符都位于一个**叶子节点**上。从根到该叶子节点的路径，就构成了该字符的编码。

**回答之前的问题：** What must the tree look like if we are to decode unambiguously? **答案：** 为了能够无歧义地解码，这棵树必须是一棵**满二叉树 (Full Binary Tree)** 的变体，其中所有待编码的字符都必须位于**叶子节点**。如果某个字符位于非叶子节点，那么它的编码必然是它子树中某个字符编码的前缀，这就违反了前缀码的性质。

例如，`a=0, u=110, x=10, z=111` 这套编码对应的树是：

graph TD subgraph 编码树 Root(( )) -- 0 --> a\[a\]; Root -- 1 --> N1(( )); N1 -- 0 --> x\[x\]; N1 -- 1 --> N2(( )); N2 -- 0 --> u\[u\]; N2 -- 1 --> z\[z\]; end

**编码成本：** 如果一个字符 $c\_i$ 的频率是 $f(c\_i)$，它在编码树中的深度是 $d\_T(c\_i)$（也即其编码长度），那么编码整个文件的总成本（总位数）为： $$Cost(T) = \\sum\_{c \\in C} f(c) \\cdot d\_T(c)$$ 我们的目标就是找到一棵编码树 $T$，使得这个 $Cost(T)$ 最小。

#### **3\. 霍夫曼的贪心算法 (1952)**

大卫·霍夫曼提出了一个非常优雅的贪心算法来解决这个问题。 **贪心准则：** 每次选择当前集合中**频率最低**的两个字符（或子树），将它们合并成一棵新的子树。新子树的频率等于它两个孩子频率之和。

**算法步骤：**

1.  为每个字符创建一个只包含一个节点的树，该节点的权重就是字符的频率。
2.  将这些树放入一个**最小优先队列 (min-priority queue)** 中（按权重排序）。
3.  当队列中还有多于一个树时： a. 从队列中取出两个权重最小的树（设为 $T\_x, T\_y$）。 b. 创建一个新的内部节点，其权重为 $T\_x$ 和 $T\_y$ 的权重之和。 c. 将 $T\_x$ 和 $T\_y$ 作为新节点的左右孩子（顺序不重要）。 d. 将新生成的树插回优先队列。
4.  当队列中只剩一个树时，这个树就是最终的霍夫曼树。

**【示例演练】** 我们用讲座中的例子来完整走一遍流程。字符集和频率如下： `a:10, e:15, i:12, s:3, t:4, sp:13, nl:1`

**初始状态** (优先队列)： `[(nl,1), (s,3), (t,4), (a,10), (i,12), (sp,13), (e,15)]`

**第1步:** 合并 `nl(1)` 和 `s(3)` -> `N1(4)` 队列: `[(t,4), (N1,4), (a,10), (i,12), (sp,13), (e,15)]`

**第2步:** 合并 `t(4)` 和 `N1(4)` -> `N2(8)` 队列: `[(N2,8), (a,10), (i,12), (sp,13), (e,15)]`

**第3步:** 合并 `N2(8)` 和 `a(10)` -> `N3(18)` 队列: `[(i,12), (sp,13), (e,15), (N3,18)]`

**第4步:** 合并 `i(12)` 和 `sp(13)` -> `N4(25)` 队列: `[(e,15), (N3,18), (N4,25)]`

**第5步:** 合并 `e(15)` 和 `N3(18)` -> `N5(33)` 队列: `[(N4,25), (N5,33)]`

**第6步:** 合并 `N4(25)` 和 `N5(33)` -> `Root(58)` 队列: `[(Root,58)]` -> 结束

最终构建的树如下：

graph TD subgraph "霍夫曼树 (总权重 58)" 58 -- 0 --> 25; 58 -- 1 --> 33; 25 -- 0 --> i(i:12); 25 -- 1 --> sp(sp:13); 33 -- 0 --> e(e:15); 33 -- 1 --> 18; 18 -- 0 --> 8; 18 -- 1 --> a(a:10); 8 -- 0 --> t(t:4); 8 -- 1 --> 4; 4 -- 0 --> nl(nl:1); 4 -- 1 --> s(s:3); end

从这棵树，我们可以得到每个字符的编码：

*   i: 00
*   sp: 01
*   e: 10
*   t: 1100
*   nl: 11010
*   s: 11011
*   a: 111

**计算总成本：** $$Cost = 12 \\times 2 (\\text{i}) + 13 \\times 2 (\\text{sp}) + 15 \\times 2 (\\text{e}) + 4 \\times 4 (\\text{t}) + 1 \\times 5 (\\text{nl}) + 3 \\times 5 (\\text{s}) + 10 \\times 3 (\\text{a})$$ $$Cost = 24 + 26 + 30 + 16 + 5 + 15 + 30 = 146$$

#### **4\. 算法实现**

**伪代码:**

```
Huffman(C):
  n = |C|
  Q = C // 将所有字符作为单节点树放入优先队列
  for i = 1 to n-1:
    z = new Node()
    z.left = x = Q.extract_min()
    z.right = y = Q.extract_min()
    z.freq = x.freq + y.freq
    Q.insert(z)
  return Q.extract_min() // 返回根节点
```

**C++ 实现:**

```cpp
#include <iostream>
#include <vector>
#include <queue>
#include <string>
#include <map>
#include <memory>

// 树节点
struct HuffmanNode {
    char data;
    unsigned freq;
    std::shared_ptr<HuffmanNode> left, right;

    HuffmanNode(char data, unsigned freq) : data(data), freq(freq), left(nullptr), right(nullptr) {}
};

// 优先队列的比较结构体
struct compare {
    bool operator()(const std::shared_ptr<HuffmanNode>& l, const std::shared_ptr<HuffmanNode>& r) {
        return l->freq > r->freq;
    }
};

// 递归函数，用于打印霍夫曼码
void printCodes(const std::shared_ptr<HuffmanNode>& root, std::string str, std::map<char, std::string>& huffmanCode) {
    if (!root) return;

    if (root->data != '$') { // '$' 表示内部节点
        huffmanCode[root->data] = str;
    }

    printCodes(root->left, str + "0", huffmanCode);
    printCodes(root->right, str + "1", huffmanCode);
}

// 构建并打印霍夫曼树
void HuffmanCodes(const std::vector<char>& data, const std::vector<unsigned>& freq) {
    std::priority_queue<std::shared_ptr<HuffmanNode>, std::vector<std::shared_ptr<HuffmanNode>>, compare> minHeap;

    for (size_t i = 0; i < data.size(); ++i) {
        minHeap.push(std::make_shared<HuffmanNode>(data[i], freq[i]));
    }

    while (minHeap.size() != 1) {
        auto left = minHeap.top(); minHeap.pop();
        auto right = minHeap.top(); minHeap.pop();

        auto top = std::make_shared<HuffmanNode>('$', left->freq + right->freq);
        top->left = left;
        top->right = right;
        minHeap.push(top);
    }
    
    std::map<char, std::string> huffmanCode;
    printCodes(minHeap.top(), "", huffmanCode);

    std::cout << "Huffman Codes:\n";
    for(auto const& [key, val] : huffmanCode) {
        std::cout << key << ": " << val << std::endl;
    }
}

int main() {
    std::vector<char> data = {'a', 'e', 'i', 's', 't', 'p', 'n'}; // sp->p, nl->n
    std::vector<unsigned> freq = {10, 15, 12, 3, 4, 13, 1};
    
    HuffmanCodes(data, freq);
    
    return 0;
}
```

**时间复杂度分析：** 假设有 $C$ 个字符。构建优先队列需要 $O(C)$。循环执行 $C-1$ 次，每次循环包含两次 `extract_min` 和一次 `insert`。在基于堆的优先队列中，这些操作的时间复杂度都是 $O(\\log C)$。因此，总时间复杂度为 $O(C \\log C)$。

#### **5\. 正确性证明**

霍夫曼算法的正确性同样依赖于**贪心选择性质**和**最优子结构**。

##### **引理1：贪心选择性质**

**【引理】** 令 $C$ 为一个字符集，令 $x$ 和 $y$ 是 $C$ 中频率最低的两个字符。那么必然存在一个最优前缀码的编码树，其中 $x$ 和 $y$ 是兄弟节点，并且位于树的最深层。

**【证明（交换论证）】**

1.  令 $T$ 是任意一棵最优编码树。令 $a$ 和 $b$ 是 $T$ 中深度最深的两个兄弟节点。
    
2.  不失一般性，假设 $f(x) \\le f(y)$ 且 $f(a) \\le f(b)$。因为 $x, y$ 是频率最低的，所以 $f(x) \\le f(a)$ 且 $f(y) \\le f(b)$。
    
3.  我们把 $T$ 中的 $a$ 和 $x$ 的位置交换，得到树 $T'$。再把 $b$ 和 $y$ 的位置交换，得到树 $T''$。在 $T''$ 中，$x$ 和 $y$ 成了兄弟。
    
4.  我们来比较 $T$ 和 $T''$ 的成本。成本的变化只与这四个节点有关：
    
    $$\\begin{aligned}Cost(T) - Cost(T'') &= \[f(a)d\_T(a) + f(b)d\_T(b) + f(x)d\_T(x) + f(y)d\_T(y)\] - \[f(x)d\_T(a) + f(y)d\_T(b) + f(a)d\_T(x) + f(b)d\_T(y)\]\\\\ &= (f(a)-f(x))d\_T(a) + (f(b)-f(y))d\_T(b) + (f(x)-f(a))d\_T(x) + (f(y)-f(b))d\_T(y)\\\\ &= (f(a)-f(x))d\_T(a) + (f(b)-f(y))d\_T(b) - (f(a)-f(x))d\_T(x) - (f(b)-f(y))d\_T(y)\\\\ &= (f(a)-f(x))(d\_T(a)-d\_T(x)) + (f(b)-f(y))(d\_T(b)-d\_T(y)) \\end{aligned}$$
    
5.  因为 $a,b$ 在最深层，所以 $d\_T(a) \\ge d\_T(x)$ 且 $d\_T(b) \\ge d\_T(y)$。同时 $f(a) \\ge f(x)$ 且 $f(b) \\ge f(y)$。
    
6.  因此，上面差值的每一项都是非负的，所以 $Cost(T) - Cost(T'') \\ge 0$，即 $Cost(T) \\ge Cost(T'')$。
    
7.  因为 $T$ 本身是最优的，所以 $Cost(T) \\le Cost(T'')$。结合两者，必有 $Cost(T) = Cost(T'')$。
    
8.  这意味着，将 $x, y$ 放到最深层作为兄弟节点，可以得到一棵成本同样最优的树 $T''$。
    

**证明完毕**。这说明我们的贪心选择（合并频率最低的两个节点）是“安全”的。

##### **引理2：最优子结构**

**【引理】** 令 $x, y$ 为频率最低的两个字符。构造一个新的字符集 $C'$，它由 $C - {x,y}$ 和一个新字符 $z$ 组成，其中 $f(z) = f(x) + f(y)$。如果树 $T'$ 是 $C'$ 的一个最优编码树，那么将 $T'$ 中代表 $z$ 的叶子节点替换为一个以 $x,y$ 为孩子的内部节点，所得到的新树 $T$ 是原字符集 $C$ 的一个最优编码树。

**【证明（反证法）】**

1.  首先，我们观察 $T$ 和 $T'$ 成本的关系： $$Cost(T) = \\sum\_{c \\in C-{x,y}} f(c)d\_T(c) + f(x)d\_T(x) + f(y)d\_T(y)$$ $$d\_T(x) = d\_T(y) = d\_{T'}(z) + 1$$ $$d\_T(c) = d\_{T'}(c) \\text{ for } c \\in C-{x,y}$$ $$Cost(T) = Cost(T') - f(z)d\_{T'}(z) + (f(x)+f(y))(d\_{T'}(z)+1)$$ $$Cost(T) = Cost(T') - f(z)d\_{T'}(z) + f(z)d\_{T'}(z) + f(z) = Cost(T') + f(x) + f(y)$$ 即 $$Cost(T) = Cost(T') + f(x) + f(y)$$这个差值是固定的。
2.  **假设** $T$ 不是 $C$ 的最优编码树。那么存在另一棵树 $T\_{opt}$，使得 $Cost(T\_{opt}) < Cost(T)$。
3.  根据引理1，我们可以假设在 $T\_{opt}$ 中，$x$ 和 $y$ 是兄弟节点。
4.  现在我们从 $T\_{opt}$ 构造一棵用于 $C'$ 的树 $T\_{opt}'$：将 $x,y$ 和它们的父节点替换成一个代表 $z$ 的叶子节点。
5.  $T\_{opt}'$ 的成本为 $Cost(T\_{opt}') = Cost(T\_{opt}) - f(x) - f(y)$。
6.  代入 $Cost(T\_{opt}) < Cost(T)$，我们得到 $$Cost(T\_{opt}') < Cost(T) - f(x) - f(y) = Cost(T')$$
7.  $Cost(T\_{opt}') < Cost(T')$ 这意味着 $T'$ 不是字符集 $C'$ 的最优编码树。
8.  这与我们的初始条件“$T'$ 是 $C'$ 的一个最优编码树”**相矛盾**。
9.  因此，假设不成立，$T$ 必须是 $C$ 的最优编码树。

**证明完毕**。两个引理结合，完美地证明了霍夫曼算法的正确性。