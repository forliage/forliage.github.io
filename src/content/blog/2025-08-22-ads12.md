---
title: "ads12:局部搜索 (Local Search)"
description: ""
pubDate: "2025-08-22"
heroImage: ""
---

# ads12:局部搜索 (Local Search)

### **第一部分：什么是局部搜索？**

想象一下，你被蒙上眼睛，放在一个连绵起伏的山脉中，你的任务是找到山脉的最高点（全局最优解）。你该怎么办？一个很自然的想法是：在你的脚下感受一下周围哪个方向是向上走的，然后朝着最陡峭的向上方向走一步。你不断重复这个过程，直到你发现无论朝哪个方向走，地势都是向下的。这时，你就到达了一个“山顶”。

这个“山顶”就是我们所说的**局部最优解 (local optimum)**。它不一定是整个山脉的最高峰（**全局最优解, global optimum**），但它确实是它附近区域的最高点。

这张图生动地展示了局部搜索的核心思想。我们从一个初始的“猜测”（Guess）开始，在它的“邻域”（Neighborhood）内寻找一个更好的解。我们不断地进行这种“爬山”式的改进，直到无法找到任何更好的邻居为止。这时，我们就陷入了一个局部最优。我们的目标，当然是希望这个局部最优点就是全局最小值（Global Minimum）。

一个关键问题是：**这个过程能在有限的步数内结束吗？** 答案是肯定的，只要我们的每一步都是在进行“改进”，并且解空间是有限的，那么我们最终会停在“直到无法再改进”的地方。

#### **1.1 局部搜索的形式化框架**

让我们用更严谨的语言来定义这个过程。局部搜索算法通常包含两个核心部分：**“局部”** 和 **“搜索”**。

*   **局部 (Local):**
    
    1.  **定义邻域 (Neighborhoods):** 对于解空间中的每一个可行解 $S$，我们需要定义一个与之相邻的解的集合，称为 $S$ 的邻域，记作 $N(S)$。邻域中的解通常都是通过对 $S$ 进行微小的改动得到的。
    2.  **定义局部最优:** 如果一个解 $S$ 在其所有邻域 $N(S)$ 中是最好的（例如成本最低或收益最高），那么它就是一个**局部最优解**。
*   **搜索 (Search):**
    
    1.  **起始:** 从一个初始的可行解开始。
    2.  **迭代:** 在当前解的邻域中寻找一个更好的解。如果找到了，就移动到那个更好的解，并继续这个过程。
    3.  **终止:** 当在当前解的邻域中找不到任何比它更好的解时，搜索停止。此时，我们就说算法收敛到了一个**局部最优解**。

#### **1.2 邻域关系与梯度下降算法**

我们来形式化地定义“邻域”。

*   **邻接关系 ($\\sim$):** 如果解 $S'$ 可以通过对解 $S$ 进行一次小的修改得到，我们就说 $S'$ 是 $S$ 的一个**邻接解 (neighboring solution)**，记作 $S \\sim S'$。
*   **邻域 ($N(S)$):** 解 $S$ 的邻域 $N(S)$ 是所有与 $S$ 邻接的解的集合，即 $N(S) = { S' \\mid S \\sim S' }$。

最简单、最直观的局部搜索算法就是**梯度下降 (Gradient Descent)** 或称为 **爬山法 (Hill Climbing)**。它的策略是“贪心”的：在每一步都选择邻域中最好的那个解。

下面是梯度下降算法的伪代码：

```
SolutionType Gradient_descent()
{
    // 从一个可行的初始解 S 开始
    S = GenerateInitialSolution();
    
    // 计算初始解的成本
    MinCost = cost(S);

    while (true) {
        // 在 S 的邻域 N(S) 中找到最好的邻居 S'
        S' = Search_Best_In_Neighborhood(N(S));
        
        // 计算 S' 的成本
        CurrentCost = cost(S');
        
        // 如果 S' 比 S 更好 (成本更低)
        if (CurrentCost < MinCost) {
            // 更新最优解和最低成本
            S = S';
            MinCost = CurrentCost;
        } else {
            // 如果邻域中没有更好的解，说明到达局部最优，退出循环
            break;
        }
    }
    
    // 返回找到的局部最优解
    return S;
}
```

### **第二部分：实例分析 - 顶点覆盖问题 (Vertex Cover)**

为了更好地理解局部搜索，我们来看一个经典的NP-Hard问题：**顶点覆盖**。

#### **2.1 问题定义**

*   **决策版本:** 给定一个无向图 $G=(V, E)$ 和一个整数 $K$，问图中是否存在一个顶点的子集 $V' \\subseteq V$，使得 $|V'| \\le K$，并且图中的每一条边 $(u, v) \\in E$ 的两个端点至少有一个在 $V'$ 中？这样的 $V'$ 就被称为一个**顶点覆盖**。
    
*   **优化版本:** 给定一个无向图 $G=(V, E)$，找到一个**最小**的顶点子集 $S \\subseteq V$，使得 $S$ 是一个顶点覆盖。
    

我们的目标是解决这个优化版本。

#### **2.2 应用局部搜索**

现在，我们将顶点覆盖问题套入局部搜索的框架中：

1.  **可行解集 (Feasible Solution Set, `FS`):** 所有的顶点覆盖都是我们的可行解。
    
2.  **成本函数 (Cost Function):** 对于一个可行解（一个顶点覆盖）`S`，其成本就是它包含的顶点数量，即 `cost(S) = |S|`。我们的目标是最小化这个成本。
    
3.  **邻域关系 (`S ~ S'`):** 如何定义邻域？一个简单的方法是：每次只修改一个顶点。但是，如果我们随意添加或删除一个顶点，得到的新集合可能不再是一个顶点覆盖。一个更巧妙的定义是：
    
    *   从一个顶点覆盖 `S` 中**删除**一个顶点 `v`。如果得到的新集合 `S' = S - {v}` 仍然是一个顶点覆盖，那么 `S'` 就是 `S` 的一个邻居。
    *   向一个顶点覆盖 `S` 中**添加**一个不在 `S` 中的顶点 `v`。得到的新集合 `S' = S + {v}` 显然还是一个顶点覆盖，但它增加了成本，这对于我们的梯度下降算法来说不是一个有吸引力的移动方向，但在更高级的算法中可能会用到。
    
    因此，对于梯度下降，我们可以定义 `S'` 是 `S` 的邻居，如果 `S'` 是通过从 `S` 中移除一个顶点得到的，并且 `S'` 仍然是顶点覆盖。
    
4.  **搜索策略:**
    
    *   **起始:** 一个简单且保证是可行解的初始方案是 `S = V`，即选择所有顶点。这显然是一个顶点覆盖，尽管成本很高。
    *   **迭代:** 在当前顶点覆盖 `S` 中，尝试删除每一个顶点 `v`，检查 `S' = S - {v}` 是否仍然是顶点覆盖。如果是，并且 `|S'| < |S|`（这总是成立的），我们就找到了一个更好的解。我们可以选择第一个找到的，也可以遍历所有可删除的顶点，选择使成本下降最多的那个。

**总结一下：**

*   每个顶点覆盖 `S` 最多有 $|S| \\le |V|$ 个邻居（通过删除一个顶点得到）。
*   搜索过程：从 `S = V` 开始，不断地尝试删除一个顶点，只要删除后仍然是顶点覆盖，就执行这个删除操作，直到无法再删除任何顶点为止。

### **第三部分：局部搜索的陷阱**

梯度下降法非常直观，但它有一个致命的弱点：**容易陷入局部最优**。我们来看几个例子。

右边的曲线图可以看作是解空间的成本函数地貌。我们的目标是找到最低谷（全局最优），但梯度下降只会带我们到某个山谷（局部最优）。

graph TD %% ====================================================== %% Group for "Case 0" %% ====================================================== subgraph case0 \[Case 0: No Edges\] A0(( )) B0(( )) C0(( )) D0(( )) end subgraph result0 \[Result for Case 0\] S\_0("S = ∅  
(Global Optimum)") end case0 -- "Gradient Descent finds" --> result0 %% ====================================================== %% Group for "Case 1" - THIS IS THE NEW, SAFE VERSION %% ====================================================== subgraph case1 \[Case 1: Star Graph\] %% STEP 1: Define every node on its own line. CenterNode Leaf1(( )) Leaf2(( )) Leaf3(( )) Leaf4(( )) %% STEP 2: Define every connection on its own line. Leaf1 --- CenterNode Leaf2 --- CenterNode Leaf3 --- CenterNode Leaf4 --- CenterNode end subgraph result1 \[Result for Case 1\] X("Local Optimum: {Leafs}  
Global Optimum: {CenterNode}") end case1 -- "May get stuck in" --> result1 %% ====================================================== %% Group for "Case 2" %% ====================================================== subgraph case2 \[Case 2: Plateau / Ridge\] %% Define nodes first R1 N1\_2 R2 N1\_3 R3 %% Define connections line by line R1 --- N1\_2 N1\_2 --- R2 R2 --- N1\_3 N1\_3 --- R3 end subgraph result2 \[Result for Case 2\] Y("Stuck in Local Optimum / Plateau") end case2 -- "May get stuck on" --> result2 %% ====================================================== %% Apply all styles at the end %% ====================================================== style CenterNode fill:red,stroke:#333,stroke-width:2px style R1 fill:red,stroke:#333,stroke-width:2px style R2 fill:red,stroke:#333,stroke-width:2px style R3 fill:red,stroke:#333,stroke-width:2px

*   **Case 0: 没有边的图**
    
    *   最小顶点覆盖是空集 `S = ∅`，成本为0。
    *   如果我们从 `S = V` 开始，我们可以一个个地删除所有顶点，最终会顺利到达全局最优解 `S = ∅`。这个例子很简单，梯度下降是有效的。
*   **Case 1: 星形图**
    
    *   这个图有一个中心节点和许多叶子节点。
    *   全局最优解是只包含中心节点，`S = {中心节点}`，成本为 1。
    *   另一个解是包含所有叶子节点，`S' = {所有叶子节点}`，成本很高。这个解也是一个顶点覆盖，而且它是一个**局部最优解**！为什么？因为你不能从 `S'` 中删除任何一个叶子节点，否则它与中心节点相连的边就没有被覆盖。因此，`S'` 没有成本更低的邻居，梯度下降如果从某个特定的初始状态出发（或者其邻域定义比较特殊），就可能卡在这里。
*   **Case 2: 高原 (Plateau) 和山脊 (Ridge)**
    
    *   在这个例子中，可能存在很多成本相同的局部最优解，它们之间无法通过简单的“下山”移动来切换。
    *   想象一下一个平坦的高原，或者一个狭长的山脊，任何方向的微小移动要么保持高度不变，要么就是下山。梯度下降算法在高原上会随机停止，在山脊上则无法跨越到另一侧可能存在的更低的山谷。

#### **讨论：梯度下降在哪些其他情况下会失效？**

这是一个很好的问题。除了上面提到的情况，一个经典的例子是**旅行商问题 (TSP)**。

*   **问题:** 找到一条访问所有城市一次并返回起点的最短路径。
*   **局部搜索框架:**
    *   **解:** 一条合法的路径（一个城市的排列）。
    *   **成本:** 路径总长度。
    *   **邻域:** 一个常用的邻域是 **2-opt**。即，选择路径中的两条边 `(a, b)` 和 `(c, d)`，删除它们，然后用 `(a, c)` 和 `(b, d)` 重新连接，从而得到一条新的路径。
*   **失效情况:** 2-opt 局部搜索很容易陷入局部最优。可能会找到一个看起来还不错的环路，但这个环路可能存在一些交叉，而要解开这些交叉可能需要同时改变3条或更多的边（即需要一个更大的邻域），而 2-opt 无法实现这一点。

### **第四部分：如何逃离局部最优？- 模拟退火**

既然梯度下降这么容易“卡住”，我们该如何改进呢？一个核心思想是：**允许偶尔接受一个更差的解**，以求“跳出”当前的局部最优，去探索更广阔的解空间。

#### **4.1 Metropolis 算法**

Metropolis 算法引入了概率。

*   如果邻居 `S'` 比当前解 `S` 好，我们总是接受它。
*   如果邻居 `S'` 比当前解 `S` 差（成本更高），我们**以一定的概率**接受它。

这个概率通常由 **Boltzmann 分布** 给出：$P(\\text{accept}) = e^{-\\Delta E / (kT)}$ 其中：

*   $\\Delta E = \\text{cost}(S') - \\text{cost}(S)$ 是成本的增量（在这里是正数）。
*   $T$ 是一个称为“温度”的参数。
*   $k$ 是玻尔兹曼常数（在算法中可以设为1）。

**直观理解:**

*   **$\\Delta E$ 越小** (即 `S'` 差得不多)，接受的概率越大。
*   **温度 $T$ 越高**，接受的概率越大。这意味着在高温时，算法更倾向于探索，甚至接受很差的解。

伪代码如下：

```
SolutionType Metropolis()
{
    // 定义常数 k 和 T
    Define constants k and T;

    // 从一个可行解开始
    S = GenerateInitialSolution();
    MinCost = cost(S); // 记录遇到的最好成本

    while (not_stopped) {
        // 从邻域中随机选择一个邻居
        S' = Randomly_chosen_from(N(S));
        CurrentCost = cost(S');

        if (CurrentCost < cost(S)) { // 注意这里是和 cost(S) 比较
            // 如果更好，总是接受
            S = S';
            if (CurrentCost < MinCost) MinCost = CurrentCost;
        } else {
            // 如果更差，计算成本差
            delta_cost = CurrentCost - cost(S);
            // 以一定概率接受
            if (random_uniform(0, 1) < exp(-delta_cost / (k * T))) {
                S = S';
            }
        }
    }
    return best_solution_found; // 返回整个过程中遇到的最好的解
}
```

#### **4.2 模拟退火 (Simulated Annealing)**

模拟退火是 Metropolis 算法的升级版。它模拟了物理中金属退火的过程：将金属加热到高温，然后非常缓慢地冷却。在高温时，分子可以自由移动（对应算法中的广泛探索），随着温度降低，分子逐渐稳定下来，形成能量最低的晶体结构（对应算法收敛到最优解）。

关键在于**冷却策略 (Cooling schedule)**：温度 `T` 不再是固定的，而是随着时间的推移慢慢降低。 $T = { T\_1, T\_2, T\_3, \\dots }$, 其中 $T\_1 > T\_2 > T\_3 > \\dots$

*   **初期 (高温):** `T` 很大，`exp(-ΔE/kT)` 接近1，算法几乎会接受所有移动，类似于随机游走，从而广泛探索解空间。
*   **中期 (中温):** `T` 减小，算法开始倾向于接受好的移动，但仍有一定机会跳出局部最优。
*   **末期 (低温):** `T` 趋近于0，`exp(-ΔE/kT)` 趋近于0，算法几乎只接受好的移动，行为类似于梯度下降，从而在当前找到的“好区域”内进行精细搜索。

理论上可以证明，如果冷却得足够慢，模拟退火算法能以概率1收敛到全局最优解。

**注**：更详细的关于模拟退火及其拓展的内容可以参考: [我的另一篇文章](https://forliage.github.io/_posts/2025-08-20-PSA.html)

### **第五部分：高级实例 1 - Hopfield 神经网络**

局部搜索在很多领域都有应用，我们来看一个来自神经网络的例子：Hopfield 网络。

#### **5.1 问题建模**

想象一个网络，由节点和带权重的边组成。

*   **图:** $G=(V, E)$，边的权重 $w\_e$ 可以是正数或负数。
    
*   **节点状态:** 每个节点 `u` 有一个状态 $s\_u$，可以是 `+1` 或 `-1`。
    
*   **约束:**
    
    *   如果边 `e=(u, v)` 的权重 $w\_e < 0$，它表示 `u` 和 `v` 希望拥有**相同**的状态 ($s\_u = s\_v$)。
    *   如果边 `e=(u, v)` 的权重 $w\_e > 0$，它表示 `u` 和 `v` 希望拥有**不同**的状态 ($s\_u = -s\_v$)。
    *   权重的绝对值 $|w\_e|$ 代表这个约束的**强度**。
*   **输出:** 我们要找一个网络的**构型 (configuration)** `S`，也就是给每个节点都分配一个状态 `+1` 或 `-1`，使得整个网络“尽可能和谐”（即满足尽可能多的、权重大的约束）。
    

graph TD A ---|7| B B ---|6| C C ---|5| A style A fill:#fff,stroke:#333,stroke-width:2px style B fill:#fff,stroke:#333,stroke-width:2px style C fill:#fff,stroke:#333,stroke-width:2px

在这个例子中，所有权重都是正的。`A`和`B`想不同，`B`和`C`想不同，`C`和`A`也想不同。这可能吗？如果我们让 $s\_A=+1$, $s\_B=-1$, $s\_C=+1$，那么 `(A,B)` 和 `(B,C)` 满足了，但 `(C,A)` 不满足。事实上，**可能不存在一个构型满足所有边的要求**。我们的目标是找到一个“足够好”的构型。

#### **5.2 形式化定义**

*   **好边 (good edge) / 坏边 (bad edge):**
    
    *   在构型 `S` 中，如果一条边 `e=(u,v)` 的约束被满足，即 $w\_e s\_u s\_v < 0$，则称它为**好边**。
        *   (若 $w\_e < 0$，需 $s\_u s\_v > 0 \\implies s\_u=s\_v$)
        *   (若 $w\_e > 0$，需 $s\_u s\_v < 0 \\implies s\_u \\ne s\_v$)
    *   否则，称之为**坏边**。
*   **满足的节点 (satisfied node):**
    
    *   一个节点 `u` 是**满足的**，如果与它相连的**好边的权重之和**大于或等于**坏边的权重之和**。
    *   数学上，这等价于 $\\sum\_{v: e=(u,v) \\in E} w\_e s\_u s\_v \\le 0$。
        *   为什么？$w\_e s\_u s\_v$ 对于好边是负数，对于坏边是正数。这个和小于等于0，意味着好边的权重（的绝对值）之和大于等于坏边的。
*   **稳定构型 (stable configuration):**
    
    *   如果一个构型中的**所有节点**都是满足的，则称该构型是**稳定**的。

**问题：** Hopfield 网络是否总存在一个稳定构型？如果存在，如何找到它？

graph TD A ---|-10| B A ---|4| C A ---|5| D C ---|-1| B D ---|-1| B subgraph Legend L\_White("State (+1)") L\_Black("State (-1)") end style A fill:#fff,stroke:#333,stroke-width:2px style B fill:#fff,stroke:#333,stroke-width:2px style C fill:#000,stroke:#333,stroke-width:2px,color:#fff style D fill:#000,stroke:#333,stroke-width:2px,color:#fff style L\_White fill:#fff,stroke:#333,stroke-width:2px style L\_Black fill:#000,stroke:#333,stroke-width:2px,color:#fff

我们来分析上图左下角的节点 `A` (状态+1)。

*   与B相连的边：$w=-10, s\_A=+1, s\_B=+1 \\implies w s\_A s\_B = -10 < 0$ (好边)
*   与C相连的边：$w=4, s\_A=+1, s\_C=-1 \\implies w s\_A s\_B = -4 < 0$ (好边)
*   与D相连的边：$w=5, s\_A=+1, s\_D=-1 \\implies w s\_A s\_B = -5 < 0$ (好边) 所有连接 `A` 的边都是好边，所以 `A` 是满足的。

现在我们看右边的计算示例，假设这是对某个节点的计算：`-4-1-1+5 = -1 <= 0`。这意味着连接该节点的边中，好边的权重之和（4+1+1）大于坏边的权重之和（5），所以该节点是满足的。

#### **5.3 状态翻转算法 (State-flipping Algorithm)**

我们可以用一个简单的局部搜索算法来寻找稳定构型。

*   **算法思想:** 只要网络中还存在不满意的节点，就随便找一个，然后**翻转它**的状态 (从+1到-1，或-1到+1)。

伪代码：

```
ConfigType State_flipping()
{
    // 从一个任意的初始构型 S 开始
    S = GenerateRandomConfiguration();
    
    // 只要 S 还不是稳定的
    while (!IsStable(S)) {
        // 找出一个不满意的节点 u
        u = GetUnsatisfied(S);
        
        // 翻转 u 的状态
        s_u = -s_u;
    }
    
    // 返回找到的稳定构型
    return S;
}
```

**关键问题：这个算法会一直运行下去吗？还是总能终止？**

graph TD A(( )) ---|8| C A ---|-10| B B(( )) ---|-5| D C ---|-1| B D ---|-1| B style C fill:#000,stroke:#333,stroke-width:2px style D fill:#000,stroke:#333,stroke-width:2px

上图给出了一个网络。我们想知道，对这个网络运行状态翻转算法，它会停下来吗？

**答案是：是的，这个算法总会终止。** 我们可以用一个非常漂亮的数学方法来证明它——**势函数法 (Potential Function Method)**。

#### **5.4 终止性证明**

**声明 (Claim):** 状态翻转算法最多在 $W = \\sum\_{e \\in E} |w\_e|$ 次迭代后，会终止于一个稳定构型。

**证明 (Proof):** 我们来定义一个“衡量系统有多好”的函数，称为势函数 $\\Phi(S)$。 让 $\\Phi(S)$ 等于当前构型 `S` 中所有**好边**的权重绝对值之和。 $$\\Phi(S) = \\sum\_{e \\text{ is good}} |w\_e|$$

现在，我们来分析当一个**不满足的节点 u** 翻转其状态后，$\\Phi(S)$ 会发生什么变化。设新构型为 `S'`。

1.  **与 `u` 相连的边:**
    *   当 `u` 的状态 $s\_u$ 变为 $-s\_u$ 时，对于任何一条边 $e=(u,v)$，乘积项 $s\_u s\_v$ 会变号。
    *   这意味着，所有原来与 `u` 相连的**好边**现在都变成了**坏边**。
    *   所有原来与 `u` 相连的**坏边**现在都变成了**好边**。
2.  **与 `u` 不相连的边:**
    *   这些边的状态（好或坏）保持不变。

所以，$\\Phi(S')$ 的值相对于 $\\Phi(S)$ 的变化是：

*   **减少了**所有原来与 `u` 相连的好边的 $|w\_e|$。
*   **增加了**所有原来与 `u` 相连的坏边的 $|w\_e|$。

$$\\Phi(S') = \\Phi(S) - \\sum\_{e=(u,v) \\in E, e \\text{ is bad in } S'} |w\_e| + \\sum\_{e=(u,v) \\in E, e \\text{ is good in } S'} |w\_e|$$ 这等价于： $$\\Phi(S') = \\Phi(S) - \\sum\_{e=(u,v) \\in E, e \\text{ is good in } S} |w\_e| + \\sum\_{e=(u,v) \\in E, e \\text{ is bad in } S} |w\_e|$$

现在，关键的一步来了。我们为什么选择翻转一个**不满足的节点 u**？ 根据定义，节点 `u` 不满足，意味着与它相连的坏边权重之和**严格大于**好边权重之和。 $$\\sum\_{e=(u,v) \\in E, e \\text{ is bad in } S} |w\_e| > \\sum\_{e=(u,v) \\in E, e \\text{ is good in } S} |w\_e|$$ (注意：$|w\_e|$ 与原始定义中的 $w\_e s\_u s\_v \\le 0$ 是对应的)

因此，$$\\Phi(S') - \\Phi(S) = \\sum\_{\\text{bad edges at } u} |w\_e| - \\sum\_{\\text{good edges at } u} |w\_e| > 0$$ 由于权重都是整数，我们甚至可以得出 $\\Phi(S') - \\Phi(S) \\ge 1$。

**结论:** 每一次翻转一个不满足的节点，势函数 $\\Phi(S)$ 的值**至少增加1**。

这个函数有上限吗？当然有。$\\Phi(S)$ 的最大值不可能超过所有边权重绝对值之和，即 $W = \\sum\_{e \\in E} |w\_e|$。 $$0 \\le \\Phi(S) \\le W$$

一个单调递增且有上界的整数序列必然是有限的。因此，状态翻转算法必然会在有限步内终止。当它终止时，网络中不存在任何不满足的节点，此时的构型就是一个稳定构型。

#### **5.5 算法复杂度分析**

*   **与局部搜索的关系:**
    *   **问题:** 最大化势函数 $\\Phi$。
    *   **可行解集 `FS`:** 所有的网络构型。
    *   **邻域关系 `S ~ S'`:** `S'` 可由 `S` 通过翻转单个节点状态得到。
*   **Claim:** 在最大化 $\\Phi$ 的状态翻转算法中，任何一个局部最大值点都是一个稳定构型。
    *   **证明:** 如果一个构型 `S` 不是稳定的，就必然存在一个不满足的节点 `u`。翻转 `u` 会得到一个邻居 `S'`，且 $\\Phi(S') > \\Phi(S)$。这意味着 `S` 不是一个局部最大值。反之，一个局部最大值必须是稳定的。

**这是一个多项式时间算法吗？(Is it a polynomial time algorithm?)**

答案是：**不一定**。它是一个**伪多项式时间 (pseudo-polynomial time)** 算法。

*   算法的运行步数上界是 $W = \\sum |w\_e|$。
*   输入图的规模由节点数 `n` 和边数 `m` 决定。权值 `w_e` 是用二进制表示的。如果权重 `W` 的数值可以非常大（例如，是 `n` 的指数级别），那么运行时间就不是 `n` 和 `m` 的多项式了。
*   **一个至今未解的公开问题 (Still an open question):** 是否存在一个算法，能在关于 `n` 和 `log W` 的多项式时间内找到稳定构型？或者，在只考虑算术运算次数的情况下，是否存在一个只关于 `n` 的多项式时间算法（与 `W` 无关）？

### **第六部分：高级实例 2 - 最大割问题 (Maximum Cut)**

最大割是另一个经典的NP-hard问题，它与 Hopfield 网络有很深的联系。

#### **6.1 问题定义**

给定一个带正权重的无向图 $G=(V, E)$，你需要将所有顶点 `V` 划分成两个不相交的集合 `(A, B)`。一个“割”指的是连接 `A` 中顶点和 `B` 中顶点的边的集合。**最大割问题**的目标是找到一个划分 `(A, B)`，使得这个割中所有边的权重之和最大。 $w(A, B) := \\sum\_{u \\in A, v \\in B} w\_{uv}$

graph TD subgraph Partition A direction LR a1(a1):::setA --- a2(a2):::setA a1 --- a3(a3):::setA end subgraph Partition B direction LR b1(b1):::setB --- b2(b2):::setB end a1 ---|5| b1 a2 ---|8| b1 a3 ---|3| b2 classDef setA fill:#f9f,stroke:#333,stroke-width:2px classDef setB fill:#9cf,stroke:#333,stroke-width:2px

上图展示了一个割。割的权重是 5 + 8 + 3 = 16。我们的目标是调整 `A` 和 `B`，使这个权重和最大。

*   **应用:**
    *   **趣味应用:** 假设有 `n` 个活动和 `m` 个人。每个人都想参加其中两项活动。你需要把所有活动安排在上午或下午，目标是让尽可能多的人能参加他们想参加的两项活动（即一项在上午，一项在下午）。
        *   建模：每个活动是一个节点。如果有人想同时参加活动 `u` 和 `v`，就在 `u` 和 `v` 之间连一条边。最大割就是将活动划分为上午(A)和下午(B)的最佳方案。
    *   **实际应用:** 电路设计（最小化不同模块间的连线）、统计物理等。

#### **6.2 最大割与局部搜索**

我们可以将最大割问题看作是 Hopfield 网络的一个特例：

*   将划分 `(A, B)` 对应于节点状态：`A` 中的节点状态为 `+1`，`B` 中的节点状态为 `-1`。
*   一条边 `(u,v)` 跨越割，当且仅当 `u` 和 `v` 在不同的集合中，即 $s\_u \\ne s\_v$。
*   为了最大化跨越割的边的权重，我们希望满足 $s\_u \\ne s\_v$ 这个约束。这对应于 Hopfield 网络中 $w\_{uv} > 0$ 的情况。
*   所以，最大割问题等价于一个**所有边权重都为正**的 Hopfield 网络，其目标是最大化 $\\sum\_{e \\text{ is good}} w\_e$。

我们可以使用与 Hopfield 网络相同的**状态翻转算法**（在这里称为 **single-flip**）：

*   **邻域:** 通过将一个节点从 `A` 移动到 `B`，或者从 `B` 移动到 `A`，来得到新的划分。
*   **算法:**
    1.  从一个任意划分 `(A, B)` 开始。
    2.  只要存在一个节点 `u`，移动它可以增加割的权重，就移动它。
    3.  直到没有节点可以通过移动来增加割的权重时停止。

这个简单的局部搜索算法，我们称之为 **Single-Flip Heuristic**。

**问题:**

1.  这个算法的运行时间是多项式吗？（同样，可能是伪多项式）
2.  它找到的局部最优解，质量如何？离全局最优解有多远？

#### **6.3 局部最优解的质量分析 (近似比)**

这是一个非常深刻和重要的结果。

**声明 (Claim):** 对于最大割问题，由 Single-Flip Heuristic 找到的任何一个局部最优划分 `(A, B)`，其割权重至少是全局最优划分 `(A*, B*)` 的一半。 即： $w(A, B) \\ge \\frac{1}{2} w(A^_, B^_)$ 这被称为 **2-近似算法**。

**证明 (Proof):** 设 `(A, B)` 是一个局部最优解。这意味着对于任何节点的移动，割的权重都不会增加。

1.  考虑任意一个在 `A` 中的节点 `u` ($u \\in A$)。如果我们将 `u` 从 `A` 移动到 `B`，割权重的变化是多少？
    
    *   原来与 `u` 相连，且另一端在 `B` 中的边（权重为 $\\sum\_{v \\in B} w\_{uv}$），现在不再跨越割。
    *   原来与 `u` 相连，且另一端在 `A` 中的边（权重为 $\\sum\_{v \\in A} w\_{uv}$），现在开始跨越割。
    *   移动 `u` 后割权重的增加量为: $\\Delta w = \\sum\_{v \\in A} w\_{uv} - \\sum\_{v \\in B} w\_{uv}$。
2.  因为 `(A, B)` 是局部最优，所以移动 `u` 不会增加割权重，即 $\\Delta w \\le 0$。 所以，对于任何 $u \\in A$，我们有： $$\\sum\_{v \\in A} w\_{uv} \\le \\sum\_{v \\in B} w\_{uv}$$
    
3.  我们将这个不等式对所有 $u \\in A$ 进行求和： $$\\sum\_{u \\in A} \\left( \\sum\_{v \\in A} w\_{uv} \\right) \\le \\sum\_{u \\in A} \\left( \\sum\_{v \\in B} w\_{uv} \\right)$$
    
4.  我们来分析这个不等式的两边：
    
    *   **右边:** $\\sum\_{u \\in A} \\sum\_{v \\in B} w\_{uv}$。这正是割 `(A, B)` 的权重定义，即 $w(A, B)$。
    *   **左边:** $\\sum\_{u \\in A} \\sum\_{v \\in A} w\_{uv}$。这个和计算了所有两个端点都在 `A` 内部的边的权重，但每条边 `(u, v)` 被计算了两次（一次在 `u` 的求和中，一次在 `v` 的求和中）。所以，这个和等于 $2 \\times w(A, A)$，其中 $w(A, A)$ 是 `A` 内部所有边的权重之和。
    
    所以我们得到： $2 \\cdot w(A, A) \\le w(A, B)$。
    
5.  同理，对于任何 $u \\in B$，移动它也不会增加割权重，我们可以得到： $2 \\cdot w(B, B) \\le w(A, B)$。
    
6.  将这两个不等式相加： $2 \\cdot w(A, A) + 2 \\cdot w(B, B) \\le 2 \\cdot w(A, B)$
    
    图中所有边的总权重 $W\_{total} = w(A,A) + w(B,B) + w(A,B)$。 将上面两个不等式相加，得到 $$2(w(A,A)+w(B,B)) \\le 2w(A,B)$$即 $$w(A,A)+w(B,B) \\le w(A,B)$$
    
    所以，$$W\_{total} = (w(A,A)+w(B,B)) + w(A,B) \\le w(A,B) + w(A,B) = 2w(A,B)$$
    
    因为全局最优解 $w(A^_,B^_)$ 不可能比所有边的总权重还大，所以 $$w(A^_,B^_) \\le W\_{total}$$
    
    因此，我们得到 $$w(A^_, B^_) \\le 2w(A, B)$$，或者写成 $$w(A, B) \\ge \\frac{1}{2} w(A^_, B^_)$$
    
    证明完毕。这个简单的局部搜索算法，保证能找到一个不差于最优解一半的答案！
    

#### **6.4 最大割近似算法的进展**

*   **\[Sahni-Gonzales 1976\]** 证明了存在一个2-近似算法（我们刚才证明的那个就是）。
*   **\[Goemans-Williamson 1995\]** 这是一个里程碑式的工作。他们使用**半定规划 (Semidefinite Programming)** 松弛技术，给出了一个 **1.1382-近似**算法（即 $1/0.878$）。这个结果非常深刻，打开了近似算法的新篇章。
*   **\[Håstad 1997\]** 他证明了，除非 P=NP，否则不存在比 **17/16 ≈ 1.0625** 更好的近似算法。这为最大割问题的近似性设置了一个理论上的上限。

#### **6.5 改进局部搜索算法**

**1\. 如何保证多项式时间？Big-improvement-flip**

Single-flip 算法的运行时间可能依赖于权重 `W`。为了让它成为真正的多项式时间算法，我们可以修改策略：不是任何一点改进都接受，而是只接受**足够大的改进**。

*   **Big-improvement-flip:** 只选择那些翻转后能让割权重至少增加 $\\frac{2\\epsilon}{|V|} w(A,B)$ 的节点进行翻转。
*   **Claim 1:** 算法终止时，能返回一个 $(2+\\epsilon)$-近似解。即 $(2+\\epsilon)w(A,B) \\ge w(A^_,B^_)$。
*   **Claim 2:** 该算法在 $O(\\frac{n}{\\epsilon} \\log W)$ 次翻转后就会终止，这是一个多项式时间算法。

**2\. 如何获得更好的解？尝试更好的`local`（邻域）**

Single-flip 的邻域太小了，容易陷入质量不高的局部最优。我们可以扩大邻域。

*   **邻域大小的权衡:**
    *   **邻域太小:** 容易陷入局部最优。
    *   **邻域太大:** 搜索邻域本身的时间开销巨大。
*   **k-flip:** 允许一次移动 `k` 个节点。邻域大小为 $O(n^k)$，当 `k` 稍大时就不可行了。
*   **Kernighan-Lin (KL) 启发式算法 \[1970\]:** 这是一个非常著名且有效的图划分算法，它的邻域定义非常巧妙。
    *   它不是一次只移动一个节点，而是进行一系列的节点交换。
    *   **Step 1:** 找到一个能带来最大收益的单次翻转（将节点 $v\_1$ 移动），即使这个收益是负的。然后“锁定” $v\_1$。
    *   **Step k:** 在剩下未锁定的节点中，找到一个能带来最大收益的单次翻转（将节点 $v\_k$ 移动），然后锁定 $v\_k$。
    *   这个过程一直持续到所有节点都被翻转过一次，我们就得到了一系列的中间划分。
    *   最后，在这一系列划分中，选择那个割权重最大的作为本次迭代的结果。
    *   整个过程的邻域大小是 $O(n^2)$，比简单的 k-flip 要高效得多。