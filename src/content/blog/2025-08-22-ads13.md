---
title: "ads13:随机算法 (Randomized Algorithms)"
description: ""
pubDate: "2025-08-22"
heroImage: ""
---

# ads13:随机算法 (Randomized Algorithms)

### 1\. 什么是随机？我们应该对什么进行随机化？

在算法分析中，我们经常会遇到“随机”这个词，但它可能出现在两个截然不同的语境中。我们必须首先厘清这个核心概念。

#### 1.1 平均情况分析 (Average-case Analysis)

想象一下，我们有一台性能稳定的汽车（一个**确定性算法**）。它的性能好坏，完全取决于路况（**算法的输入**）。如果路况总是很好（随机生成的好输入），那这台车跑得就很快。但如果路况很差（最坏情况输入），它就可能堵在路上动弹不得。

这就是**平均情况分析**： **世界是随机的** —— 我们假设算法的输入是随机生成的，并分析一个**传统确定性算法**在这些随机输入下的期望表现。

这种分析的优点是能告诉我们算法在“通常”情况下的表现。但它有一个巨大的弱点：它依赖于对输入分布的假设。如果现实世界中的输入不符合我们假设的随机分布，那么分析结果就毫无意义。攻击者可以特意构造一个“最坏情况”的输入，让我们的算法性能急剧下降。

graph TD subgraph 平均情况分析 A\[随机生成的世界/输入\] --> B{确定性算法}; B --> C\[性能表现\]; end style A fill:#f9f,stroke:#333,stroke-width:2px

#### 1.2 随机算法 (Randomized Algorithms)

现在，我们换一种思路。我们不再对路况做任何假设，我们承认路况可能非常糟糕（**最坏情况输入**）。但是，我们给汽车装上一个神奇的导航系统，它在每个路口都会**随机选择**一条路走（**算法本身是随机的**）。虽然某一次选择可能很糟糕，但从期望上来看，它总能避开最拥堵的路线，快速到达目的地。

这就是**随机算法**的核心思想： **算法是随机的** —— 算法在执行过程中会做出一些随机决策（比如抛硬币）。我们分析的是，对于**任意给定的、甚至是“最坏”的输入**，算法的期望表现。

这里的关键在于，随机性掌握在我们自己手中，而不是依赖于外界输入。无论输入有多么“恶意”，我们的算法都能通过内部的随机选择，大概率地避免最坏情况的发生。这就赋予了算法一种强大的鲁棒性。

graph TD subgraph 随机算法 D\[最坏情况的世界/输入\] --> E{随机化算法}; E --> F\[期望性能表现\]; end style E fill:#ccf,stroke:#333,stroke-width:2px

我们讨论的，就是第二种，即算法本身是随机的。

### 2\. 我们为什么要拥抱随机？

引入随机性听起来似乎让算法变得不可靠了，但实际上它带来了诸多好处，甚至在很多场景下是必不可少的。

1.  **性能与正确性的权衡**：
    
    *   一个高效的确定性算法必须在所有情况下都给出正确答案。而一个**高效的随机算法**可能只需要**以高概率**给出正确答案。这种放宽可以带来巨大的性能提升。这类算法被称为**蒙特卡洛算法 (Monte Carlo Algorithms)**。
    *   或者，一个随机算法可以**保证答案永远正确**，但其运行时间是随机的，我们只保证它的**期望运行时间**是高效的。这类算法被称为**拉斯维加斯算法 (Las Vegas Algorithms)**。我们今天主要讨论这类算法。
2.  **对称性破缺 (Symmetry-breaking)**：
    
    *   在分布式系统中，想象有两个完全相同的进程需要竞争同一个资源。如果它们的行为是确定性的，它们可能会永远同步地尝试、失败、再尝试、再失败，陷入死锁。引入随机的等待时间可以有效地打破这种对称性，让其中一个进程获得资源。
3.  **更简洁的设计**：
    
    *   在很多情况下，随机算法的设计和实现比解决同样问题的确定性算法要简单得多。我们即将看到的快速排序就是一个绝佳的例子。

### 3\. 概率论基础快览

在我们深入具体的例子之前，让我们快速回顾一下几个核心的概率论概念，它们是我们分析随机算法的基石。

*   **概率 (Probability)**: 对于一个事件 $A$，我们用 $\\Pr\[A\]$ 来表示它发生的概率。$0 \\le \\Pr\[A\] \\le 1$。
    
*   **补事件 (Complementary Event)**: 事件 $\\bar{A}$ 表示事件 $A$ **不发生**。它们的概率关系是： $$\\Pr\[A\] + Pr\[\\bar{A}\] = 1$$
    
*   **随机变量 (Random Variable)**: 一个随机变量（比如 $X$）是一个其值由随机事件决定的变量。例如，掷一次骰子的点数就是一个随机变量。
    
*   **期望 (Expectation)**: 随机变量 $X$ 的期望，记作 $E\[X\]$，是它的“平均值”。如果我们多次重复这个随机过程， $E\[X\]$ 就是我们期望得到的 $X$ 的平均值。对于一个取非负整数值的随机变量 $X$，其计算公式为： $$E\[X\] = \\sum\_{j=0}^{\\infty} j \\cdot Pr\[X = j\]$$ 这个公式的直观理解是：将每个可能的取值 $j$ 与其出现的概率 $\\Pr\[X=j\]$ 相乘，然后求和。
    
*   **期望的线性性质 (Linearity of Expectation)**: 这是随机算法分析中**最重要**的性质之一！无论随机变量 $X$ 和 $Y$ 是否独立，总有： $$E\[X+Y\] = E\[X\] + E\[Y\]$$ 这个性质可以推广到任意多个随机变量。它允许我们把一个复杂的随机变量分解成若干个简单的随机变量，分别计算它们的期望，再相加得到总的期望。
    

### 4\. 案例分析一：招聘问题 (The Hiring Problem)

这是一个非常经典的例子，它完美地展示了从平均情况分析到随机算法的思维转变。

#### 4.1 问题描述

*   **场景**: 你需要通过猎头公司招聘一名新的办公室助理。
*   **流程**: 猎头公司每天会推荐一位候选人，总共有 $N$ 位候选人，持续 $N$ 天。你每天都需要面试一位。
*   **决策**: 面试后，如果你觉得这位候选人比你当前雇佣的助理更优秀，你就会解雇当前的助理，然后雇佣这位新的候选人。如果你觉得不如当前的，就什么都不做。为了简化问题，我们假设没有两位候选人的能力是完全相同的。
*   **成本**:
    *   面试成本: $C\_i$ (interview cost)，每次面试都要支付，相对较低。
    *   雇佣成本: $C\_h$ (hiring cost)，每次雇佣新人时支付（包括解雇旧人、办理入职等），非常高。即 $C\_i \\ll C\_h$。
*   **目标**: 我们不分析算法的运行时间，而是分析整个过程的总成本。

假设我们总共面试了 $N$ 个人，雇佣了 $M$ 个人。那么总成本可以表示为： **Total Cost** = $N \\cdot C\_i + M \\cdot C\_h$ 由于面试成本 $N \\cdot C\_i$ 是固定的，我们的分析重点是最小化雇佣成本，也就是最小化雇佣次数 $M$。

#### 4.2 朴素的确定性算法

这个问题的算法流程是固定的，非常简单：

```
// 伪代码
int Hiring(Candidate C[], int N) {
    // 为了方便，我们假设一个能力值为0的虚拟候选人0作为初始员工
    int best_so_far_id = 0;
    int best_so_far_quality = 0; // 假设质量是数值，越高越好
    
    for (int i = 1; i <= N; i++) {
        // 面试第 i 位候选人，产生面试成本 Ci
        int current_quality = interview(C[i]); 
        
        if (current_quality > best_so_far_quality) {
            // 发现一个更好的人，雇佣他！产生雇佣成本 Ch
            best_so_far_quality = current_quality;
            best_so_far_id = i;
            hire(C[i]);
        }
    }
    return best_so_far_id;
}
```

现在我们来分析这个确定性算法的成本。

*   **最好情况**: 最优秀的候选人第一天就来了。之后再也没有人比他更优秀。我们只雇佣1次。总成本为 $N \\cdot C\_i + 1 \\cdot C\_h$。
*   **最坏情况**: 候选人按能力**严格递增**的顺序到来。第一天来的最差，第二天来的第二差，...，最后一天来的最好。这样一来，我们每次面试完都会发现新人比旧人好，所以我们每次都会雇佣！总共雇佣了 $N$ 次。 总成本为 $O(N \\cdot C\_i + N \\cdot C\_h)$。由于 $C\_h$ 很高，这个成本是无法接受的。

这个最坏情况是由输入的顺序决定的。如果有人能恶意安排候选人的顺序，我们的成本就会非常高。

#### 4.3 平均情况分析：假设输入是随机的

现在，我们做一个美好的假设：**这 $N$ 位候选人是以完全随机的顺序出现的**。也就是说，任何一种排列顺序的概率都是 $1/N!$。在这种假设下，我们来计算期望的雇佣次数 $E\[M\]$。

我们用一个强大的工具——**指示器随机变量 (Indicator Random Variables)**。

*   令 $X$ 为我们总的雇佣次数（也就是上面说的 $M$）。
*   我们定义 $N$ 个指示器随机变量 $X\_1, X\_2, \\ldots, X\_N$，其中： $$X\_i = \\begin{cases} 1 & \\text{如果第 i 位候选人被雇佣} \\\\ 0 & \\text{如果第 i 位候选人未被雇佣} \\end{cases}$$

那么，总的雇佣次数 $X = \\sum\_{i=1}^{N} X\_i$。 根据期望的线性性质，我们有： $$E\[X\] = E\\left\[\\sum\_{i=1}^{N} X\_i\\right\] = \\sum\_{i=1}^{N} E\[X\_i\]$$

对于一个指示器随机变量，它的期望等于它所指示的事件发生的概率。所以： $$E\[X\_i\] = 1 \\cdot Pr\[\\text{第 i 位候选人被雇佣}\] + 0 \\cdot Pr\[\\text{第 i 位候选人未被雇佣}\] = Pr\[\\text{第 i 位候选人被雇佣}\]$$

现在，问题的核心变成了：**第 $i$ 位候选人被雇佣的概率是多少？**

第 $i$ 位候选人被雇佣的**充要条件**是：他比前面 $i-1$ 位所有候选人都优秀。 由于我们假设所有 $N$ 个候选人的到来顺序是完全随机的，那么对于前 $i$ 位候选人，他们的任意排列也是完全随机的。在这 $i$ 个候选人中，**最优秀的那个人**出现在第1位、第2位、...、第 $i$ 位的概率是均等的，都是 $1/i$。

只有当这 $i$ 个人中最优秀的那个人恰好出现在第 $i$ 个位置时，第 $i$ 位候选人才会被雇佣。 因此，$\\Pr\[\\text{第 i 位候选人被雇佣}\] = \\frac{1}{i}$。

现在我们可以计算总的期望雇佣次数了： $$E\[X\] = \\sum\_{i=1}^{N} E\[X\_i\] = \\sum\_{i=1}^{N} Pr\[\\text{第 i 位候选人被雇佣}\] = \\sum\_{i=1}^{N} \\frac{1}{i}$$

这个级数 $\\sum\\limits\_{i=1}^{N} \\frac{1}{i}$ 被称为**调和级数 (Harmonic Series)**，记作 $H\_N$。它有一个非常著名的近似： $$H\_N = \\ln(N) + \\gamma + O(1/N)$$其中 $\\gamma \\approx 0.577$ 是欧拉-马歇罗尼常数。 $E\[X\] \\approx \\ln(N)$。

在随机输入的假设下，期望总成本为 $$O(N \\cdot C\_i + \\ln(N) \\cdot C\_h)$$ 这是一个非常好的结果！例如，如果有10000个候选人，$\\ln(10000) \\approx 9.2$，我们平均只需要雇佣9-10次，而不是最坏情况的10000次。

但是，这个分析的致命弱点在于它依赖“输入是随机的”这个假设。

#### 4.4 随机算法：自己创造随机性！

现在我们进入随机算法的领域。我们不再祈祷输入的顺序是随机的，而是**我们自己动手，在算法内部将输入序列随机化**。

```
// 伪代码
int RandomizedHiring(Candidate C[], int N) {
    // 关键步骤：在开始之前，随机打乱候选人数组C！
    RandomlyPermute(C); // 这步是新加的
    
    // 后面的代码和朴素算法完全一样
    int best_so_far_id = 0;
    int best_so_far_quality = 0;
    
    for (int i = 1; i <= N; i++) {
        int current_quality = interview(C[i]);
        if (current_quality > best_so_far_quality) {
            best_so_far_quality = current_quality;
            best_so_far_id = i;
            hire(C[i]);
        }
    }
    return best_so_far_id;
}
```

我们只是在最开始增加了一步 `RandomlyPermute`。这一步的意义是革命性的：

*   **我们不再依赖于输入的分布**。无论输入的候选人顺序是按能力递增、递减还是任何其他恶意构造的顺序，经过我们第一步的随机打乱后，它都会变成一个随机排列。
*   因此，对于**任何输入**，后续算法所面对的序列都是随机的。这意味着我们刚才的平均情况分析，现在变成了对这个随机算法在**任何输入下**的**期望性能分析**。

随机化这一步，将“运气”从外部世界拉到了我们自己手中。我们保证了算法的期望性能，使其不再受恶意输入的攻击。

#### 4.5 如何实现随机排列？

那么，`RandomlyPermute` 该如何实现呢？

**方法一：基于排序的排列 (Permute-By-Sorting)**

这是一个很直观的方法：

1.  为数组中的每个元素 $A\[i\]$ 分配一个随机的优先级 $P\[i\]$。
2.  根据这些优先级对数组 $A$ 进行排序。

```
// 伪代码
void PermuteBySorting(Element A[], int N) {
    for (int i = 1; i <= N; i++) {
        // 分配一个足够大的范围内的随机数以避免冲突
        A[i].priority = rand() % (N*N*N);
    }
    // 使用任何 O(N log N) 的排序算法，根据 priority 排序
    Sort(A, based_on_priority);
}
```

**为什么优先级范围是 $N^3$？** 这是为了极大地降低优先级冲突的概率。根据生日悖论，如果我们在 $M$ 个数中随机选 $N$ 个数，当 $N \\approx \\sqrt{M}$ 时，冲突概率就会显著上升。为了让冲突概率非常小（例如小于 $1/N$），我们需要 $M \\gg N^2$。选择 $N^3$ 是一个比较保险和方便的选择。如果所有优先级都是唯一的，这个方法可以产生一个**均匀随机排列**（即每种排列出现的概率都是 $1/N!$）。

**方法二：Fisher-Yates (或 Knuth) Shuffle**

这是一个更高效、原地（in-place）的算法，时间复杂度为 $O(N)$。 思想是：从后往前（或从前往后）遍历数组，对于当前位置 $i$，从前面的 $i$ 个元素中（包括自己）随机选一个，与当前元素交换。

```cpp
// C++ 实现 Fisher-Yates Shuffle
#include <vector>
#include <random>
#include <algorithm> // for std::swap

template<typename T>
void RandomlyPermute(std::vector<T>& arr) {
    int n = arr.size();
    // 使用更好的随机数生成器
    std::random_device rd;
    std::mt19937 gen(rd());

    for (int i = n - 1; i > 0; --i) {
        std::uniform_int_distribution<> distrib(0, i);
        int j = distrib(gen); // 在 [0, i] 范围内随机选一个索引
        std::swap(arr[i], arr[j]);
    }
}
```

这个算法是目前生成随机排列的标准做法，因为它既高效又保证了均匀性。

### 5\. 案例分析二：在线招聘问题 (Online Hiring Problem / Secretary Problem)

现在我们来看一个招聘问题的变种，它更具挑战性，也是一个非常著名的决策问题。

#### 5.1 问题描述

*   **规则变化**: 你仍然要从 $N$ 个随机排列的候选人中选出最好的一个。
*   **核心约束**:
    1.  你只能雇佣**一个人**。
    2.  对于每个候选人，面试后你必须**立即决定**是否雇佣他。如果决定不雇佣，你以后就再也不能回头选择他了。
    3.  如果你雇佣了一个人，过程立刻结束。
*   **目标**: 设计一个策略，使得你成功雇佣到**最好的那一位**候选人的概率最大化。

这是一个典型的“在线”问题，充满了不确定性。如果你太早做决定，可能会选到一个 mediocre 的人；如果你太晚做决定，可能最好的那个人已经被你错过了。

#### 5.2 最佳策略

一个非常著名且效果惊人的策略如下：

1.  确定一个参数 $k$，其中 $1 < k < N$。
2.  **考察阶段**: 面试前 $k$ 位候选人，但**一律不雇佣**。只记住这 $k$ 个人中能力最强的人，我们称他的能力值为 `best_quality_in_first_k`。
3.  **选择阶段**: 从第 $k+1$ 位候选人开始，只要遇到一个比 `best_quality_in_first_k` 更优秀的人，就**立即雇佣他**，然后停止过程。
4.  如果面试完所有 $N$ 个人都没有找到比 `best_quality_in_first_k` 更好的人，说明最好的候选人就在前 $k$ 个人当中（我们已经错过了），那么策略失败。为了完整性，我们只能被迫雇佣第 $N$ 个人。

```
// 伪代码
int OnlineHiring(Candidate C[], int N, int k) {
    // 假设 C 已经是随机排列
    int best_quality_in_first_k = -1; // 假设质量为非负数
    // 1. 考察阶段
    for (int i = 1; i <= k; i++) {
        int quality = interview(C[i]);
        if (quality > best_quality_in_first_k) {
            best_quality_in_first_k = quality;
        }
    }
    
    // 2. 选择阶段
    for (int i = k + 1; i <= N; i++) {
        int quality = interview(C[i]);
        if (quality > best_quality_in_first_k) {
            return hire(C[i]); // 立即雇佣并返回
        }
    }
    
    // 3. 策略失败，被迫雇佣最后一人
    return hire(C[N]);
}
```

现在的问题是：

1.  对于一个给定的 $k$，我们成功雇佣到最佳人选的概率是多少？
2.  如何选择最优的 $k$ 来最大化这个概率？

#### 5.3 成功概率分析

*   令 $S$ 表示我们成功雇佣到最佳人选的事件。
*   令 $S\_i$ 表示“第 $i$ 位候选人是最佳人选，并且我们成功雇佣了他”的事件。
*   显然，$S = \\bigcup\_{i=k+1}^{N} S\_i$，因为如果最佳人选在前 $k$ 位，我们肯定会错过他。这些 $S\_i$ 事件是互斥的（最佳人选只有一个），所以 $\\Pr\[S\] = \\sum\\limits\_{i=k+1}^{N} \\Pr\[S\_i\]$。

现在我们来计算 $\\Pr\[S\_i\]$。事件 $S\_i$ 发生需要两个条件同时成立：

*   **事件 A**: 第 $i$ 位候选人是所有 $N$ 个人中最好的。
*   **事件 B**: 我们没有在第 $k+1$ 到 $i-1$ 位之间雇佣任何人。

这两个事件是**独立**的。为什么？

*   事件 A 的发生，只取决于所有 $N$ 个人的能力值的排列，具体来说，就是能力值最高的那个人被放在了第 $i$ 个位置。
*   事件 B 的发生，取决于在**前 $i-1$ 个人中**，能力最强的那个人是否出现在前 $k$ 个位置。
*   事件 A 的信息（第 $i$ 个人是全局最佳）并不影响前 $i-1$ 个人的内部相对排序。所以 A 和 B 独立。

我们来计算它们的概率：

*   $Pr\[A\]$: 由于是随机排列，最佳人选出现在任何位置的概率都是一样的。所以 $\\Pr\[A\] = \\frac{1}{N}$。
*   $Pr\[B\]$: 我们没有在 $k+1$ 到 $i-1$ 之间雇佣人，意味着从第 $k+1$ 位到第 $i-1$ 位的所有候选人，能力值都**没有超过**前 $k$ 位的最高水平。这等价于说，**在前 $i-1$ 个人中，最优秀的那个人出现在了前 $k$ 个位置**。因为在这 $i-1$ 个位置中，那个最优秀的人是随机分布的，所以他出现在前 $k$ 个位置的概率是 $\\frac{k}{i-1}$。

所以，$$\\Pr\[S\_i\] = \\Pr\[A \\cap B\] = \\Pr\[A\] \\cdot \\Pr\[B\] = \\frac{1}{N} \\cdot \\frac{k}{i-1}$$

现在我们可以计算总的成功概率了： $$\\Pr\[S\] = \\sum\_{i=k+1}^{N} \\Pr\[S\_i\] = \\sum\_{i=k+1}^{N} \\frac{1}{N} \\cdot \\frac{k}{i-1} = \\frac{k}{N} \\sum\_{i=k+1}^{N} \\frac{1}{i-1}$$ 为了方便，我们换元，令 $j = i-1$，则求和变为： $$\\Pr\[S\] = \\frac{k}{N} \\sum\_{j=k}^{N-1} \\frac{1}{j}$$

#### 5.4 求解与证明

**课堂Discussion: 证明积分不等式**

我们需要用积分来近似这个求和 $\\sum\_{j=k}^{N-1} \\frac{1}{j}$。 考虑函数 $f(x) = 1/x$。它是一个递减函数。

graph TD subgraph 积分与求和 A\[积分是曲线下面积\] --> B\[求和是矩形面积之和\]; end

我们可以用矩形面积来界定积分面积：

*   **下界**: 对于区间 $\[j, j+1\]$，矩形高度为 $1/(j+1)$，面积为 $1/(j+1)$。求和 $\\sum\_{j=k}^{N-1} \\frac{1}{j}$ 比积分 $\\int\_{k}^{N} \\frac{1}{x} dx$ 要大。更准确地说，$\\sum\_{j=k}^{N-1} \\frac{1}{j} = \\frac{1}{k} + \\sum\_{j=k+1}^{N-1} \\frac{1}{j}$。
*   **上界**: 对于区间 $\[j-1, j\]$，矩形高度为 $1/(j-1)$，面积为 $1/(j-1)$。求和 $\\sum\_{j=k}^{N-1} \\frac{1}{j}$ 比积分 $\\int\_{k-1}^{N-1} \\frac{1}{x} dx$ 要小。

一个更紧凑的界定是： $$\\int\_{k}^{N} \\frac{1}{x} dx \\le \\sum\_{j=k}^{N-1} \\frac{1}{j} \\le \\int\_{k-1}^{N-1} \\frac{1}{x} dx$$

计算积分： $$\\int \\frac{1}{x} dx = \\ln(x)$$ 所以， $$\\ln(N) - \\ln(k) \\le \\sum\_{j=k}^{N-1} \\frac{1}{j} \\le \\ln(N-1) - \\ln(k-1)$$ $$\\ln(\\frac{N}{k}) \\le \\sum\_{j=k}^{N-1} \\frac{1}{j} \\le \\ln(\\frac{N-1}{k-1})$$

代回到 $\\Pr\[S\]$ 的表达式中，我们得到： $$\\frac{k}{N} \\ln\\left(\\frac{N}{k}\\right) \\le \\Pr\[S\] \\le \\frac{k}{N} \\ln\\left(\\frac{N-1}{k-1}\\right)$$

**课堂Discussion: 找到最优的 $k$**

我们的目标是最大化成功概率 $\\Pr\[S\]$。当 $N$ 很大时，上下界非常接近，我们可以近似地认为 $\\Pr\[S\] \\approx \\frac{k}{N} \\ln(\\frac{N}{k})$。

我们令 $x = k/N$，问题就变成了求函数 $f(x) = x \\ln(\\frac{1}{x}) = -x \\ln(x)$ 在 $(0, 1)$ 区间上的最大值。 我们使用微积分来求解： 求导数 $f'(x)$: $$f'(x) = \\frac{d}{dx}(-x \\ln x) = -1 \\cdot \\ln x - x \\cdot \\frac{1}{x} = -\\ln x - 1$$ 令导数等于0，求极值点： $$f'(x) = 0 \\implies -\\ln x - 1 = 0 \\implies \\ln x = -1 \\implies x = e^{-1} = \\frac{1}{e}$$

所以当 $x = k/N = 1/e$ 时，函数取最大值。 这意味着最优的策略是选择 $$k \\approx N/e$$ 将 $x=1/e$ 代回 $f(x)$，我们得到最大概率： $$\\Pr\[S\]\_{max} \\approx f(1/e) = -\\frac{1}{e} \\ln(\\frac{1}{e}) = -\\frac{1}{e} (-1) = \\frac{1}{e}$$

**结论**:

*   最优策略是先拒绝前 $N/e$ 的候选人，然后选择第一个比他们都好的人。
*   这个策略能成功雇佣到最佳人选的概率约为 $1/e \\approx 0.37$。
*   这是一个非常惊人的结果！无论 $N$ 有多大（100人还是100万人），我们都有大约 37% 的机会选到最好的那个人。

### 6\. 案例分析三：随机化快速排序 (Randomized Quicksort)

快速排序是大家非常熟悉的算法，但它的确定性版本有一个致命弱点。

#### 6.1 确定性快速排序的问题

*   **工作原理**: 选择一个主元 (pivot)，将数组划分为小于主元和大于主元两部分，然后递归地对两部分进行排序。
*   **性能**:
    *   **平均情况**: $O(N \\log N)$，前提是输入是随机排列的。
    *   **最坏情况**: $O(N^2)$。当每次选择的主元都极度不平衡时发生。例如，如果对一个**已经排好序**的数组，每次都选择第一个或最后一个元素作为主元，那么每次划分都会产生一个大小为0和一个大小为 $n-1$ 的子问题，导致递归深度为 $N$，时间复杂度为 $O(N^2)$。

同样，这个问题又回到了算法性能依赖于输入上。

#### 6.2 随机化

如何解决这个问题？很简单，**随机选择主元**！ 在划分之前，我们不再固定地选择第一个或最后一个元素，而是在当前子数组中**随机均匀地**选择一个元素作为主元。

这个小小的改动带来了巨大的好处：

*   现在，对于**任何输入**（即使是已经排好序的），由于我们的主元是随机选择的，我们期望每次划分都比较平衡。
*   我们不再有特定的“最坏情况输入”，因为算法的表现只取决于内部的随机数生成器，而不是输入数据的排列。
*   随机化快速排序的**期望运行时间**是 $O(N \\log N)$。

#### 6.3 性能分析

我们用一个稍微不同的角度来分析，这个角度在幻灯片中被提及。

*   **中心分割点 (Central Splitter)**: 我们定义一个“好”的主元为“中心分割点”。一个主元是中心分割点，如果它划分后的两个子数组，大小都**至少是原数组大小的 1/4**。
    
    *   在排好序的数组中，哪些元素是中心分割点？中间那 $N/2$ 个元素（从第 N/4 个到第 3N/4 个）。
    *   因此，如果我们随机选一个主元，选到中心分割点的概率是 $\\frac{N/2}{N} = \\frac{1}{2}$。
*   **找到中心分割点的期望次数**:
    
    *   这就像抛硬币，每次有 $1/2$ 的概率正面朝上（找到中心分割点）。
    *   这是一个几何分布。为了得到一次成功，期望的试验次数是 $1/p$。在这里，$p=1/2$，所以期望次数是 $1/(1/2) = 2$ 次。
    *   **结论**: 我们期望只需要进行2次划分（每次成本 $O(n)$）就能找到一个中心分割点，得到一个“好”的划分。
*   **期望运行时间分析**:
    
    *   当我们得到一个中心分割点时，最坏的情况是划分成 $1/4$ 和 $3/4$。
    *   递归的深度将由最长的路径决定，即每次都沿着 $3/4$ 的分支走。
    *   递归树的深度为 $\\log\_{4/3}(N) = O(\\log N)$。
    *   在递归树的每一层，所有子问题的总大小是 $O(N)$，所以每一层的总划分成本是 $O(N)$。
    *   总的期望运行时间 = (层数) $\\times$ (每层成本) = $O(\\log N) \\cdot O(N) = O(N \\log N)$。

这个分析虽然不是最严格的，但它清晰地展示了随机化如何保证了划分的平衡性（在期望意义上），从而保证了 $O(N \\log N)$ 的高效性能。

### 7\. 拓展内容：跳表 (Skip Lists)

随机化不仅能优化现有算法，还能创造出全新的、优雅的数据结构。**跳表 (Skip List)** 就是一个完美的例子。

#### 7.1 动机

*   **有序链表**: 插入简单，但查找慢，平均 $O(N)$。
*   **平衡二叉搜索树 (如AVL树, 红黑树)**: 查找、插入、删除都是 $O(\\log N)$，但实现复杂，需要旋转等操作来维持平衡。

有没有一种数据结构，能像平衡树一样快，但实现起来像链表一样简单？跳表就是答案。

#### 7.2 结构

跳表是在有序链表的基础上构建的。它是一个多层的链表结构：

*   **第0层**: 一个包含所有元素的标准有序链表。
*   **第1层**: 一个包含第0层部分元素的“快速通道”链表。
*   **第i层**: 包含第 $i-1$ 层部分元素的“更快的通道”链表。

graph TD subgraph Skip List direction LR H3(H) --> L3\_1(30) --> T3(T); H2(H) --> L2\_1(10) --> L2\_2(30) --> L2\_3(50) --> T2(T); H1(H) --> L1\_1(10) --> L1\_2(20) --> L1\_3(30) --> L1\_4(40) --> L1\_5(50) --> T1(T); subgraph Level 3 H3 --- L3\_1 --- T3; end subgraph Level 2 H2 --- L2\_1 --- L2\_2 --- L2\_3 --- T2; end subgraph Level 1 H1 --- L1\_1 --- L1\_2 --- L1\_3 --- L1\_4 --- L1\_5 --- T1; end H3 -- down --> H2; H2 -- down --> H1; L3\_1 -- down --> L2\_2; L2\_1 -- down --> L1\_1; L2\_2 -- down --> L1\_3; L2\_3 -- down --> L1\_5; T3 -- down --> T2; T2 -- down --> T1; end

#### 7.3 随机化在这里的作用

一个节点如何决定它应该出现在哪些层？**抛硬币**！

*   当插入一个新节点时，它肯定会出现在第0层。
*   然后我们抛一次硬币，如果是正面，我们就将这个节点也插入到第1层。
*   如果第1层插入成功了，我们再抛一次硬币，如果是正面，就继续插入到第2层。
*   这个过程一直持续，直到我们抛出反面，或者达到了预设的最大层数。

这样一来，每一层的节点数大约是下一层的一半，整个结构在期望上是平衡的。

#### 7.4 操作

*   **查找 (Search)**:
    
    1.  从最高层的头节点开始。
    2.  在当前层向右移动，直到下一个节点大于或等于目标值。
    3.  从当前节点下降一层。
    4.  重复步骤2和3，直到到达第0层。
    5.  在第0层找到的最终位置，就是目标元素的位置（如果存在的话）。
*   **插入 (Insert)**:
    
    1.  查找待插入位置，并记录下每层经过的路径。
    2.  在第0层插入新节点。
    3.  开始抛硬币，决定新节点的“高度”。
    4.  对于每一个正面，就在对应层利用之前记录的路径插入新节点。
*   **删除 (Delete)**:
    
    1.  查找待删除元素，并记录路径。
    2.  在所有包含该元素的层中，利用路径信息将其删除。

#### 7.5 性能与实现

*   **性能**: 因为层数在期望上是 $O(\\log N)$，每层查找的步数在期望上是常数，所以查找、插入、删除的**期望时间复杂度都是 $O(\\log N)$**。
*   **实现**: 跳表的代码实现比红黑树等要简单得多，因为它不涉及复杂的旋转操作，只需要链表指针的操作。

**C++ 实现框架**:

```cpp
#include <iostream>
#include <vector>
#include <random>
#include <memory>

// 跳表节点
template<typename T>
struct SkipNode {
    T value;
    std::vector<std::shared_ptr<SkipNode<T>>> forward; // 指向各层下一个节点的指针

    SkipNode(T val, int level) : value(val), forward(level + 1, nullptr) {}
};

template<typename T>
class SkipList {
private:
    int max_level;
    float probability;
    int current_level;
    std::shared_ptr<SkipNode<T>> header;
    std::mt19937 gen; // 随机数生成器

    // 随机生成新节点的层高
    int random_level() {
        int lvl = 0;
        std::uniform_real_distribution<> dis(0.0, 1.0);
        while (dis(gen) < probability && lvl < max_level) {
            lvl++;
        }
        return lvl;
    }

public:
    SkipList(int max_lvl = 16, float p = 0.5) 
        : max_level(max_lvl), probability(p), current_level(0) {
        
        std::random_device rd;
        gen.seed(rd());
        // 头节点，可以存一个最小值
        header = std::make_shared<SkipNode<T>>(T(), max_level); 
    }

    // 搜索
    bool search(T target) {
        auto current = header;
        for (int i = current_level; i >= 0; i--) {
            while (current->forward[i] && current->forward[i]->value < target) {
                current = current->forward[i];
            }
        }
        current = current->forward[0];
        return current && current->value == target;
    }

    // 插入
    void insert(T value) {
        std::vector<std::shared_ptr<SkipNode<T>>> update(max_level + 1, nullptr);
        auto current = header;

        for (int i = current_level; i >= 0; i--) {
            while (current->forward[i] && current->forward[i]->value < value) {
                current = current->forward[i];
            }
            update[i] = current;
        }

        current = current->forward[0];

        // 如果值已存在，则不插入
        if (current && current->value == value) {
            return;
        }

        int new_level = random_level();
        if (new_level > current_level) {
            for (int i = current_level + 1; i <= new_level; i++) {
                update[i] = header;
            }
            current_level = new_level;
        }

        auto new_node = std::make_shared<SkipNode<T>>(value, new_level);
        for (int i = 0; i <= new_level; i++) {
            new_node->forward[i] = update[i]->forward[i];
            update[i]->forward[i] = new_node;
        }
    }

    // 删除
    void remove(T value) {
        std::vector<std::shared_ptr<SkipNode<T>>> update(max_level + 1, nullptr);
        auto current = header;

        for (int i = current_level; i >= 0; i--) {
            while (current->forward[i] && current->forward[i]->value < value) {
                current = current->forward[i];
            }
            update[i] = current;
        }

        current = current->forward[0];

        if (current && current->value == value) {
            for (int i = 0; i <= current_level; i++) {
                if (update[i]->forward[i] != current) {
                    break;
                }
                update[i]->forward[i] = current->forward[i];
            }

            while (current_level > 0 && header->forward[current_level] == nullptr) {
                current_level--;
            }
        }
    }
};
```