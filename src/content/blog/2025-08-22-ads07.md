---
title: "ads07:分治法 (Divide and Conquer)"
description: ""
pubDate: "2025-08-22"
heroImage: ""
---

# ads07:分治法 (Divide and Conquer)

### 1\. 分治法的核心思想

分治，正如其名，即“分而治之”。当我们面对一个规模庞大、难以直接下手的复杂问题时，分治策略引导我们将其拆解。具体来说，它遵循一个递归的、三步走的过程：

1.  **分解 (Divide):** 将原始问题分解为若干个规模更小、但结构与原问题完全相同的子问题。
2.  **解决 (Conquer):** 递归地求解这些子问题。当子问题的规模小到足以轻松解决时（即达到递归的“基例”），便直接求解。
3.  **合并 (Combine):** 将各个子问题的解合并起来，最终构筑成原问题的解。

这种模式的运行时间，或者说时间复杂度，可以用一个统一的**递归关系式**来刻画：

$$T(N) = aT(N/b) + f(N)$$

我们来解读一下这个公式的每个部分：

*   $T(N)$ 代表解决规模为 $N$ 的问题所需的总时间。
*   $a$ 是分解后产生的子问题的数量（$a \\ge 1$）。
*   $N/b$ 是每个子问题的规模（其中 $b > 1$）。
*   $f(N)$ 代表了**分解**问题与**合并**子问题解这两个步骤所消耗的时间。

分治法的思想在算法的殿堂中熠熠生辉，许多我们熟知的经典算法都是其应用的典范：

*   **最大子序列和问题:** 存在一个复杂度为 $O(N \\log N)$ 的分治解法。
*   **树的遍历:** 无论是前序、中序还是后序遍历，其本质都是先处理根节点，然后对左右子树进行分治处理，总时间复杂度为 $O(N)$。
*   **归并排序 (Mergesort) 与 快速排序 (Quicksort):** 这两大排序算法是分治思想最杰出的代表，它们的平均时间复杂度都达到了高效的 $O(N \\log N)$。

### 2\. 案例研究：最近点对问题 (Closest Points Problem)

**问题描述：** 给定二维平面上的 $N$ 个点，请找出其中距离最近的两个点及其距离。 (特殊情况：若两点坐标完全相同，则它们是最近点对，距离为 0。)

#### 2.1 朴素解法：穷举搜索

最直接的思路是什么？暴力枚举。计算每对点之间的距离，然后找出其中的最小值。

对于 $N$ 个点，总共可以构成多少个点对呢？组合数学告诉我们是 $C(N, 2) = \\frac{N(N-1)}{2}$。 因此，我们需要进行 $O(N^2)$ 次的距离计算。这个方法虽然简单易懂，但当点的数量 $N$ 巨大时，其平方级的复杂度是不可接受的。

```
// 伪代码：穷举法
function BruteForceClosestPair(points):
  min_dist = infinity
  closest_pair = null
  for i from 0 to n-1:
    for j from i+1 to n-1:
      dist = distance(points[i], points[j])
      if dist < min_dist:
        min_dist = dist
        closest_pair = (points[i], points[j])
  return min_dist
```

#### 2.2 分治策略

$O(N^2)$ 显然不是我们追求的极致效率。现在，让我们请出分治法来优化这个问题。

1.  **分解 (Divide):**
    
    *   为了有效地“切分”点集，我们首先将所有点按照 $x$ 坐标进行全局排序。
    *   找到所有点 $x$ 坐标的中位数 $x\_{mid}$，并以此为基准画一条垂直的分割线 $L$。
    *   这条线 $L$ 将点集几乎均等地分为左右两个子集：左集 $P\_L$ 和右集 $P\_R$。
2.  **解决 (Conquer):**
    
    *   递归地在左子集 $P\_L$ 中寻找最近点对，得到其最短距离 $\\delta\_L$。
    *   递归地在右子集 $P\_R$ 中寻找最近点对，得到其最短距离 $\\delta\_R$。
    *   到目前为止，我们已经获得了两个半区内部的最小距离，令 $\\delta = \\min(\\delta\_L, \\delta\_R)$。
3.  **合并 (Combine):**
    
    *   这是整个算法最核心、最巧妙的部分。我们已经有了 $\\delta$，但真正的最近点对有没有可能**跨越**了分割线 $L$？也就是说，一个点来自 $P\_L$，另一个点来自 $P\_R$。
    *   如果存在这样一个“跨界”点对 $(p\_l, p\_r)$，并且它们的距离小于我们已知的 $\\delta$，那么这两个点必然满足一个重要的几何特性：它们各自到分割线 $L$ 的水平距离都必须小于 $\\delta$。
    *   为什么？可以反证。假设左侧的点 $p\_l$ 到 $L$ 的距离大于等于 $\\delta$，那么它与任何在 $L$ 右侧的点 $p\_r$ 的距离，仅在 $x$ 轴上的投影就已经大于等于 $\\delta$ 了，总距离必然也大于 $\\delta$，这种点对我们无需考虑。
    *   因此，我们的注意力可以聚焦于一个以 $L$ 为中心、宽度为 $2\\delta$ 的狭长\*\*“条带” (strip)\*\* 区域。所有可能构成更短距离的跨界点对，都必须位于这个条带内。

下面这幅流程图清晰地展示了这个过程：

graph TD subgraph A \["初始状态"\] P(\["原始点集 P"\]) end subgraph B \["1.分解 (Divide)"\] DivideProcess{"按 x 坐标排序并沿中线 L 分割"} end subgraph C \["2.解决 (Conquer) - 递归求解"\] direction LR subgraph C\_L \["左子问题"\] PL(\["左点集 P\_L"\]) RecurseL{"递归调用"} delta\_L\["距离 δ\_L"\] PL --> RecurseL --> delta\_L end subgraph C\_R \["右子问题"\] PR(\["右点集 P\_R"\]) RecurseR{"递归调用"} delta\_R\["距离 δ\_R"\] PR --> RecurseR --> delta\_R end end subgraph D \["3.合并 (Combine)"\] delta\_LR\["δ = min(δ\_L, δ\_R)"\] StripProcess{"1. 构建 2δ 宽度的条带  
2\. 筛选条带内的点  
3\. 按 y 排序并检查跨界点对"} delta\_cross\["跨界最小距离 δ\_cross"\] FinalMin\["最终结果 = min(δ, δ\_cross)"\] end P --> DivideProcess DivideProcess --> PL & PR delta\_L --> delta\_LR delta\_R --> delta\_LR delta\_LR -- "用 δ 定义条带" --> StripProcess StripProcess --> delta\_cross delta\_LR --> FinalMin delta\_cross --> FinalMin style P fill:#f9f,stroke:#333 style delta\_L fill:#ccf,stroke:#333 style delta\_R fill:#ccf,stroke:#333 style delta\_LR fill:#9f9,stroke:#333 style StripProcess fill:#ff9,stroke:#333 style FinalMin fill:#9f9,stroke:#333,stroke-width:2px

现在，核心问题转化为：**我们能否在线性时间 $O(N)$ 内，高效地处理这个条带区域，找出其中可能存在的更短的跨界点对？**

#### 2.3 $O(N)$ 时间复杂度的合并步骤

这里的洞察力至关重要。让我们来分析这个宽度为 $2\\delta$ 的条带。

1.  将所有位于该条带内的点收集到一个列表 $S$ 中。
2.  **将列表 $S$ 中的所有点，按照它们的 $y$ 坐标进行排序。**
3.  现在，我们遍历 $S$ 中的每一个点 $p$。对于点 $p$，我们需要和哪些点比较距离呢？
    *   我们只需要检查那些在 $y$ 坐标排序后紧随 $p$ 之后，并且与 $p$ 的 $y$ 坐标之差小于 $\\delta$ 的点。因为如果两点间仅 $y$ 坐标的差值就已经不小于 $\\delta$，那么它们之间的欧几里得距离必然也大于 $\\delta$。
    *   那么，在 $y$ 坐标排序的前提下，对于每个点 $p$，需要检查的后续点究竟有多少个？答案是惊人的：**一个常数！**

**证明：** 对于条带中的任意一点 $p$，我们只关心那些 $y$ 坐标在区间 $\[p.y, p.y + \\delta)$ 内的点 $q$。这些候选点 $q$ 必须位于一个尺寸为 $2\\delta \\times \\delta$ 的矩形区域内。更进一步，由于 $p$ 和 $q$ 分属不同半区，假设 $p$ 在左，那么 $q$ 必须位于右侧一个 $\\delta \\times \\delta$ 的矩形区域内。

现在思考一个问题：在一个 $\\delta \\times \\delta$ 的正方形区域内，最多能放入多少个点，才能保证它们两两之间的距离都不小于 $\\delta$？ 我们可以运用**鸽巢原理**。将这个 $\\delta \\times \\delta$ 的正方形划分为四个 $\\delta/2 \\times \\delta/2$ 的小正方形。每个小正方形的对角线长度是 $\\frac{\\delta}{\\sqrt{2}} < \\delta$。这意味着每个小正方形内最多只能容纳一个点。因此，这个 $\\delta \\times \\delta$ 区域内最多只能有4个点。

一个更严谨的几何证明表明，对于每个点 $p$，我们最多只需要检查其后（按 $y$ 坐标排序）的 **7** 个点就足够了。在实际编程中，这个常数通常更小。

graph TD subgraph "对于点 p 的检查区域" direction TB subgraph "右侧 δ x δ 区域" A\["(•)"\]; B\["(•)"\]; C\["(•)"\]; D\["(•)"\] end p("(• p)") -- "检查" --> A p -- "检查" --> B p -- "检查" --> C p -- "检查" --> D note\["在这个区域内, 最多只有少数几个点需要与 p 比较距离"\] end

**算法流程优化：** 合并步骤中包含一个 $y$ 坐标排序，这本身需要 $O(N \\log N)$ 的时间。如果每次递归都进行一次排序，总复杂度会变差。 **精妙的优化**在于：在进入递归之前，我们先创建两个点集副本，一个按 $x$ 坐标排序（$P\_x$），另一个按 $y$ 坐标排序（$P\_y$）。在每次递归调用时，我们可以通过对 $P\_y$ 进行一次线性扫描（$O(N)$），根据点的 $x$ 坐标是否小于 $x\_{mid}$，将 $P\_y$ 划分成对应左右子集的、已按 $y$ 排序的两个列表。这样就避免了在递归中反复排序，使得合并步骤的真正时间复杂度降至 $O(N)$。

#### 2.4 最终复杂度分析

经过优化后，最近点对分治算法的递归关系式为：

$$T(N) = 2T(N/2) + O(N)$$

这个关系式的解是 $T(N) = O(N \\log N)$（我们稍后会严格证明这一点）。 再加上初始的全局排序需要 $O(N \\log N)$，整个算法的总时间复杂度为 $O(N \\log N)$，这相较于 $O(N^2)$ 是一个巨大的飞跃。

#### 2.5 C++ 代码实现

```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <algorithm>
#include <iomanip>
#include <limits>

struct Point {
    double x, y;
};

// 计算两点间的欧几里得距离
double dist(Point p1, Point p2) {
    return std::sqrt(std::pow(p1.x - p2.x, 2) + std::pow(p1.y - p2.y, 2));
}

// 用于按x坐标排序的比较函数
bool compareX(const Point& a, const Point& b) {
    return a.x < b.x;
}

// 用于按y坐标排序的比较函数
bool compareY(const Point& a, const Point& b) {
    return a.y < b.y;
}

// 朴素解法，用于处理递归基例（点数很少时）
double bruteForce(const std::vector<Point>& points_by_x, int left, int right) {
    double min_dist = std::numeric_limits<double>::max();
    for (int i = left; i <= right; ++i) {
        for (int j = i + 1; j <= right; ++j) {
            min_dist = std::min(min_dist, dist(points_by_x[i], points_by_x[j]));
        }
    }
    return min_dist;
}

// 分治算法的核心递归函数
double closestUtil(std::vector<Point>& points_by_x, std::vector<Point>& points_by_y, int left, int right) {
    // 递归基例：当点数很少时，直接用朴素解法
    if (right - left + 1 <= 3) {
        // 注意：这里需要对 points_by_x 的子集进行操作
        return bruteForce(points_by_x, left, right);
    }

    // 1. 分解 (Divide)
    int mid_idx = left + (right - left) / 2;
    Point mid_point = points_by_x[mid_idx];

    // 将按y排序的数组线性地划分为左右两部分
    std::vector<Point> left_y, right_y;
    for (const auto& p : points_by_y) {
        if (p.x <= mid_point.x && (p.x != mid_point.x || p.y != mid_point.y)) { // 避免重复点
            left_y.push_back(p);
        } else {
            right_y.push_back(p);
        }
    }

    // 2. 解决 (Conquer)
    double dl = closestUtil(points_by_x, left_y, left, mid_idx);
    double dr = closestUtil(points_by_x, right_y, mid_idx + 1, right);
    double d = std::min(dl, dr);

    // 3. 合并 (Combine)
    // 筛选出在2d条带区域内的点
    std::vector<Point> strip;
    for (const auto& p : points_by_y) {
        if (std::abs(p.x - mid_point.x) < d) {
            strip.push_back(p);
        }
    }

    // 检查条带内的点对，寻找更小的距离
    for (size_t i = 0; i < strip.size(); ++i) {
        // 对于每个点，只需检查其后常数个点
        for (size_t j = i + 1; j < strip.size() && (strip[j].y - strip[i].y) < d; ++j) {
            d = std::min(d, dist(strip[i], strip[j]));
        }
    }

    return d;
}

// 主函数，负责初始排序和调用递归
double closestPair(std::vector<Point>& points) {
    if (points.size() < 2) return std::numeric_limits<double>::max();
    
    std::vector<Point> points_by_x = points;
    std::vector<Point> points_by_y = points;

    std::sort(points_by_x.begin(), points_by_x.end(), compareX);
    std::sort(points_by_y.begin(), points_by_y.end(), compareY);

    return closestUtil(points_by_x, points_by_y, 0, points.size() - 1);
}

int main() {
    std::vector<Point> points = {{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}};
    std::cout << "The smallest distance is " << std::fixed << std::setprecision(6) << closestPair(points) << std::endl;
    return 0;
}
```

* * *

### 3\. 如何求解递归关系式？

我们已经看到，分析分治算法的效率最终归结为求解形如 $T(N) = aT(N/b) + f(N)$ 的递归式。主要有三种方法来攻克它们：

1.  **代入法 (Substitution Method):** 猜测一个解的形式，然后用数学归纳法来严格证明。
2.  **递归树法 (Recursion-tree Method):** 将递归过程可视化为一棵树，通过累加树中所有节点的代价来求得最终解。
3.  **主方法 (Master Method):** 提供一个强大的“菜谱式”定理，能直接解决大部分常见形式的递归式。

在分析中，我们通常做一些简化假设，例如忽略 $N/b$ 是否为整数（即地板 `floor` 和天花板 `ceiling` 的影响），并假设对于足够小的 $n$， $T(n) = \\Theta(1)$。

#### 3.1 代入法：猜想与证明

此方法分为两步：**猜测解** 和 **归纳证明**。

**【例1】** $T(N) = 2T(N/2) + N$ (归并排序与优化后的最近点对问题的递归式)

*   **猜测:** $T(N) = O(N \\log N)$。
    
*   **证明:** 我们需要证明存在正常数 $c$ 和 $N\_0$，使得对所有 $N \\ge N\_0$，不等式 $T(N) \\le cN \\log N$ 成立。
    
    **归纳假设:** 假设对于所有 $m < N$， $T(m) \\le cm \\log m$ 成立。 特别地，对于 $m = N/2$，我们有 $T(N/2) \\le c(N/2) \\log(N/2)$。
    
    **归纳步骤:** 将此假设代入原递归式： $$\\begin{aligned}T(N) &= 2T(N/2) + N\\\\ &\\le 2 \\cdot \[c(N/2) \\log(N/2)\] + N\\\\ &= cN \\log(N/2) + N\\\\ &= cN (\\log N - \\log 2) + N\\\\ &= cN \\log N - cN \\log 2 + N\\\\ &= cN \\log N - cN + N\\\\ &= cN \\log N - (c-1)N \\end{aligned}$$
    
    我们的目标是证明 $$T(N) \\le cN \\log N$$我们已经推导出 $$T(N) \\le cN \\log N - (c-1)N$$ 只要 $$-(c-1)N \\le 0$$ 即 $$(c-1)N \\ge 0$$该不等式就成立。 我们只需选择 $c \\ge 1$，即可满足此条件。
    
    **基例:** 对于小的 $N$ (如 $T(2)$, $T(3)$)，我们总可以通过选择一个足够大的 $c$ 来使得 $T(N) \\le cN \\log N$ 成立。 因此，我们的猜测 $T(N) = O(N \\log N)$ 是正确的。
    

**【例2】错误猜测的教训**

对于同样的递归式 $T(N) = 2T(N/2) + N$，如果我们错误地猜测 $T(N) = O(N)$。

*   **证明尝试:** 假设 $T(N/2) \\le c(N/2)$。
    
    $$\\begin{aligned}T(N) &= 2T(N/2) + N\\\\ &\\le 2 \\cdot \[c(N/2)\] + N\\\\ &= cN + N\\\\ &= (c+1)N \\end{aligned}$$
    
    我们希望证明 $T(N) \\le cN$，但我们得到的结论是 $T(N) \\le (c+1)N$。我们永远无法找到一个常数 $c$ 使得 $cN+N \\le cN$ 成立。这说明我们的猜测太紧了， $O(N)$ 是不正确的。**使用代入法时，必须证明猜测的精确形式，而不仅仅是证明其属于某个大O类别。**
    

#### 3.2 递归树法：眼见为实

递归树是将递归过程图形化的强大工具。树的每个节点代表一次函数调用，节点的值是该次调用中非递归部分的代价（即 $f(N)$ 部分）。

**【例1】** $T(N) = 3T(N/4) + cN^2$

我们可以构建如下的递归树：

graph TD subgraph "Level 0" A("cN^2") end subgraph "Level 1" B("c(N/4)^2"); C("c(N/4)^2"); D("c(N/4)^2") end subgraph "Level 2" E("c(N/16)^2"); F("..."); G("...") end subgraph "Level log\_4(N)" H("...T(1)") end A --> B & C & D; B --> E & F & G; subgraph "Cost per Level" L0\["Total: cN^2"\] L1\["Total: 3 \* c(N^2/16) = (3/16)cN^2"\] L2\["Total: 9 \* c(N^2/256) = (3/16)^2 cN^2"\] LN\["Leaves: 3^{\\log\_4 N} \\cdot T(1) = N^{\\log\_4 3} \\cdot \\Theta(1) = \\Theta(N^{\\log\_4 3})"\] end

*   **树的深度:** 规模从 $N$ 降到 1，每次除以 4，所以深度为 $\\log\_4 N$。
*   **每层代价:**
    *   第 $i$ 层 (根为第0层) 有 $3^i$ 个节点，每个节点的代价为 $c(N/4^i)^2$。该层总代价为 $3^i \\cdot c(N^2/16^i) = (3/16)^i cN^2$。
*   **总代价 T(N):** $T(N) = \\sum\\limits\_{i=0}^{\\log\_4 N - 1} (\\frac{3}{16})^i cN^2 + \\Theta(N^{\\log\_4 3})$ 这是一个公比为 $r = 3/16 < 1$ 的几何级数求和。 该级数的和收敛于一个常数，小于 $\\frac{1}{1-3/16} = \\frac{16}{13}$。 所以求和部分为 $cN^2 \\cdot O(1) = O(N^2)$。 由于 $\\log\_4 3 \\approx 0.79 < 2$，叶子节点的代价 $\\Theta(N^{\\log\_4 3})$ 被 $O(N^2)$ 所主导。 因此，$T(N) = O(N^2)$。

**【例2】** $T(N) = T(N/3) + T(2N/3) + cN$ (不平衡的递归树)

*   **每层代价:** 尽管子问题大小不同，但每一层的子问题规模总和都是 $N$。例如，第一层是 $N/3$ 和 $2N/3$，代价是 $c(N/3) + c(2N/3) = cN$。因此，每层的代价都是 $cN$。
*   **树的深度:** 树的深度由最长的路径决定，即每次都沿着 $2/3$ 的分支走。设深度为 $k$，则 $N \\cdot (2/3)^k \\approx 1$，解得 $k \\approx \\log\_{3/2} N$。
*   **总代价:** 粗略估计为 (层数) $\\times$ (每层代价) = $O(\\log N) \\cdot O(N) = O(N \\log N)$。这个猜测是正确的，可以通过代入法严格证明。

* * *

### 4\. 主方法 (Master Method)

主方法为求解 $T(N) = aT(N/b) + f(N)$ 形式的递归式提供了一个强大的、公式化的解决方案。其核心思想是比较**非递归代价 $f(N)$** 与**由递归产生的叶子节点代价相关的函数 $N^{\\log\_b a}$** 的增长速度。

#### **主定理 (Master Theorem)**

设 $a \\ge 1, b > 1$ 为常数， $f(N)$ 为一个函数， $T(N)$ 的递归定义为 $$T(N) = aT(N/b) + f(N)$$那么 $T(N)$ 的界可以按以下三种情况确定：

1.  **情况 1 (叶子节点主导):** 如果 $$f(N) = O(N^{\\log\_b a - \\varepsilon})$$对于某个常数 $\\varepsilon > 0$。 (即 $f(N)$ 的增长速度**多项式地慢于** $N^{\\log\_b a}$) 那么$$T(N) = \\Theta(N^{\\log\_b a})$$
    
2.  **情况 2 (权重均衡):** 如果 $$f(N) = \\Theta(N^{\\log\_b a})$$ (即 $f(N)$ 与 $N^{\\log\_b a}$ 的增长速度相同) 那么$$T(N) = \\Theta(N^{\\log\_b a} \\log N)$$
    
3.  **情况 3 (根节点主导):** 如果 $$f(N) = \\Omega(N^{\\log\_b a + \\varepsilon})$$对于某个常数 $\\varepsilon > 0$。 (即 $f(N)$ 的增长速度**多项式地快于** $N^{\\log\_b a}$) 并且，如果 $f(N)$ 满足**正则条件**: $a f(N/b) \\le c f(N)$，对于某个常数 $c < 1$ 和所有足够大的 $N$。 那么$$T(N) = \\Theta(f(N))$$
    

**【应用示例】**

*   **Mergesort:** $T(N) = 2T(N/2) + N$ $a=2, b=2, f(N)=N$。计算 $N^{\\log\_b a} = N^{\\log\_2 2} = N^1 = N$。 $f(N) = \\Theta(N^{\\log\_2 2})$，符合**情况2**。 因此，$T(N) = \\Theta(N \\log N)$。
    
*   **$T(N) = 4T(N/2) + N \\log N$** $a=4, b=2, f(N)=N \\log N$。计算 $N^{\\log\_b a} = N^{\\log\_2 4} = N^2$。 比较 $f(N) = N \\log N$ 和 $N^2$。显然 $N \\log N = O(N^{2 - \\varepsilon})$ (例如取 $\\varepsilon = 0.5$)。 符合**情况1**。 因此，$T(N) = \\Theta(N^2)$。
    
*   **$T(N) = 2T(N/2) + N \\log N$** $a=2, b=2, f(N)=N \\log N$。计算 $N^{\\log\_b a} = N^{\\log\_2 2} = N$。 $f(N) = N \\log N$ 比 $N$ 增长快，但不满足 $f(N) = \\Omega(N^{1+\\varepsilon})$。它落在了情况2和情况3的“缝隙”中，**标准主定理不适用**。需要使用扩展版主定理来解决，结果是 $T(N) = \\Theta(N \\log^2 N)$。
    

#### **主定理的严谨数学证明**

递归树的总代价可以表示为所有层级代价之和，加上叶子节点代价： $$T(n) = \\sum\\limits\_{j=0}^{\\log\_b n - 1} a^j f(n/b^j) + \\Theta(n^{\\log\_b a})$$

**证明情况 1: $f(n) = O(n^{\\log\_b a - \\varepsilon})$** 由条件可知，存在常数 $c\_1 > 0$ 使得 $f(n) \\le c\_1 n^{\\log\_b a - \\varepsilon}$。 代入求和式：

$$\\begin{aligned}\\sum a^j f(n/b^j) &\\le \\sum a^j c\_1 (n/b^j)^{\\log\_b a - \\varepsilon}\\\\ &= c\_1 n^{\\log\_b a - \\varepsilon} \\sum ( \\frac{a}{(b^{\\log\_b a - \\varepsilon})^j} )\\\\ &= c\_1 n^{\\log\_b a - \\varepsilon} \\sum ( \\frac{a}{a \\cdot b^{-\\varepsilon}} )^j = c\_1 n^{\\log\_b a - \\varepsilon} \\sum (b^{\\varepsilon})^j\\end{aligned}$$

这是一个公比为 $b^{\\varepsilon} > 1$ 的几何级数，其和由最后一项主导：$O((b^{\\varepsilon})^{\\log\_b n}) = O(n^\\varepsilon)$。 所以，求和部分为 $O(n^{\\log\_b a - \\varepsilon} \\cdot n^\\varepsilon) = O(n^{\\log\_b a})$。 因此$$T(n) = O(n^{\\log\_b a}) + \\Theta(n^{\\log\_b a}) = \\Theta(n^{\\log\_b a})$$

**证明情况 2: $f(n) = \\Theta(n^{\\log\_b a})$** 由条件可知，$f(n/b^j) = \\Theta((n/b^j)^{\\log\_b a}) = \\Theta(n^{\\log\_b a} / a^j)$。 代入求和式： $$\\sum a^j f(n/b^j) = \\sum a^j \\Theta(n^{\\log\_b a} / a^j) = \\sum \\Theta(n^{\\log\_b a})$$ 这个求和共有 $\\log\_b n$ 项，每项都是 $\\Theta(n^{\\log\_b a})$。 所以，求和部分为 $\\Theta(n^{\\log\_b a} \\log\_b n) = \\Theta(n^{\\log\_b a} \\log n)$。 因此$$T(n) = \\Theta(n^{\\log\_b a} \\log n) + \\Theta(n^{\\log\_b a}) = \\Theta(n^{\\log\_b a} \\log n)$$

**证明情况 3: $f(n) = \\Omega(n^{\\log\_b a + \\varepsilon})$ 和正则条件 $a f(n/b) \\le c' f(n)$ for $c' < 1$** 从正则条件 $a f(n/b) \\le c' f(n)$ 递归展开，可得 $a^j f(n/b^j) \\le (c')^j f(n)$。 代入求和式： $$\\sum a^j f(n/b^j) \\le \\sum (c')^j f(n) = f(n) \\sum\_{j=0}^{\\log\_b n - 1} (c')^j$$ 这是一个公比 $c' < 1$ 的几何级数，其和收敛于常数 $\\frac{1}{1-c'}$。 所以，求和部分为 $O(f(n))$。 $$T(n) = O(f(n)) + \\Theta(n^{\\log\_b a})$$ 根据情况3的初始条件，$f(n)$ 多项式地快于 $n^{\\log\_b a}$，所以 $f(n)$ 主导了最终结果。 因此$$T(n) = \\Theta(f(n))$$

* * *

### 5\. 主方法的扩展形式

#### 5.1 更通用的定理

对于包含对数因子的更普遍情况，有一个更强大的定理。

**定理:** 对于 $$T(N) = aT(N/b) + \\Theta(N^k \\log^p N)$$其中 $a \\ge 1, b > 1, k \\ge 0, p$是实数。 其解为：

*   **情况 1:** 如果 $a > b^k$ (等价于 $\\log\_b a > k$)，则 $T(N) = \\Theta(N^{\\log\_b a})$。
*   **情况 2:** 如果 $a = b^k$ (等价于 $\\log\_b a = k$)，则
    *   如果 $p > -1$，$T(N) = \\Theta(N^k \\log^{p+1} N)$
    *   如果 $p = -1$，$T(N) = \\Theta(N^k \\log \\log N)$
    *   如果 $p < -1$，$T(N) = \\Theta(N^k)$
*   **情况 3:** 如果 $a < b^k$ (等价于 $\\log\_b a < k$)，则 $T(N) = \\Theta(N^k \\log^p N)$。

**【应用示例】**

*   **$T(N) = 3T(N/2) + O(N)$:** $a=3, b=2, k=1, p=0$。 $a > b^k$ (因为 $3 > 2^1$)，符合情况1。 $T(N) = \\Theta(N^{\\log\_2 3}) \\approx \\Theta(N^{1.585})$。
    
*   **$T(N) = 3T(N/2) + O(N^2)$:** $a=3, b=2, k=2, p=0$。 $a < b^k$ (因为 $3 < 2^2=4$)，符合情况3。 $T(N) = \\Theta(N^2 \\log^0 N) = \\Theta(N^2)$。
    
*   **$T(N) = 2T(N/2) + N / \\log N$:** $a=2, b=2, k=1, p=-1$。 $a = b^k$ ($2=2^1$)，符合情况2。 $p = -1$，所以 $T(N) = \\Theta(N^1 \\log \\log N) = \\Theta(N \\log \\log N)$。