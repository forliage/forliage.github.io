---
title: "数据库系统设计03:索引(下)—— 哈希与LSM树"
description: ""
pubDate: "2025-08-26"
heroImage: ""
---

# 数据库系统设计03:索引(下)—— 哈希与LSM树

回忆一下B+树的插入操作：每次插入都可能引发节点的**分裂**，这是一个**随机写**操作。在高并发、写密集型的场景下——比如双十一的订单系统、社交网络的消息流、物联网设备的海量数据上报——成千上万的并发写请求，都可能在B+树这棵参天大树的各个角落引发随机I/O的风暴。这种“写惩罚”可能会成为整个系统的瓶颈。

此外，B+树的优势在于支持**范围查询** (`> < BETWEEN`)。但如果我们99%的查询都只是简单的**等值查询** (`WHERE id = ?`)，我们是否还需要维护B+树复杂的有序结构呢？有没有一种更直接、更暴力、更快速的方法？

今天，我们将探索两个截然不同的答案：

1.  **哈希索引 (Hash Indexing):** 为追求极致的等值查询速度，我们能否放弃“有序”，实现真正的 $O(1)$ 查找？
2.  **日志结构合并树 (Log-Structured Merge-Tree, LSM-Tree):** 为应对海量的写入，我们能否彻底抛弃“原地更新”，将所有随机写转化为顺序写？

这两条路径，将带领我们进入一个全新的设计范式，让我们看到在不同业务需求下，索引结构是如何演化出截然不同的形态。

### **第一部分：哈希**

哈希的思想，在计算机科学中无处不在。它的核心是：**通过一个哈希函数，将一个任意长度的输入（我们的搜索键），映射成一个固定长度的、看似随机的输出（哈希值），并以此作为数据存储位置的“门牌号”。**

#### **1.1 静态哈希**

让我们从最简单的**静态哈希 (Static Hashing)** 开始。

*   **组件:**
    
    1.  **桶 (Buckets):** 一组固定数量（$N$）的数据页/块，用于存储数据记录。它们就像一个数组，`Bucket[0], Bucket[1], ..., Bucket[N-1]`。
    2.  **哈希函数 (Hash Function):** $h(K)$，它接受一个搜索键 `K`，返回一个桶号 `i`，其中 $0 \\le i < N$。一个简单的例子是 $h(K) = K \\pmod{N}$。
*   **操作流程:**
    
    *   **插入 `<K, V>`:** 计算 `i = h(K)`，然后将记录 `<K, V>` 放入 `Bucket[i]`。
    *   **查找 `K`:** 计算 `i = h(K)`，然后**只**在 `Bucket[i]` 中查找。
    *   **删除 `K`:** 计算 `i = h(K)`，在 `Bucket[i]` 中找到并删除。
*   **理想情况下的性能:**
    
    *   如果每个桶只对应一个磁盘块，且从不溢出，那么**任何操作都只需要一次I/O**！这看起来是 $O(1)$ 的终极梦想。

#### **1.2 哈希冲突与数据增长**

##### **1.2.1 哈希冲突 (Hash Collisions) 与溢出链 (Overflow Chaining)**

*   **问题:** 完美的哈希函数（能将不同键均匀映射到不同桶）是不存在的。总会有多个不同的键 `K1`, `K2`，使得 `h(K1) = h(K2)`。这就是**哈希冲突**。
*   **后果:** 当一个桶被装满后，新的冲突记录无处可放，这就产生了**溢出 (Overflow)**。
*   **解决方案：溢出链 (Overflow Chaining)**
    *   每个主桶(Primary Bucket)都有一个指针，指向一个由溢出块(Overflow Pages)组成的链表。
    *   当主桶满了，新记录就放入链表的第一个溢出块。如果溢出块也满了，就再挂一个新的。
*   **性能退化:** 查找一个键时，如果主桶未命中，就必须沿着溢出链顺序扫描。在最坏情况下（数据分布极不均匀，或负载因子过高），哈希索引的性能会退化成**线性扫描**。

##### **1.2.2 静态结构的“原罪”：数据增长与缩减**

*   **问题:** 静态哈希的桶数量 $N$ 是固定的。
    *   **如果数据量暴增:** 桶的数量太少，导致负载因子（记录数/桶数）过高，哈希冲突激增，溢出链变得很长，性能急剧下降。
    *   **如果数据量骤减:** 大量的桶会变空，浪费大量空间。
*   **解决方案？—— Rehash:** 当性能下降到某个阈值时，暂停所有操作，创建一个新的、更大（或更小）的桶数组，然后将所有旧数据重新哈希到新数组中。
*   **代价:** **Rehash是一个成本极高的“世界暂停” (Stop-the-World) 事件**，对于需要7x24小时服务的在线系统是不可接受的。

静态哈希，就像一个为特定尺寸设计的完美西装，一旦身材变化，就完全无法穿着。我们需要一种能够**动态、平滑地**适应数据量变化的哈希方案。

#### **1.3 动态哈希方案的演化**

##### **1.3.1 可扩展哈希 (Extendible Hashing)**

*   **核心思想:** 解耦哈希函数的输出空间和桶数组的实际大小，通过一个**目录 (Directory)** 实现间接寻址。
    
    1.  **哈希函数:** $h(K)$ 返回一个很长的二进制串 (e.g., 32位)。
    2.  **目录:** 一个指针数组。目录的大小为 $2^i$，其中 $i$ 被称为**全局深度 (Global Depth)**。我们只使用 $h(K)$ 的**前 $i$ 位**来索引这个目录。
    3.  **桶:** 每个桶有一个**局部深度 (Local Depth) $j$** ($j \\le i$)。这意味着，这个桶内所有键的哈希值，其前 $j$ 位都相同。
    4.  **映射关系:** 目录中的多个条目可以指向**同一个桶**。具体来说，如果一个桶的局部深度为 $j$，那么会有 $2^{(i-j)}$ 个目录条目指向它。
*   **插入与分裂 (Split) 过程:**
    
    *   **当一个桶满了:** a. **检查局部深度 $j$ 和全局深度 $i$:**
        *   **Case 1: $j < i$ (目录有“富余”):**
            1.  创建一个新桶。
            2.  两个桶的局部深度都增加到 $j+1$。
            3.  将旧桶中的数据根据其哈希值的第 $j+1$ 位，重新分配到新旧两个桶中。
            4.  **只修改目录指针**：将原来指向旧桶的那些目录条目中的一半，改指向新桶。**目录本身大小不变！** 这是一个非常轻量的操作。
        *   **Case 2: $j = i$ (目录已“饱和”):**
            1.  **目录翻倍:** 将全局深度 $i$ 增加到 $i+1$，目录大小从 $2^i$ 变为 $2^{i+1}$。
            2.  复制旧目录的指针到新目录。
            3.  现在情况转化为 Case 1，继续分裂桶和修改目录指针。
*   **优点:**
    
    *   插入操作最多只会分裂一个桶，Rehash的范围被局限在单个桶内。
    *   数据增长是平滑的，没有“世界暂停”。
    *   查找通常只需要两次I/O（一次读目录，一次读桶）。
*   **缺点:**
    
    *   目录本身可能会变得非常大，如果无法放入内存，会增加一次额外的I/O。
    *   数据分布不均时，可能导致目录大小急剧增长，而实际桶的数量增长不多。

##### **1.3.2 线性哈希 (Linear Hashing)**

*   **核心思想:** 不使用目录，以一种**线性的、可预测的**方式来增加桶。
    
    1.  **哈希函数族:** 使用两个哈希函数 $h\_i(K) = K \\pmod{2^i N}$ 和 $h\_{i+1}(K) = K \\pmod{2^{i+1} N}$，其中 $N$ 是初始桶数，$i$ 是当前的“轮次”(level)。
    2.  **分裂指针 (Split Pointer):** 一个指针 `next`，指向下一个即将被分裂的桶，从0开始。
    3.  **分裂时机:** 当任何一个桶发生溢出时，就触发一次分裂，但分裂的**不是**当前溢出的桶，而是 `next` 指向的那个桶。
    4.  **分裂过程:** a. 将 `Bucket[next]` 中的所有记录，用新的哈希函数 $h\_{i+1}$ 重新哈希。 b. 一部分记录会留在 `Bucket[next]`，另一部分会移动到新创建的 `Bucket[N+next]`。 c. `next` 指针加1。 d. 当 `next` 达到 $2^i N$ 时，一轮分裂完成，`next` 重置为0，轮次 $i$ 增加1。
*   **查找过程的复杂性:**
    
    *   要查找键 `K`，先用 $h\_i(K)$ 计算桶号 `b`。
    *   如果 $b < \\text{next}$，说明这个桶已经被分裂过了，应该用 $h\_{i+1}(K)$ 来确定最终的桶号。
    *   如果 $b \\ge \\text{next}$，说明这个桶还没分裂，直接在 `Bucket[b]` 查找。
*   **优点:**
    
    *   不需要目录，空间开销更小。
    *   桶的扩展是平滑、线性的。
*   **缺点:**
    
    *   负载因子可能暂时不均匀，因为分裂是按顺序的，而不是按需的。
    *   可能需要处理更长的溢出链。

#### **1.4 B+树 vs. 哈希索引**

特性

B+树索引

哈希索引

**设计权衡 (Trade-off)**

**优化查询类型**

**等值查询 & 范围查询**

**仅等值查询**

**功能性 vs. 专一性**：哈希放弃了范围查询的能力，换取了等值查询的极致性能。

**数据有序性**

**有序 (物理或逻辑)**

**无序**

有序性是支持范围查询、排序、MIN/MAX等操作的基础，但维护它有成本。

**性能 (理想)**

$O(\\log\_B N)$

$O(1)$

哈希在理想情况下的常数时间性能是无与伦比的。

**性能 (最坏)**

$O(\\log\_B N)$ (稳定)

$O(N)$ (退化为线性)

**稳定性 vs. 峰值性能**：B+树性能稳定可预测，而哈希性能依赖于数据分布和哈希函数。

**空间开销**

相对较高，内部节点有开销

相对较低，但溢出页和目录是额外开销

两者都需要权衡填充率和空间浪费。

**结论:** 在数据库世界中，**B+树是默认的、通用的索引选择**，因为它能应对更多样的查询模式。哈希索引则作为一种**特种武器**，用在那些可以确定查询模式为纯等值查找，且对性能要求极为苛刻的场景中（例如，某些内存数据库的内部实现）。

### **第二幕：LSM树**

B+树和哈希索引都属于\*\*“原地更新” (Update-in-place)\*\* 的思想范畴。而LSM树，则代表了一种完全不同的哲学：**“追加写入，延迟合并” (Append-only & Deferred Merge)**。

#### **2.1 写密集型负载的挑战**

*   **HDD时代:** 随机写意味着昂贵的寻道，LSM树通过将随机写转化为顺序写来优化。
*   **SSD时代:** 随机写意味着“读-修改-擦除-写”循环，导致**写放大**。LSM树的顺序追加模型，完美契合了SSD的物理特性。
*   **应用场景:** 日志分析、时序数据、消息队列、NoSQL数据库 (Google Bigtable, Cassandra, RocksDB) 等。

#### **2.2 LSM树的核心结构**

LSM树不是一棵“树”，而是一个**分层的、级联的存储系统**。

*   **L0: MemTable (内存)**
    
    *   **结构:** 一个内存中的、有序的数据结构，通常是**跳表 (Skip List)** 或**红黑树**。
    *   **所有写操作（插入、更新、删除）首先进入这里**。
    *   **优点:** 内存操作，速度极快。
    *   **阈值:** 当MemTable的大小达到一个预设阈值（如几十MB）时，它会被**冻结 (freeze)**，并被一个**后台线程**刷写到磁盘。
*   **L1, L2, ..., Lk: SSTables (磁盘)**
    
    *   **SSTable (Sorted String Table):** 一个磁盘上的、**不可变的 (Immutable)**、有序的文件块。
    *   **Flush (刷写):** 冻结的MemTable被顺序地写入磁盘，形成一个新的L1层的SSTable。
    *   **层次结构:**
        *   从L1到Lk，每一层的总容量通常是上一层的**N倍**（N被称为**Size Ratio**，通常为10）。
        *   每一层由一个或多个SSTable文件组成。
    *   **Compaction (合并):** 当某一层 $L\_i$ 的文件总大小超过其容量阈值时，会触发**合并过程**。后台线程会选择 $L\_i$ 层的一个或多个SSTable，连同 $L\_{i+1}$ 层中与它们键范围重叠的SSTable，一起读入内存，进行一次**多路归并排序**，然后将合并后的结果，写成一个新的、更大的SSTable，放入 $L\_{i+1}$ 层。

#### **2.3 LSM树的操作流程**

##### **2.3.1 写操作 (Insert / Update)**

1.  **写入WAL (Write-Ahead Log):** 为了持久性，写操作首先被顺序追加到日志文件中。
2.  **写入MemTable:** 然后在内存中的MemTable里插入或更新。
3.  **向客户端返回成功。**

*   **性能分析:** 写操作 = 1次顺序磁盘写 (WAL) + 1次内存操作。**极快！** 这就是LSM树“写优化”的根源。所有昂贵的磁盘I/O都被推迟到后台的Compaction中。

##### **2.3.2 删除操作 (Delete)**

*   **LSM树从不原地删除！**
*   **墓碑 (Tombstone):** 当请求删除一个键 `K` 时，LSM树会向MemTable中插入一个特殊的标记 `<K, TOMBSTONE>`。
*   **在Compaction时清理:** 当Compaction过程同时遇到一个键 `K` 的正常值和它的墓碑时，两者会“同归于尽”，这个键就从新生成的SSTable中被物理清除了。

##### **2.3.3 读操作 (Query)**

这是LSM树为写性能付出的**代价**。

*   **查找流程:**
    
    1.  **先查MemTable。** 如果找到（且不是墓碑），直接返回。
    2.  如果MemTable中未找到或找到的是墓碑，**再依次从L1, L2, ..., Lk层查找**。
    3.  在每一层，可能需要查找该层的多个SSTable。
    4.  查找以**第一次找到**的记录为准（因为层数越小，数据越新）。如果先找到墓碑，则表示数据已被删除，立即停止并返回空。
*   **读放大的问题 (Read Amplification):** 一次点查询，在最坏情况下，可能需要检查每一层的每一个SSTable，引发多次磁盘I/O。
    
*   **读性能优化策略:**
    
    1.  **布隆过滤器 (Bloom Filter):**
        *   每个SSTable都关联一个布隆过滤器。这是一种概率性数据结构，可以**快速地判断一个元素“肯定不存在”**，或者“可能存在”。
        *   在查询SSTable之前，先查其布隆过滤器。如果过滤器说“不存在”，我们就可以**100%确定地跳过**对这个文件的昂贵I/O。
        *   布隆过滤器极大地减少了读放大，是LSM树读性能的关键。
    2.  **SSTable内部索引:** 每个SSTable文件内部都有自己的索引块，可以快速定位到数据块。
    3.  **Compaction策略优化:** 选择合适的Compaction策略（如Leveled vs. Tiered）来平衡读、写和空间放大。

#### **2.4 Compaction**

Compaction是LSM树的后台管家，也是其最复杂、最消耗资源的部分。

*   **Compaction的职责:**
    
    1.  控制树的形状，确保层次结构。
    2.  合并小文件，减少读放大。
    3.  清理被覆盖和被删除的数据，回收空间。
*   **两大主流策略:**
    
    *   **Tiered Compaction (分层合并 / Size-tiered):**
        *   当一层有 $N$ 个大小相似的SSTable时，将它们合并成一个大的SSTable，放入下一层。
        *   **优点:** 写放大较小（一个数据只会被合并几次）。
        *   **缺点:** **空间放大**严重（同一键的多个版本可能同时存在于不同层的SSTable中），**读放大**也较严重（一层内有多个文件）。
    *   **Leveled Compaction (分级合并):**
        *   每一层 $L\_i$ (i>0) 的SSTable都被切分成固定大小、键范围不重叠的文件。
        *   当 $L\_i$ 超限时，选择一个文件，连同它在 $L\_{i+1}$ 中重叠的所有文件，合并成 $L\_{i+1}$ 中的几个新文件。
        *   **优点:** **空间放大和读放大都得到了很好的控制**（一个键最多只存在于一层）。
        *   **缺点:** **写放大**较大（数据在下沉过程中，会被反复读出和重写）。

#### **2.5 B+树 vs. LSM树**

特性

B+树 (Read-Optimized)

LSM树 (Write-Optimized)

**写操作**

**随机写 (原地更新/分裂)**

**顺序写 (追加日志/MemTable)**

**写性能**

稳定，但受限于随机I/O

**极高**，写操作基本在内存完成

**读操作**

**点查、范围查都高效**

**点查性能依赖优化(Bloom Filter)，范围查较复杂**

**读性能**

**稳定可预测 ($O(\\log N)$)**

可能较高 (缓存命中) 或较低 (穿透多层)

**空间放大**

较小 (只有约50%的填充率)

较大 (Tiered Compaction) 或较小 (Leveled)

**并发控制**

复杂 (Latch/Lock树的节点)

相对简单 (写操作不冲突，用MVCC处理读)

**适用场景**

**OLTP (在线事务处理)，读多写少**

**OLAP (在线分析处理)，写密集，时序数据**

**\[拓展\] 缓冲树 (Buffer Tree / Fractal Tree Index):** 可以看作是B+树和LSM树思想的结合。它是一棵B+树，但每个**内部节点**都带有一个**缓冲区**。插入操作只写入路径上节点的缓冲区，当缓冲区满时，再批量“下推”到下一层。它试图在B+树的优秀读性能和LSM树的写性能之间找到一个平衡点。