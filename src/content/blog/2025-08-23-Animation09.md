---
title: "计算机动画09:关节(角色)动画·下"
description: ""
pubDate: "2025-08-23"
heroImage: ""
---

# 计算机动画09:关节(角色)动画·下

### **第一部分：运动捕捉(Mocap)**

#### **1.1 Mocap数据宝库：CMU Motion Capture Database**

要想研究和使用运动数据，我们首先需要数据本身。学术界和工业界最著名、最宝贵的资源之一，就是**卡内基梅隆大学图形学实验室的运动捕捉数据库 (CMU Graphics Lab Motion Capture Database)**。

这个数据库包含了海量的、分门别类的真人运动数据，涵盖了：

*   **人类交互 (Human Interaction)**: 两个对象之间的互动，如握手、拥抱。
*   **与环境交互 (Interaction with Environment)**: 在不同地形（如操场、不平坦地面）上的运动。
*   **移动 (Locomotion)**: 各式各样的跑步、走路姿态。
*   **体育活动 (Physical Activities & Sports)**: 篮球、舞蹈等。
*   **情景与剧本 (Situations & Scenarios)**: 日常行为、表情、哑剧等。

这是一个巨大的宝藏，为无数的动画研究和应用提供了基础。

#### **1.2 数据是如何被捕获的？**

CMU的数据集是使用一套专业的**Vicon光学运动捕捉系统**来记录的。这个过程大致如下：

1.  **环境设置**: 在一个约 3米 x 8米 的工作空间内，布置了12个红外MX-40摄像头。这些摄像头可以以120Hz的频率、400万像素的分辨率进行拍摄。
2.  **演员准备**: 演员穿上一件紧身的黑色衣服，并在全身的关键位置（关节、重要骨骼点）贴上**41个反光标记点 (markers)**。
3.  **数据捕获**: 演员开始表演。红外摄像头会发射红外光，这些光被标记点反射回摄像头。每个摄像头都会记录下标记点在它的二维图像上的位置。
4.  **三维重建**: 系统通过至少两个或更多摄像头捕捉到的同一标记点的二维位置，利用三角测量原理，可以精确地重建出该标记点在三维空间中的坐标。

最终，我们得到的是一系列随时间变化的三维点云数据。

**“Garbage in, garbage out!”** 这是一个在Mocap领域流传的至理名言，意思是“输入的是垃圾，输出的也是垃圾”。这强调了原始数据质量的极端重要性。如果捕捉过程中出现标记点脱落、遮挡、或者演员表演不到位，那么后期无论用多么先进的算法都难以修复，最终得到的动画效果也会大打折扣。

**数据格式**: 捕获到的数据通常以特定的格式存储，最常见的有：

*   **ASF/AMC (Acclaim Skeleton File / Acclaim Motion Capture)**: 一种较早的格式，ASF文件定义骨架的层级结构和骨骼长度，AMC文件则存储每一帧的关节旋转数据。
*   **BVH (Biovision Hierarchy)**: 目前更通用的格式。它在一个文件中同时包含了骨架的层次结构信息（Hierarchy）和随后的运动数据（Motion data）。

#### **1.3 前沿进展：高精度人体运动神经求解器**

传统的Mocap数据处理流程，在面对标记点被遮挡、丢失或标注错误时，常常会产生不准确的结果。近年来，随着深度学习的发展，研究者们开始利用神经网络来更智能、更鲁棒地解决这些问题。

近两年进展：

*   **Siggraph Asia 2023**: 提出了一个**基于局部性的神经求解器**。它结合了几何的局部性信息和从数据中学到的先验知识，能更好地处理复杂运动中（如翻滚、身体接触）的遮挡情况，并且能够准确求解精细的手部动作。
*   **Siggraph Asia 2024 (RoMo)**: 提出了一个**鲁棒的光学动作捕捉标注与解算器**。这个系统能够应对更具挑战性的情况，比如**未标记**的Mocap点云（不知道哪个点对应哪个身体部位），并能处理异常值和遮挡。它通过一个混合神经网络，利用全局关节位置作为中间表示，有效避免了传统沿运动链累积误差的问题。

这些工作表明，AI技术正在深刻地改变着Mocap数据的处理方式，使其变得更加自动化、精确和鲁棒。

### **第二部分：如何使用Mocap数据？**

拿到了原始的Mocap数据后，我们并不能直接使用。需要经过一系列处理，才能将其应用到我们的角色上。

#### **2.1 离线(Off-line) vs. 在线(On-line)**

*   **离线处理**: 这是最常见的用法。数据被捕获后，会经过一系列的后期处理，如滤波去噪、数据修复，然后被存入一个**运动数据库**。动画师可以从中挑选、混合、修改这些动作，用于制作动画。
*   **在线处理 (表演动画)**: 这是实时应用的场景，也叫**表演捕捉 (Performance Capture)**。演员的表演被实时捕捉，并直接驱动虚拟角色进行同步的动作。这在虚拟直播、实时预演等领域应用广泛。

#### **2.2 Mocap数据的处理流程**

1.  **运动信号处理 (Motion Signal Processing)** 我们可以把一个角色的运动看作是一个高维的时间序列信号，其中每一维对应一个关节的旋转或位置。
    
    *   **把运动看成多维信号**: 每个关节的旋转（如四元数的4个分量）随时间变化，构成了一条曲线。
    *   **低通滤波**: 原始Mocap数据中常有高频噪声（由设备或环境引起）。通过低通滤波器可以滤除这些噪声，使运动变得更平滑。这对应于运动的**基本部分**，如走路的整体姿态。
    *   **高通滤波**: 高频部分除了噪声，也可能包含有用的**运动细节**，如走路时轻微的跛行、身体的细微晃动等。高通滤波可以提取这些“风格”信息。
    *   **挑战**: 简单地用滤波来修改运动并非易事，因为它可能会破坏重要的物理约束（如脚接触地面）或运动的自然感。
2.  **运动数据的重定向 (Motion Retargeting)** 这是Mocap应用中最核心、最常见的问题：**如何将为一个角色（如真人演员）捕获的运动，应用到一个体型、骨骼比例完全不同的虚拟角色（如一个卡通兽人）上？**
    
    **问题的数学描述**:
    
    *   设原始运动为 $m\_o(t)$，这是一个描述所有关节随时间 $t$ 变化状态的函数。
    *   我们想要求解一个新的运动 $m(t)$，它是对 $m\_o(t)$ 的一种修改：$m(t) = m\_o(t) + d(t)$，其中 $d(t)$ 是我们施加的偏移量。
    *   这个新运动 $m(t)$ 必须满足一系列**约束 (Constraints)**。
    
    **约束的例子**:
    
    *   **脚不能穿透地面 (Foot-skate prevention)**: 脚部关节的Y坐标必须大于等于0。
    *   **身体不能自交叉 (Self-penetration prevention)**。
    *   **保持与环境的交互**: 如果原始运动是坐在椅子上，新角色也必须能准确地坐到目标椅子上。
    
    **优化问题的构建**: 我们将运动重定向构建为一个**带约束的优化问题**：
    
    *   **目标函数 (Objective Function)**: 我们希望新的运动与原始运动尽可能相似。因此，我们的目标是**最小化**它们之间的差异。一个常用的目标函数是两者差值的平方积分： $$g(m) = \\min \\int\_t ||m(t) - m\_o(t)||^2 dt = \\min \\int\_t ||d(t)||^2 dt$$
    *   **约束条件 (Constraints)**: 新运动 $m(t)$ 必须满足所有物理和几何约束。我们可以将这些约束写成函数形式： $$f(m(t)) \\diamond c$$ 其中 $f(\\cdot)$ 是约束函数（例如，计算脚底板的高度），$\\diamond$ 是关系符（如 $\\ge$），$c$ 是常数值（如0）。
    
    通过求解这个优化问题，我们就能找到一个既保留了原始运动风格，又适应了新角色身体比例和环境约束的全新运动。
    
3.  **运动数据的合并与过渡 (Motion Blending and Transition)**
    
    *   **运动混合 (Motion Blending)**: Mocap数据通常是一段段的短片。我们可以通过插值来混合不同的运动。例如，为了得到一个介于“走路”和“跑步”之间的“慢跑”动作，我们可以对走路动作 $m\_0(t)$ 和跑步动作 $m\_1(t)$ 进行线性插值： $$m(t) = (1-\\alpha) m\_0(t) + \\alpha m\_1(t)$$ 通过调整混合系数 $\\alpha$ 从0到1，就可以得到从走到跑的平滑速度变化。
    *   **运动过渡 (Motion Transition)**: 如何将两个独立的运动片段（如“走路”和“停下”）平滑地连接起来？
        *   如果两个运动在衔接处的姿态很接近，过渡会比较容易，简单的插值就能工作。
        *   如果姿态相差很远（如从“跑步”直接切换到“坐下”），就需要更复杂的算法来生成一个自然的过渡动画。我们实验室在Siggraph 2022提出的 **MIB (Motion In Between)** 工作，就是研究如何为角色生成实时可控的、高质量的运动过渡。

### **第三部分：人脸表情动画**

身体动画赋予了角色生命，而**人脸动画**则赋予了角色灵魂。它是计算机动画中最具挑战性，也最具感染力的领域。

#### **3.1 核心挑战：参数化 (Parameterization)**

一个高精度的人脸模型有数万甚至数十万个顶点。我们不可能为每个顶点都去K帧。核心问题是：**如何构建一个低维度的、直观的参数空间，用少数几个参数（比如几个滑条）来控制复杂的面部表情变化？**

这个过程在身体动画中叫 **Rigging**，在面部动画中也是如此。我们希望建立一个映射关系： $E = E(w)$ 其中，$w$ 是低维的控制参数（比如微笑、愤怒、张嘴的程度），$E$ 是高维的人脸顶点位置（表情空间）。

#### **3.2 经典方法：3D可变形人脸模型 (Morphable 3D Faces)**

这是1999年SIGGRAPH上由Volker Blanz和Thomas Vetter提出的里程碑式的工作，它为基于数据驱动的人脸建模和动画奠定了基础。

**核心思想**: 通过分析一个包含大量真人3D人脸扫描的数据库，学习到一个“平均脸”以及人脸形状和纹理的**主要变化模式**。任何一张新的人脸都可以被表示为“平均脸”与这些变化模式的线性组合。

**数学推导 (基于主成分分析PCA)**:

1.  **数据表示**:
    
    *   假设我们有 $m$ 个3D人脸扫描，每个模型都有 $n$ 个顶点，且顶点之间已经建立了**稠密对应关系**（即第 $i$ 个模型上的第 $j$ 个顶点，对应的是第 $k$ 个模型上的第 $j$ 个顶点，它们都代表脸上的同一个语义点，比如左眼角）。
    *   我们可以将每个模型的几何形状 $S\_i$ 和纹理 $T\_i$ 展平成一个长向量： $$S\_i = (X\_1, Y\_1, Z\_1, ..., X\_n, Y\_n, Z\_n)^T \\in \\mathbb{R}^{3n}$$ $$T\_i = (R\_1, G\_1, B\_1, ..., R\_n, G\_n, B\_n)^T \\in \\mathbb{R}^{3n}$$
2.  **计算平均脸**: 计算所有样本的平均形状 $\\bar{S}$ 和平均纹理 $\\bar{T}$： $$\\bar{S} = \\frac{1}{m}\\sum\_{i=1}^{m} S\_i$$ $$\\bar{T} = \\frac{1}{m}\\sum\_{i=1}^{m} T\_i$$
    
3.  **计算差异**: 计算每个样本与平均脸的差异： $$\\Delta S\_i = S\_i - \\bar{S}$$ $$\\Delta T\_i = T\_i - \\bar{T}$$
    
4.  **主成分分析 (PCA)**:
    
    *   将所有差异向量 $\\Delta S\_i$ 构成一个数据矩阵。对这个矩阵进行PCA分析（本质上是计算数据协方差矩阵的特征向量）。
    *   PCA会找到一组正交的基向量（特征脸）${s\_1, s\_2, ..., s\_{m-1}}$，它们按重要性（特征值大小）排序。这些基向量代表了人脸形状最主要的变化方向（例如，第一个基向量可能控制脸的胖瘦，第二个控制脸的长短等）。
    *   对纹理差异向量 $\\Delta T\_i$ 做同样的操作，得到纹理的基向量 ${t\_1, t\_2, ..., t\_{m-1}}$。
5.  **新的模型表示**: 现在，任何一张新的人脸都可以通过下面的公式来合成： $$S\_{model} = \\bar{S} + \\sum\_{i=1}^{m-1} a\_i s\_i$$ $$T\_{model} = \\bar{T} + \\sum\_{i=1}^{m-1} b\_i t\_i$$
    
    这里的系数 ${a\_i}$ 和 ${b\_i}$ 就是我们想要的**低维、直观的控制参数**。通过调整这些系数，我们就可以在“人脸空间”中生成无穷无尽、千变万化的新面孔。
    

这个强大的模型不仅可以用于人脸生成，还可以通过优化算法，从一张2D照片中反向求解出最匹配的系数，从而重建出三维人脸模型。这一思想也被扩展到了人体，催生了著名的**SMPL (Skinned Multi-Person Linear body model)** 模型。

#### **3.3 工业界标准：Blend Shapes**

尽管可变形模型在学术上非常优雅，但在实际的动画制作流程中，**Blend Shapes（融合变形）** 是更常用、更直观、艺术家更友好的技术。

**核心思想**:

*   艺术家预先雕刻好一系列**关键表情姿态 (Key Poses)**，比如“微笑”、“愤怒”、“张大嘴”、“O”型嘴等。这些模型被称为 **Blend Shapes** 或 **Targets**。
*   所有这些Blend Shape模型都与一个中性的基础模型（Base Mesh）拥有完全相同的顶点数量和拓扑结构。它们之间的区别仅仅在于顶点位置。
*   任意一个复杂的、介于中间的表情，都可以通过对这些关键表情进行**加权线性插值**来得到。

**数学表示**: 设最终的表情网格为 $S\_{final}$，基础网格为 $S\_{base}$，我们有 $F$ 个Blend Shape目标 $S\_1, S\_2, ..., S\_F$。那么：

$$S\_{final} = S\_{base} + \\sum\_{f=1}^{F} c\_f (S\_f - S\_{base})$$ 其中 $c\_f$ 是第 $f$ 个Blend Shape的权重，通常在 $\[0, 1\]$ 区间内。

如果我们将每个表情表示为相对于基础网格的**位移向量** $\\Delta\_f = S\_f - S\_{base}$，公式可以简化为：

$$S\_{final} = S\_{base} + \\sum\_{f=1}^{F} c\_f \\Delta\_f$$

**Face IK**: 直接控制几十上百个Blend Shape权重对动画师来说依然是繁琐的。动画师更喜欢像操作木偶一样，直接拖拽脸上的几个控制点（比如嘴角、眉毛），然后让整个表情自然地形成。这就是 **Face IK**。

**数学原理 (最小二乘法求解)**:

*   **输入**: 动画师在主模型上选择了 $L$ 个控制顶点，并指定了它们的目标位置 $\\mathbf{P}\_1, \\mathbf{P}\_2, ..., \\mathbf{P}\_L$。
*   **目标**: 求解出一组Blend Shape权重 $c\_1, c\_2, ..., c\_F$，使得这 $L$ 个顶点在混合后的位置最接近目标位置。
*   **方程建立**: 对于每一个控制顶点 $l$，其混合后的位置为： $$\\mathbf{S}\_{l,final} = \\mathbf{S}\_{l,base} + \\sum\_{f=1}^{F} c\_f (\\mathbf{S}\_{l,f} - \\mathbf{S}\_{l,base})$$ 我们希望 $\\mathbf{S}\_{l,final} = \\mathbf{P}\_l$。整理后得到： $$\\sum\_{f=1}^{F} c\_f \\Delta\_{l,f} = \\mathbf{P}\_l - \\mathbf{S}\_{l,base}$$ 这是一个关于未知数 $c\_f$ 的**线性方程组**。由于控制点数量 $L$ 通常远小于Blend Shape数量 $F$，这是一个欠定方程组。我们可以使用**带约束的最小二乘法**来求解，找到一个既满足控制点约束，又使权重变化尽可能平滑的解。

### **第四部分：案例研究 - The Digital Emily Project**

为了让大家更具体地理解照片级真实感人脸动画的制作流程，我们来看一个经典的案例：南加州大学ICT实验室的**The Digital Emily Project**。

这个项目完整地展示了从数据采集到最终动画的全过程，其工作流程至今仍是行业标杆。

**动画流程**:

1.  **表演捕获 (Performance Capture)**: 演员在一个布满摄像机和光源的特殊设备（Light Stage）中进行表演。
2.  **表演分析 (Performance Analysis)**: 利用计算机视觉技术，从拍摄的视频中跟踪演员面部的细微特征。
3.  **角色构建与绑定 (Rigging)**: 并行地，艺术家会为CG角色构建一个高质量的三维模型，并为其创建一套复杂的面部绑定系统（通常是基于Blend Shapes）。
4.  **表情迁移 (Retargeting)**: 将从真人表演中分析出的表情数据，迁移到CG角色的绑定系统上，驱动CG角色做出与真人相同的表情。
5.  **关键帧动画 (Keyframe Animation)**: 动画师在迁移结果的基础上进行手动的调整和精修，以达到最终的艺术效果。

**数据获取的细节**:

*   **FACS (Facial Action Coding System)**: 为了系统地捕捉所有可能的表情，项目采用了由心理学家Paul Ekman提出的面部动作编码系统。该系统将人脸肌肉运动分解为一系列**动作单元 (Action Units, AU)**。演员被要求依次做出33个基本的FACS表情，并保持几秒钟。
*   **Light Stage 5**: 这是一个测地线穹顶，内部装有156个LED光源。它可以在极短的时间内（3秒），从不同角度、用不同光照（如Diffuse、Specular）拍摄演员的脸部，从而获取：
    *   **高精度几何 (Geometry)**: 通过投射条纹图案，重建出皮肤毛孔级别的细节。
    *   **高精度纹理 (Texture)**: 分离出漫反射贴图和高光贴图。
    *   **反射属性 (Reflectance Properties)**: 分析皮肤的次表面散射等光学特性。

**角色构建的挑战与解决方案**:

*   **原始扫描数据的问题**: 原始扫描出的模型拓扑混乱、有破洞（如眼睛、牙齿区域）、覆盖范围不一致。
*   **构建流程**:
    1.  **重新拓扑 (Re-mesh)**: 美术师对一个中性表情的扫描模型进行手工的重新拓扑，创建一个干净、规则、适合绑定的**主网格 (Master Mesh)**。
    2.  **建立对应关系**: 这是最关键的一步。需要找到主网格上的每一个顶点，与所有其他表情扫描模型上的对应点。这通常通过一个半自动的过程完成：
        *   在脸上贴上标记点，提供**稀疏对应**。
        *   使用光流等算法，结合稀疏标记点，计算出**稠密对应**。
    3.  **建立Blend Shapes**: 有了对应关系，就可以将每个表情扫描的形变信息，传递给主网格，生成一系列与主网格拓扑一致的Blend Shape目标。

**最终，我们就拥有了一套高质量的Blend Shape表情库，可以用于前面提到的Face IK或直接权重控制，来实现照片级的实时表情动画。** 这一整套技术后来被广泛应用于电影《返老还童》等作品中。