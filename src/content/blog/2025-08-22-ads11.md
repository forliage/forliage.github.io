---
title: "ads11:近似算法(Approximation)"
description: ""
pubDate: "2025-08-22"
heroImage: ""
---

# ads11:近似算法(Approximation)

### **第一部分：为何我们需要“近似”？直面计算的极限**

我们首先要问一个根本性的问题：我们为什么要满足于一个“近似”的解，而不是最优解呢？答案在于，我们面对着一类被称为**NP-hard**的“硬”问题。

这些问题，比如著名的旅行商问题（TSP）、集合覆盖问题以及我们稍后会详细讨论的装箱问题和背包问题，都具有一个共同的特点：到目前为止，还没有人能找到一个能在多项式时间内（即高效地）求解它们最优解的算法。它们的计算复杂度往往是指数级的，例如 $O(2^N)$。

这意味着，当问题规模 $N$ 稍微增大一点，计算时间就会爆炸式增长，甚至超出全球所有计算机的算力总和。

面对这种困境，我们有三条路可以走：

1.  **接受指数时间**：如果问题规模 `N` 非常小（比如 N < 30），$O(2^N)$ 的算法或许还能在可接受的时间内运行完毕。但这显然不是一个普适的解决方案。
2.  **求解特例**：有些 NP-hard 问题在特定条件下会“退化”成简单问题。例如，虽然一般图的顶点覆盖问题是 NP-hard 的，但在二分图上却存在多项式时间的解法。这种方法很有价值，但它无法解决问题的普遍情况。
3.  **放宽对“最优”的执着**：这是我们今天的核心。如果我们不再强求那个遥不可及的最优解，而是转向寻找一个**在多项式时间内可以找到的、并且质量有保证的“次优解”或“近似解”**，那么问题就变得豁然开朗。

第三条路，正是由**近似算法 (Approximation Algorithm)** 铺就的。我们的目标是：**在效率（多项式时间）和效果（接近最优）之间取得最佳平衡**。

### **第二部分：衡量的标尺 —— 近似理论的核心概念**

既然是“近似”，就必须有一个精确的数学工具来衡量我们的解到底有多“好”。这个工具就是**近似比 (Approximation Ratio)**。

#### **1\. 近似比 (Approximation Ratio)**

【**定义**】 对于一个优化问题，我们假设 $C$ 是我们的近似算法给出的解的成本（对于最小化问题，成本是解的目标函数值；对于最大化问题，成本也是目标函数值），而 $C^\*$ 是该问题最优解的成本。如果对于任何规模为 $n$ 的输入，这两个成本的比值都满足以下不等式：

$$ \\max \\left( \\frac{C}{C^\*}, \\frac{C^\*}{C} \\right) \\le \\rho(n) $$

我们就称这个算法拥有一个大小为 $\\rho(n)$ 的**近似比**。这里 $\\rho(n) \\ge 1$。

这个定义可以根据问题的类型简化：

*   **对于最小化问题**（如装箱问题，目标是箱子数最小），我们总是有 $C \\ge C^\* $。此时，近似比的定义简化为：$\\frac{C}{C^\*} \\le \\rho(n)$。一个近似比为 2 的算法意味着它使用的箱子数最坏不会超过最优解的两倍。
*   **对于最大化问题**（如背包问题，目标是利润最大），我们总是有 $C \\le C^\* $。此时，近似比的定义简化为：$\\frac{C^\*}{C} \\le \\rho(n)$。一个近似比为 2 的算法意味着最优解的利润最坏不会超过我们的解的两倍（或者说，我们的解至少达到了最优解利润的 50%）。

如果一个算法的近似比为 $\\rho(n)$，我们就称它是一个 **$\\rho(n)$-近似算法**。

#### **2\. 近似方案 (Approximation Scheme)**

有些算法更加强大，它们像一个可以调节精度的旋钮，允许用户自己决定想要的近似程度。

【**定义**】 一个**近似方案**是一种特殊的近似算法。它除了接收问题的实例作为输入外，还接收一个额外的参数 $\\epsilon > 0$。对于任何给定的 $\\epsilon$，该算法都能成为一个 **$(1+\\epsilon)$-近似算法**。

这意味着，我们可以通过减小 $\\epsilon$ 来让解无限地逼近最优解。比如，我们想要一个误差在 1% 以内的解，只需设置 $\\epsilon = 0.01$，算法就能保证其结果与最优解的差距在 1% 之内。

根据运行时间对 $\\epsilon$ 的依赖程度，近似方案又分为两类：

*   **多项式时间近似方案 (PTAS - Polynomial-Time Approximation Scheme)** 对于**任意固定**的 $\\epsilon > 0$，算法的运行时间是输入规模 `n` 的多项式。例如，运行时间可能是 $O(n^{2/\\epsilon})$。请注意，当 $\\epsilon$ 变得非常小时，指数项 $2/\\epsilon$ 会变得巨大，导致算法非常慢。但只要 $\\epsilon$ 固定，运行时间关于 `n` 的增长就是多项式的。
    
*   **全多项式时间近似方案 (FPTAS - Fully Polynomial-Time Approximation Scheme)** 这是近似算法的“圣杯”。算法的运行时间**同时**是输入规模 `n` 和 $1/\\epsilon$ 的多项式。例如，$O\\left(\\left(\\frac{1}{\\epsilon}\\right)^2 n^3\\right)$。这意味着即使我们要求非常高的精度（即 $\\epsilon$ 很小），算法的效率依然能够保持在多项式级别。
    

graph TD A\[近似算法\] --> B\["近似比 ρ"\] A --> C\["近似方案: (1+ε)-近似"\] C --> D\["**PTAS**  
运行时间是 n 的多项式  
(对固定的 ε)  
e.g., O(n1/ε)"\] C --> E\["**FPTAS**  
运行时间是 n 和 1/ε 的多项式  
e.g., O((1/ε)²n³)"\] subgraph "性能与灵活性" E -- "更优" --> D end

### **第三部分：案例研究一 —— 装箱问题 (Bin Packing)**

让我们通过一个经典的 NP-hard 问题来感受近似算法的魅力。

**问题描述**： 给定 `N` 个物品，其尺寸分别为 $S\_1, S\_2, ..., S\_N$，其中 $0 < S\_i \\le 1$。我们的任务是将这些物品装入最少数目的箱子中，每个箱子的容量都是单位 1。

**【一个例子】**

*   **输入**: N = 7, 物品尺寸 $S\_i$ = {0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8}
*   **一个最优解**: 需要 3 个箱子。
    *   $B\_1 = {0.8, 0.2}$ (总尺寸 = 1.0)
    *   $B\_2 = {0.7, 0.3}$ (总尺寸 = 1.0)
    *   $B\_3 = {0.5, 0.4, 0.1}$ (总尺寸 = 1.0)

graph TD subgraph "最优装箱 (3个箱子)" subgraph "B1 (已用 1.0)" direction TB b1\_1\["0.8"\] --> b1\_2\["0.2"\] end subgraph "B2 (已用 1.0)" direction TB b2\_1\["0.7"\] --> b2\_2\["0.3"\] end subgraph "B3 (已用 1.0)" direction TB b3\_1\["0.5"\] --> b3\_2\["0.4"\] --> b3\_3\["0.1"\] end end

#### **1\. 在线算法**

在线算法模拟的是一种信息受限的决策过程：每来一个物品，你必须立刻决定把它放进哪个箱子，并且这个决定是**不可撤销**的。你对未来将要到来的物品一无所知。

##### **算法1：Next Fit (邻近适应算法)**

这是最简单、最“健忘”的策略。

*   **规则**: 始终只关注当前正在装填的箱子。当一个新物品到来时，检查它是否能放入**当前箱子**。如果能，就放进去。如果不能，就**永久关闭**当前箱子，开启一个新箱子来装这个物品。

**伪代码:**

```
void NextFit(const vector<double>& items) {
    if (items.empty()) return;
    int bin_count = 1;
    double current_bin_capacity = 1.0;

    for (double item_size : items) {
        if (item_size <= current_bin_capacity) {
            current_bin_capacity -= item_size;
        } else {
            bin_count++;
            current_bin_capacity = 1.0 - item_size;
        }
    }
    // ... return bin_count ...
}
```

**【定理】**: 令 M 为最优解所需的箱子数。Next Fit 算法使用的箱子数不会超过 $2M - 1$。

**【严谨证明】**: 让我们用反证法。假设 Next Fit 使用了 $K \\ge 2M$ 个箱子，分别为 $B\_1, B\_2, ..., B\_K$。 根据 Next Fit 的规则，当算法决定开启箱子 $B\_{i+1}$ 时，意味着上一个物品放不进 $B\_i$。因此，箱子 $B\_i$ 的已用空间和箱子 $B\_{i+1}$ 的已用空间之和必然大于1。否则， $B\_i$ 和 $B\_{i+1}$ 的所有物品本可以被连续地放入一个箱子中。 考虑任意两个相邻的箱子 $B\_i$ 和 $B\_{i+1}$。它们所装物品的总尺寸 $S(B\_i) + S(B\_{i+1}) > 1$。

现在，我们将这 $K$ 个箱子两两配对：$(B\_1, B\_2), (B\_3, B\_4), \\dots$。由于 $K \\ge 2M$，我们至少可以配出 $M$ 对。 对于每一对 $(B\_{2i-1}, B\_{2i})$，我们有 $S(B\_{2i-1}) + S(B\_{2i}) > 1$。 将这 $M$ 个不等式相加，我们得到所有物品的总尺寸： $$ \\sum\_{j=1}^{K} S(B\_j) \\ge \\sum\_{i=1}^{M} (S(B\_{2i-1}) + S(B\_{2i})) > \\sum\_{i=1}^{M} 1 = M $$ 所有物品的总尺寸严格大于 $M$。

另一方面，我们知道一个基本的下界：最优解所需的箱子数 $M$ 必须至少能装下所有物品的总和。即： $$ M \\ge \\sum\_{\\text{all items}} \\text{size} i $$ 因为箱子容量为1，所以更准确的下界是向上取整： $$ M \\ge \\left\\lceil \\sum{\\text{all items}} \\text{size}i \\right\\rceil = \\left\\lceil \\sum{j=1}^{K} S(B\_j) \\right\\rceil $$ 我们已经证明了 $\\sum\\limits\_{j=1}^{K} S(B\_j) > M$。一个大于 $M$ 的数向上取整，结果必然至少是 $M+1$。 于是我们推导出 $M \\ge M+1$，这是一个明显的矛盾！ 因此，我们的初始假设“Next Fit 使用了 $K \\ge 2M$ 个箱子”是错误的。它最多使用 $2M-1$ 个箱子。 这个定理表明 Next Fit 是一个 **2-近似算法**（对于较大的M，-1可以忽略）。

##### **算法2：First Fit (首次适应算法)**

First Fit 相比 Next Fit 更有“记忆力”。

*   **规则**: 处理每一个新物品时，**从第一个箱子开始**依次检查，将它放入**第一个**能容纳它的已开箱子。如果所有已开箱子都放不下，才打开一个新箱子。

**【定理】**: First Fit 算法使用的箱子数不会超过 $\\frac{17}{10}M + 2$。这是一个比 Next Fit 更好的近似保证，大约是 1.7-近似。通过使用平衡二叉搜索树等数据结构维护箱子的剩余空间，其时间复杂度可以优化到 $O(N \\log N)$。

##### **算法3：Best Fit (最佳适应算法)**

Best Fit 更加“精打细算”。

*   **规则**: 处理每一个新物品时，遍历所有已开箱子，找到一个能放下该物品且**剩余空间最小**的箱子（即最“紧凑”的那个），然后放进去。如果都放不下，才开新箱子。

**【定理】**: Best Fit 的近似比与 First Fit 类似，最坏情况下也是 1.7-近似，时间复杂度同样可优化至 $O(N \\log N)$。

##### **【实例演算】**

现在，我们来亲自一下开头那个例子，看看这三种在线算法的表现。 **输入**: $S\_i$ = {0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8}

*   **Next Fit**:
    
    1.  `0.2` -> $B\_1$: {0.2} (当前箱)
    2.  `0.5` -> 放入 $B\_1$ -> $B\_1$: {0.2, 0.5} (当前箱)
    3.  `0.4` -> $B\_1$ 放不下 (0.7+0.4>1) -> 开 $B\_2$: {0.4} (当前箱)
    4.  `0.7` -> $B\_2$ 放不下 (0.4+0.7>1) -> 开 $B\_3$: {0.7} (当前箱)
    5.  `0.1` -> 放入 $B\_3$ -> $B\_3$: {0.7, 0.1} (当前箱)
    6.  `0.3` -> $B\_3$ 放不下 (0.8+0.3>1) -> 开 $B\_4$: {0.3} (当前箱)
    7.  `0.8` -> $B\_4$ 放不下 (0.3+0.8>1) -> 开 $B\_5$: {0.8} (当前箱) **结果: 5 个箱子。**
*   **First Fit**:
    
    1.  `0.2` -> $B\_1$: {0.2}
    2.  `0.5` -> 放入 $B\_1$: {0.2, 0.5}
    3.  `0.4` -> $B\_1$ 放不下 -> 开 $B\_2$: {0.4}
    4.  `0.7` -> $B\_1, B\_2$ 都放不下 -> 开 $B\_3$: {0.7}
    5.  `0.1` -> 检查 $B\_1$，能放下 -> $B\_1$: {0.2, 0.5, 0.1}
    6.  `0.3` -> $B\_1$ 放不下，检查 $B\_2$，能放下 -> $B\_2$: {0.4, 0.3}
    7.  `0.8` -> $B\_1, B\_2, B\_3$ 都放不下 -> 开 $B\_4$: {0.8} **结果: 4 个箱子。**
*   **Best Fit**:
    
    1.  `0.2` -> $B\_1$: {0.2} (剩 0.8)
    2.  `0.5` -> 放入 $B\_1$: {0.2, 0.5} (剩 0.3)
    3.  `0.4` -> $B\_1$ 放不下 -> 开 $B\_2$: {0.4} (剩 0.6)
    4.  `0.7` -> $B\_1, B\_2$ 都放不下 -> 开 $B\_3$: {0.7} (剩 0.3)
    5.  `0.1` -> 放入 $B\_1$ 剩 0.2，放入 $B\_2$ 剩 0.5，放入 $B\_3$ 剩 0.2。$B\_1, B\_3$ 都是最紧的，选第一个 -> $B\_1$: {0.2, 0.5, 0.1} (剩 0.2)
    6.  `0.3` -> 放入 $B\_2$ 剩 0.3，放入 $B\_3$ 剩 0。$B\_3$ 是最佳选择 -> $B\_3$: {0.7, 0.3} (剩 0)
    7.  `0.8` -> $B\_1, B\_2, B\_3$ 都放不下 -> 开 $B\_4$: {0.8} **结果: 4 个箱子。**

**结论**: 在这个例子中，First Fit 和 Best Fit 表现优于 Next Fit，但仍未达到最优的 3 箱。这揭示了在线算法的固有局限性。

**【定理】**: 存在特定的输入，会迫使**任何**在线装箱算法使用的箱子数至少是最优解的 $\\frac{4}{3}$ 倍（一个更早的结论是5/3）。因为它们无法预知未来，总可能被“刁钻”的输入序列所欺骗。

#### **2\. 离线算法**

离线算法拥有“上帝视角”，它可以在开始决策前**审视所有**待装的物品。这给了我们优化的巨大空间。

**核心洞察**: 装箱问题的“麻烦制造者”通常是那些**大件物品**，它们既占空间又不容易和其它小件凑整。一个自然的想法是：**优先处理大件**。

##### **算法4：First Fit Decreasing (FFD)**

*   **规则**:
    1.  将所有物品按尺寸**从大到小**进行排序。
    2.  按照排序后的顺序，对每个物品使用 First Fit 策略。

**【示例重演】**:

*   **排序后**: {0.8, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1}
*   **FFD过程**:
    1.  `0.8` -> $B\_1$: {0.8}
    2.  `0.7` -> $B\_2$: {0.7}
    3.  `0.5` -> $B\_3$: {0.5}
    4.  `0.4` -> 放入 $B\_3$ (因为 $B\_1, B\_2$ 放不下) -> $B\_3$: {0.5, 0.4}
    5.  `0.3` -> 放入 $B\_2$ -> $B\_2$: {0.7, 0.3}
    6.  `0.2` -> 放入 $B\_1$ -> $B\_1$: {0.8, 0.2}
    7.  `0.1` -> 放入 $B\_3$ -> $B\_3$: {0.5, 0.4, 0.1}
*   **结果**: 3 个箱子！ 我们这次通过简单的预排序，就达到了最优解！

graph TD subgraph "FFD 装箱过程 (达到最优)" direction TB subgraph "B1 (已用 1.0)" b1\_1\["0.8"\] --> b1\_2\["0.2"\] end subgraph "B2 (已用 1.0)" b2\_1\["0.7"\] --> b2\_2\["0.3"\] end subgraph "B3 (已用 1.0)" b3\_1\["0.5"\] --> b3\_2\["0.4"\] --> b3\_3\["0.1"\] end end

**【定理】**: FFD 算法使用的箱子数 $N\_{FFD}$ 不会超过 $\\frac{11}{9}M + \\frac{6}{9}$。 这是一个非常出色的近似比，大约是 1.22。实践证明，FFD 在绝大多数情况下都表现得极其接近最优。这有力地说明了：**简单的贪心启发式，如果用对地方，就能产生巨大的威力。**

### **第四部分：案例研究二 —— 背包问题 (Knapsack Problem)**

这是另一个经典的优化问题，与资源分配息息相关。

#### **1\. 分数背包问题：可分割的宝藏**

**问题描述**: 有一个容量为 $M$ 的背包和 $N$ 件物品。每件物品 $i$ 有重量 $w\_i$ 和利润 $p\_i$。你可以选择将物品的**任意一部分**（分数）放入背包。目标是使装入背包的物品总利润最大化。

**数学模型**: 最大化 $\\sum\\limits\_{i=1}^{n} p\_i x\_i$ 约束条件: $\\sum\\limits\_{i=1}^{n} w\_i x\_i \\le M$ 并且 $0 \\le x\_i \\le 1$

**贪心策略**: 这个问题存在一个简单且**最优**的贪心解法。我们应该优先装哪个物品？不是最贵的，也不是最轻的，而是**性价比最高**的！我们计算每个物品的**利润密度** $p\_i / w\_i$，然后按密度从高到低依次装入。

**【示例】**:

*   $n = 3, M = 20$
*   $(p\_1, p\_2, p\_3) = (25, 24, 15)$
*   $(w\_1, w\_2, w\_3) = (18, 15, 10)$

1.  **计算密度**: $d\_1 = 25/18 \\approx 1.39$, $d\_2 = 24/15 = 1.6$, $d\_3 = 15/10 = 1.5$
2.  **排序**: Item 2 > Item 3 > Item 1
3.  **装包**:
    *   装入全部 Item 2 $(w=15, p=24)$。背包剩余容量 $20 - 15 = 5$。
    *   装入 Item 3 $(w=10, p=15)$。背包容量只够装 $5/10 = 0.5$ 的 Item 3。获得利润 $15 \\times 0.5 = 7.5$。
    *   背包已满。 **最优解**: 装入1个Item 2和0.5个Item 3，总利润 $P$ = $24 + 7.5 = 31.5$。

#### **2\. 0-1 背包问题：不可分割的抉择**

这是背包问题的 NP-hard 版本。

**问题描述**: 与分数背包类似，但每个物品要么**整个装入**，要么**完全不装**。

**贪心策略的“滑铁卢”**: 我们沿用刚才的“按密度贪心”策略，看看会发生什么。

**【示例】**:

*   $n = 5, M = 11$
*   $p = (1, 6, 18, 22, 28)$
*   $w = (1, 2, 5, 6, 7)$
*   **密度**: $d=(1, 3, 3.6, 3.67, 4)$
*   **贪心顺序**: Item 5 -> 4 -> 3 -> 2 -> 1

1.  **贪心解**:
    
    *   装入 Item 5 $(w=7, p=28)$。剩余容量 $11 - 7 = 4$。
    *   Item 4 $(w=6)$ 和 Item 3 $(w=5)$ 都装不下了。
    *   装入 Item 2 $(w=2, p=6)$。剩余容量 $4 - 2 = 2$。
    *   装入 Item 1 $(w=1, p=1)$。
    *   **贪心解**: {Item 1, 2, 5}。总重量 10，总利润 $1+6+28 = 35$。
2.  **最优解**:
    
    *   是 {Item 3, Item 4}。总重量 11，总利润 $18+22 = 40$。

贪心算法失败了。但它的表现有多差呢？

**【定理】**: 对于 0-1 背包问题，一个改进的贪心策略（按密度贪心，然后与能装下的单个最大价值物品比较，取较大者）是一个 **2-近似算法**。

**【证明思路】**: 令 $P\_{greedy}$ 为贪心算法得到的利润，$P\_{opt}$ 为最优利润。令 $P\_{frac}$ 为分数背包的最优利润，我们知道 $P\_{opt} \\le P\_{frac}$。 在贪心过程中，当第一个物品 $k$ 装不进去时，之前装的物品总利润为 $P\_{greedy}$。分数背包的解此时会装入一部分 $k$，所以有 $$P\_{frac} \\le P\_{greedy} + p\_k$$ 结合起来，我们有 $$P\_{opt} \\le P\_{greedy} + p\_k$$ 此时，最优解 $P\_{opt}$ 不会同时比 $P\_{greedy}$ 和 $p\_k$ 都大很多。具体来说，$$P\_{opt} \\le P\_{greedy} + p\_k \\le 2 \\max(P\_{greedy}, p\_k)$$如果我们算法的输出是 $\\max(P\_{greedy}, \\text{单个能装下的最大利润物品})$，就可以保证 2-近似。

#### **3\. 动态规划：追求最优的代价**

0-1背包问题虽然是NP-hard，但它有一个著名的**伪多项式时间**解法——动态规划。

**定义状态**: $W\_{i,p}$ 表示从前 $i$ 个物品中，凑出**恰好**为 $p$ 的总利润所需要的**最小重量**。 **状态转移方程**: $$ W\_{i,p} = \\begin{cases} \\infty & \\text{if } i=0, p>0 \\\\ 0 & \\text{if } i=0, p=0 \\\\ W\_{i-1, p} & \\text{if item } i \\text{ is not taken (} p\_i > p \\text{ or we choose not to)} \\\\ \\min{ W\_{i-1,p}, w\_i + W\_{i-1,p-p\_i} } & \\text{otherwise} \\end{cases} $$ **时间复杂度**: $O(n \\cdot P\_{total})$，其中 $P\_{total}$ 是所有物品的总利润，或者更精确地说是 $O(n \\cdot n \\cdot p\_{max})$。 问题来了：如果利润值 $p\_{max}$ 非常巨大（比如64位整数），这个算法的运行时间就不再是多项式了，因为它依赖于输入数值的大小，而不是输入的长度。

**【示例】**: 看这两组物品，背包容量 $M=11$:

Item

Profit

Weight

1

134,221

1

2

656,342

2

3

1,810,013

5

4

22,217,800

6

5

28,343,199

7

Item

Profit

Weight

1

2

1

2

7

2

3

19

5

4

223

6

5

284

7

左边这组数据的利润值极大，会导致动态规划失效。而右边这组数据，其实是左边利润值经过某种“压缩”得到的。两组数据的最优解是相同的！

这启发我们：我们能否通过**牺牲一点点精度**来**大幅降低利润值的范围**，从而让动态规划算法变得可用？

#### **4\. 打造 FPTAS：缩放与近似的艺术**

这正是构建0-1背包问题 FPTAS 的核心思想。

**算法步骤**:

1.  给定一个精度参数 $\\epsilon > 0$。
2.  设 $P\_{max}$ 是所有物品中的最大利润。
3.  定义一个缩放因子 $K = \\frac{\\epsilon \\cdot P\_{max}}{n}$。
4.  对于每个物品 `i`，创建一个新的、被“压缩”的利润值 $p'\_i = \\lfloor \\frac{p\_i}{K} \\rfloor$。
5.  使用动态规划算法，以新的利润 $p'\_i$ 和原始重量 $w\_i$ 来解决这个“压缩后”的背包问题。
6.  返回动态规划找到的解所对应的**原始总利润**。

**分析**:

*   **运行时间**: 新问题的最大利润 $$p'{max} = \\lfloor \\frac{P{max}}{K} \\rfloor = \\lfloor \\frac{P\_{max}}{\\epsilon P\_{max} / n} \\rfloor = \\lfloor \\frac{n}{\\epsilon} \\rfloor$$ 动态规划的时间复杂度变为 $$O(n \\cdot \\sum p'i) = O(n \\cdot n \\cdot p'{max}) = O(n^2 \\cdot \\frac{n}{\\epsilon}) = O(\\frac{n^3}{\\epsilon})$$ 这是一个关于 $n$ 和 $1/\\epsilon$ 的多项式，所以这是一个 **FPTAS**！
*   **近似比证明 (思路)**: 每一步的取整操作 $\\lfloor \\cdot \\rfloor$ 会带来误差。对于每个物品 $i$，我们有 $$K p'\_i \\le p\_i \\le K p'\_i + K$$ 设最优解包含的物品集合为 $O$，我们的算法找到的解的集合为 $A$。 最优解的总利润 $$P^\* = \\sum\_{i \\in O} p\_i$$ 我们的算法得到的总利润 $$P\_{alg} = \\sum\_{j \\in A} p\_j$$ 可以证明，这个算法得到的解 $P\_{alg}$ 满足 $P\_{alg} \\ge (1-\\epsilon)P^\*$，这等价于一个 $(1+\\epsilon')$ 的近似比。关键在于证明，由缩放带来的总误差 $\\sum K$ 最多为 $nK = \\epsilon P\_{max}$，而 $P\_{max} \\le P^\*$，所以总误差不超过 $\\epsilon P^\*$。

### **第五部分：案例研究三 —— K-中心问题 (K-center Problem)**

这是一个在设施选址、数据聚类等领域有广泛应用的问题。

**问题描述**:

*   **输入**: $n$ 个“站点”（比如居民区、客户位置）$s\_1, ..., s\_n$ 和一个整数$K$。
*   **目标**: 从这些站点中选择 $K$ 个作为“中心”（比如建基站、开超市），使得每个站点到其**最近**的那个中心的**最大距离**被最小化。

graph TD subgraph "K-Center Problem (K=4)" subgraph Cluster1 C1((center)) S1\_1(site) --- C1 S1\_2(site) --- C1 S1\_3(site) --- C1 end subgraph Cluster2 C2((center)) S2\_1(site) --- C2 S2\_2(site) --- C2 end subgraph Cluster3 C3((center)) S3\_1(site) --- C3 S3\_2(site) --- C3 end subgraph Cluster4 C4((center)) S4\_1(site) --- C4 S4\_2(site) --- C4 S4\_3(site) --- C4 end end style C1,C2,C3,C4 fill:#369,stroke:#fff,stroke-width:2px,color:#fff Goal\["目标: 最小化最长的蓝线(覆盖半径)"\] style Goal fill:#lightyellow

我们假设距离满足度量空间的基本性质：

*   **同一性**: $dist(x, x) = 0$
*   **对称性**: $dist(x, y) = dist(y, x)$
*   **三角不等式**: $dist(x, z) \\le dist(x, y) + dist(y, z)$

#### **1\. 一个失败的贪心策略**

一个很自然的想法是：

1.  找到一个能使覆盖半径最小的“最佳”单中心位置。
2.  固定它，然后在此基础上再找第二个中心，使得新的覆盖半径最小。
3.  如此迭代 $K$ 次。

这个策略是\*\*任意差 (arbitrarily bad)\*\*的。如下图所示，如果 $K=2$，这个策略可能会把第一个中心放在两大簇点的正中间，导致第二个中心无论放在哪里，覆盖半径都非常大。而最优解是在每个簇里各放一个中心。

graph TD subgraph "糟糕的贪心选择 (K=2)" subgraph "第一步：选择全局最优的单中心" C((Center)) end subgraph "点集" S1(site); S2(site); S3(site); S4(site); S5(site); S6(site); S7(site); S8(site); S9(site); S10(site); end style C fill:red end subgraph "最优选择" C\_opt1((C1)); C\_opt2((C2)); style C\_opt1,C\_opt2 fill:green end

#### **2\. 一个成功的 2-近似贪心算法**

我们需要一个更聪明的策略。这个算法被称为 **Farthest-First Traversal**。

**算法步骤**:

1.  任意选择一个站点作为第一个中心 $c\_1$，加入中心集合 $C$。
2.  当 $|C| < K$ 时，循环执行：
    *   找到一个站点 $s$，它距离**当前已有的中心集合 $C$** 最远。（即，计算每个站点到它最近的中心的距离，取这个距离最大的那个站点 $s$）。
    *   将这个最远的站点 $s$ 作为新的中心，加入集合 $C$。
3.  返回集合 $C$。

**伪代码:**

```
vector<Site> GreedyKCenter(const vector<Site>& sites, int K) {
    if (sites.empty() || K <= 0) return {};
    vector<Site> centers;
    // 1. 任意选择第一个中心
    centers.push_back(sites[0]);

    vector<double> min_dist(sites.size(), numeric_limits<double>::max());

    while (centers.size() < K) {
        // 更新所有点到中心集的最近距离
        Site new_center = centers.back();
        double max_farthest_dist = -1;
        int farthest_site_idx = -1;

        for (int i = 0; i < sites.size(); ++i) {
            min_dist[i] = min(min_dist[i], distance(sites[i], new_center));
            if (min_dist[i] > max_farthest_dist) {
                max_farthest_dist = min_dist[i];
                farthest_site_idx = i;
            }
        }
        // 2. 选择最远的点作为下一个中心
        if (farthest_site_idx != -1) {
            centers.push_back(sites[farthest_site_idx]);
        } else { // All points are centers
            break;
        }
    }
    return centers;
}
```

**【定理】**: 上述贪心算法是一个 **2-近似算法**。即 $r(C) \\le 2r(C^\*)$，其中 $r(C)$ 是该算法解的覆盖半径， $r(C^\*)$ 是最优解的覆盖半径。

**【严谨证明】**: 我们再次使用反证法。令 $r^\*$ 为最优覆盖半径。假设我们的算法产生的覆盖半径 $r(C) > 2r^\*$。 这意味着，在我们的解中，存在一个站点 $s$，它到所有我们选择的 $K$ 个中心 $c\_1, ..., c\_K$ 的距离都大于 $2r^\*$。 特别地，当算法选择第 $K+1$ 个（假设的）中心 $c\_{K+1}$ 时（这个点就是最远的 $s$），我们有 $dist(c\_{K+1}, c\_i) > 2r^\*$ 对于所有 $i=1, ..., K$。 这样，我们就得到了 $K+1$ 个点 ${c\_1, ..., c\_{K+1}}$，它们两两之间的距离都大于 $2r^\*$。

现在考虑最优解。最优解用 $K$ 个中心 $c^\*\_1, ..., c^\*\_K$ 覆盖了所有的站点，其半径为 $r^\*$。这意味着，我们找到的这 $K+1$ 个点 ${c\_1, ..., c\_{K+1}}$ 也必然被这 $K$ 个最优中心所覆盖。 根据**鸽巢原理**，这 $K+1$ 个点中，必然至少有两个点，比如说 $c\_i$ 和 $c\_j$，被同一个最优中心 $c^\*\_m$ 所覆盖。 “被覆盖”意味着：

*   $dist(c\_i, c^\*\_m) \\le r^\*$
*   $dist(c\_j, c^\*\_m) \\le r^\*$

现在，利用**三角不等式**，我们可以估算 $c\_i$ 和 $c\_j$ 之间的距离： $$ dist(c\_i, c\_j) \\le dist(c\_i, c^\*\_m) + dist(c^\*\_m, c\_j) \\le r^\* + r^\* = 2r^\* $$ 这个结论 $dist(c\_i, c\_j) \\le 2r^\*$ 与我们之前的推论“它们两两之间的距离都大于 $2r^\*$”产生了直接的矛盾！ 因此，我们的初始假设“$r(C) > 2r^\*$”是错误的。必须有 $r(C) \\le 2r^\*$。证明完毕。

#### **3\. 近似比的极限**

这个 2-近似算法是不是已经足够好了？我们还能找到比如 1.5-近似或者 1.99-近似的算法吗？答案是**不能，除非 P=NP**。

**【定理】**: 对于任何 $\\rho < 2$，都不存在 K-中心问题的 $\\rho$-近似算法，除非 P=NP。

**【证明思路 (通过规约)】**: 这个证明的核心思想是，如果存在一个 $(2-\\epsilon)$-近似算法，我们就可以用它来在多项式时间内解决一个已知的 NP-complete 问题——**支配集 (Dominating Set)** 问题。

*   **支配集问题**: 在图 G=(V,E) 中，找到一个最小的顶点子集 $D \\subseteq V$，使得所有不在 D 中的顶点都至少与 D 中一个顶点相邻。
*   **规约**:
    1.  给定一个支配集问题实例图 G。
    2.  构造一个 K-中心问题实例：图 G 的所有顶点就是我们的“站点”。任意两点间的距离定义为：如果两点在 G 中有边相连，则距离为 1；否则距离为 2。
    3.  现在，问“是否存在一个大小为 K 的支配集”就等价于问“是否存在 K 个中心，使得覆盖半径为 1”。
*   **关键**:
    *   如果图 G 存在大小为 K 的支配集，那么 K-中心问题的最优半径 $r^\*$ 就是 1。
    *   如果我们有一个 $(2-\\epsilon)$-近似算法，它作用在这个实例上，返回的半径必然是 $(2-\\epsilon) \\times r^\* = 2-\\epsilon$。由于距离只能是整数1或2，所以返回的半径必须是 1。这意味着算法找到了一个最优解！
    *   因此，我们可以通过调用这个近似算法来判断是否存在大小为 K 的支配集，从而在多项式时间内解决了 NP-complete 问题。这是不可能的，除非 P=NP。
*   **结论**: 2 是我们能为 K-中心问题达到的最佳近似比。