---
title: "ads01：AVL树、Splay树和摊还分析"
description: ""
pubDate: "2025-08-21"
heroImage: ""
---

# ads01：AVL树、Splay树和摊还分析

（补档：将以前的ads笔记做修缮后上传）

### 第一部分：AVL树 (Adelson-Velskii-Landis Trees)

#### 1.1 问题的提出：为何需要AVL树？

让我们回顾一下二叉搜索树（BST）。

*   **目标 (Target):** 我们希望能够快速地进行查找、插入和删除操作。
*   **工具 (Tool):** 二叉搜索树。它的核心性质是：对于任意节点，其左子树中所有节点的值都小于它，右子树中所有节点的值都大于它。
*   **问题 (Problem):** 在BST中，一次查找、插入或删除操作的时间复杂度是 $O(\\text{height})$，即与树的高度成正比。

理想情况下，一棵包含 $N$ 个节点的树，其高度可以达到 $O(\\log N)$。但最坏情况下，树的高度可能退化成 $O(N)$，就像一个链表一样。

**【示例】月份的插入**

我们来看一个直观的例子。假设我们按顺序（“Jan”, “Feb”, “Mar”, ... , “Dec”）插入12个月份的英文名。由于月份的字典序是杂乱的，我们可能会得到一棵相对平衡的树，也可能得到一棵非常倾斜的树。

下面是两个例子：

1.  **按“Jan”到“Dec”顺序插入（部分示意）** 这会形成一棵比较倾斜的树。例如，“Jan”是根，“Feb”比“Jan”小，在左边。“Mar”比“Jan”大，在右边。“Apr”比“Jan”小，比“Feb”小，在“Feb”的左边... 最终形成的树会比较杂乱且不平衡。
    
    *   **平均查找时间 = 3.5** （需要遍历的平均节点数）
    *   **最坏查找时间**（查找最深的节点）会更长。如果插入顺序恰好是升序或降序，树会退化成链表，平均查找时间将是 $(1+2+...+N)/N$，约为 $N/2$，即 $O(N)$。对于12个月份，最坏情况（例如按字母序插入）的平均查找时间会是 $(1+2+...+12)/12 = 6.5$。
    
    graph TD Jan --> Feb Jan --> Mar Feb --> Apr Mar --> June Mar --> May Apr --> Aug Aug --> July July --> Dec May --> Sept Sept --> Oct Oct --> Nov
    
    _（这是一个示意图，实际结构取决于完整的插入顺序）_
    
2.  **经过优化的平衡树** 如果我们选择一个中间值（如“July”）作为根，然后递归地构建左右子树，我们会得到一棵非常平衡的树。
    
    *   **平均查找时间 = 3.1**
    
    graph TD July --> Feb July --> May Feb --> Aug Feb --> Jan May --> Mar May --> Oct Aug --> Apr Aug --> Dec Jan --> June Mar --> Nov Oct --> Sept
    
    _（这是一个平衡后的示意图）_
    

这个例子清晰地告诉我们：**树的结构至关重要**。我们不能奢望用户总能提供一个好的插入顺序。我们需要一种数据结构，它能在插入和删除后，**自动维持自身的平衡**。AVL树就是第一个被发明的自平衡二叉搜索树。

#### 1.2 AVL树的定义

AVL树由其发明者 G. M. **A**delson-**V**elskii 和 E. M. **L**andis 于1962年提出。它本质上是一棵二叉搜索树，但增加了一个额外的约束条件。

**【定义】高度平衡 (Height Balanced)**

一棵二叉树是高度平衡的，当且仅当：

1.  它是一棵空树。
2.  或者，它的左右两棵子树都是高度平衡的，并且**左右子树的高度差的绝对值不超过1**。

为了方便定义，我们规定：**空树的高度为 -1**。一个只包含根节点的树，其左右子树都是空树，高度为-1，所以该树的高度为 $\\max(-1, -1) + 1 = 0$。

**【定义】平衡因子 (Balance Factor, BF)**

对于树中的任意一个节点，我们定义其**平衡因子**为： `BF(node) = height(node->left) - height(node->right)`

根据AVL树的定义，对于一棵合法的AVL树，其**所有节点**的平衡因子 `BF` 只能是 **\-1, 0, 或 1**。

*   `BF = 1`: 左子树比右子树高1。
*   `BF = 0`: 左右子树等高。
*   `BF = -1`: 右子树比左子树高1。

如果任何一个节点的 `BF` 变成了 `2` 或 `-2`，那么这棵树就不再是AVL树，需要进行调整。

我们来看几个例子：

graph TD subgraph "树 1 (非法AVL)" A(4) --> B(3) A --> C(5) B --> D(2) D --> E(1) C --> F(6) F --> G(7) end subgraph "树 2 (合法AVL)" H(5) --> I(2) H --> J(8) I --> K(1) I --> L(4) L --> M(3) L --> N(7) end subgraph "树 3 (非法AVL)" O(7) --> P(2) O --> Q(8) P --> R(1) P --> S(4) S --> T(3) S --> U(5) end

*   **树 1**：节点 `4` 的左子树高度为2（根为`3`），右子树高度为1（根为`5`），`BF(4) = 2 - 1 = 1`。但是，节点 `3` 的左子树高度为1（根为`2`），右子树为空（高度-1），`BF(3) = 1 - (-1) = 2`。**非法**。
*   **树 2**：所有节点的平衡因子都在 `{-1, 0, 1}` 范围内。**合法**。
*   **树 3**：节点 `2` 的右子树（根为`4`）高度为1，左子树（根为`1`）高度为0，`BF(2) = 0 - 1 = -1`。节点 `7` 的左子树（根为`2`）高度为2，右子树（根为`8`）高度为0，`BF(7) = 2 - 0 = 2`。**非法**。

#### 1.3 AVL树的平衡修复：旋转操作

当我们在AVL树中进行插入或删除操作时，可能会破坏某些节点的平衡（即 `BF` 变为 `2` 或 `-2`）。这个被破坏平衡的节点中，离插入/删除位置最近的那个，我们称之为“失衡点”。我们的任务就是通过**旋转 (Rotation)** 操作来修复它。

旋转分为两大类：单旋转和双旋转。

##### 1.3.1 单旋转 (Single Rotation)

单旋转用于解决“一边倒”的失衡情况。

**情况1：右右 (RR) 失衡**

当一个节点的**右子树**的**右子树**中插入了新节点，导致失衡时，我们采用RR旋转（也叫左单旋）。

**【示例】** 依次插入 “Mar”, “May”, “Nov”。（字典序 Mar < May < Nov）

1.  插入 “Mar”：树只有根节点 `Mar`。
    
2.  插入 “May”：`May` > `Mar`，成为 `Mar` 的右孩子。此时 `BF(Mar) = -1`。
    
3.  插入 “Nov”：`Nov` > `May`，成为 `May` 的右孩子。
    
    *   此时 `height(May)` 为 1， `height(Mar的左子树)` 为 -1。
    *   `BF(Mar) = -1 - 1 = -2`。`Mar` 成为失衡点。
    *   失衡是由于在 `Mar` 的**右**孩子 (`May`) 的**右**子树 (`Nov`) 插入导致的，这就是 **RR** 失衡。

**操作：左单旋**

将 `Mar` 的右孩子 `May` “提”上来成为新的根，`Mar` “降”下去成为 `May` 的左孩子。`May` 原来的左子树（本例中为空）成为 `Mar` 的新右子树。

**图解 RR 旋转（通用情况）**

假设失衡点为 `A`，其 `BF(A) = -2`。它的右孩子是 `B`。

graph TD subgraph "旋转前 (RR失衡)" A -- BF=-2 --> B A --> AL B --> BL B --> BR end subgraph "旋转后 (平衡)" B -- BF=0 --> A B --> BR A -- BF=0 --> AL A --> BL end

*   **旋转前：** `A` 是根，`B` 是其右孩子。`AL` 是 `A` 的左子树，`BL` 和 `BR` 分别是 `B` 的左、右子树。插入发生在 `BR` 中，导致 `height(BR)` 增加，从而 `BF(A)` 变为-2。
*   **旋转后：** `B` 成为新的根。`A` 成为 `B` 的左孩子。`A` 的左子树仍然是 `AL`。`B` 的右子树仍然是 `BR`。关键在于 `B` 原来的左子树 `BL`，现在成为了 `A` 的新右子树。由于二叉搜索树的性质 `AL < A < BL < B < BR`，这个结构是正确的。旋转后，树恢复平衡。

**情况2：左左 (LL) 失衡**

这与RR失衡完全对称。当一个节点的**左子树**的**左子树**中插入了新节点，导致失衡时（`BF = 2`），我们采用LL旋转（也叫右单旋）。操作与左单旋相反。

##### 1.3.2 双旋转 (Double Rotation)

双旋转用于解决“之字形”的失衡情况。

**情况3：左右 (LR) 失衡**

当一个节点的**左子树**的**右子树**中插入了新节点，导致失衡时（`BF = 2`），我们需要进行LR旋转。

**【示例】** 假设我们有如下的树，现在要插入 “Jan”。

graph TD subgraph "插入前" May --> Aug May --> Nov Aug --> Apr Aug --> Mar end

插入 "Jan" (Jan < Mar)：

graph TD subgraph "插入 Jan 后 (LR失衡)" May("May(BF=2)") --> Aug("Aug(BF=-1)") May --> Nov("Nov(BF=0)") Aug --> Apr("Apr(BF=0)") Aug --> Mar("Mar(BF=1)") Mar --> Jan("Jan(BF=0)") end

*   我们重新算一下： `h(Jan)=0`，`h(Mar)` 变为 `max(-1, 0)+1=1`。`h(Apr)=0`。`h(Aug)` 变为 `max(h(Apr), h(Mar)) + 1 = max(0, 1) + 1 = 2`。`h(Nov)=0`。`h(May)` 变为 `max(h(Aug), h(Nov)) + 1 = max(2, 0) + 1 = 3`。
*   我们来看平衡因子：`BF(May) = h(Aug) - h(Nov) = 2 - 0 = 2`。失衡点是 `May`。
*   失衡是由于在 `May` 的**左**孩子 (`Aug`) 的**右**子树 (`Mar`) 插入导致的，这就是 **LR** 失衡。

**操作：LR旋转**

LR旋转实际上是两次单旋转的组合：

1.  **先对子节点进行一次RR旋转（左单旋）**：对 `Aug` 和 `Mar` 进行左单旋，`Mar` 上升，`Aug` 下降。
2.  **再对失衡点进行一次LL旋转（右单旋）**：对 `May` 和 `Mar` 进行右单旋，`Mar` 上升，`May` 下降。

**图解 LR 旋转（通用情况）**

假设失衡点为 `A` (`BF=2`)，其左孩子为 `B`，`B` 的右孩子为 `C`。

graph TD subgraph "旋转前 (LR失衡)" A -- BF=2 --> B B -- BF=-1 --> C A --> AR B --> BL C --> CL C --> CR end subgraph "第一步：对B左旋" A -- BF=2 --> C C --> B C --> CR B --> BL B --> CL A --> AR end subgraph "第二步：对A右旋 (最终结果)" C --> B C --> A B --> BL B --> CL A --> CR A --> AR end

*   **旋转前：** `A` 是失衡点，插入发生在 `C` 的子树 (`CL` 或 `CR`) 中。
*   **最终结果：** `C` 成为了新的根，`B` 和 `A` 成为了它的左右孩子。`B` 和 `A` 原来的子树 `BL`, `AR` 不变，而 `C` 的两个子树 `CL`, `CR` 重新分配给了 `B` 的新右子树和 `A` 的新左子树。二叉搜索树的性质 `BL < B < CL < C < CR < A < AR` 依然保持。

**情况4：右左 (RL) 失衡**

与LR失衡完全对称。当一个节点的**右子树**的**左子树**中插入了新节点，导致失衡时 (`BF = -2`)，我们采用RL旋转。它由一次右单旋和一次左单旋组成。

#### 1.4 AVL树的高度分析（重要数学推导）

我们已经知道AVL树可以自平衡，但它的高度到底是多少？它能真正保证 $O(log N)$ 吗？答案是肯定的。

让我们来做一个严谨的数学推导。

**问题**：一个高度为 $h$ 的AVL树，最少包含多少个节点？

我们用 $n\_h$ 来表示高度为 $h$ 的AVL树所需的最少节点数。

*   $n\_0 = 1$ (一个根节点)
*   $n\_1 = 2$ (一个根节点，一个孩子)
*   $n\_{-1} = 0$ (空树)

为了让节点数最少，根节点的左右子树的高度必须尽可能地不相等，即高度差为1。假设左子树高度为 $h-1$，右子树高度为 $h-2$（或者反过来）。

那么，这棵高度为 $h$ 的树的总节点数就是： `1 (根节点) + 左子树的最少节点数 + 右子树的最少节点数`

所以我们得到了递推关系式： $n\_h = 1 + n\_{h-1} + n\_{h-2}$

这个式子看起来是不是很眼熟？它和斐波那契数列非常相似。 斐波那契数列 $F\_i$ 定义为: $$F\_0 = 0, F\_1 = 1, F\_i = F\_{i-1} + F\_{i-2} (\\forall i > 1)$$ 数列为: 0, 1, 1, 2, 3, 5, 8, ...

我们来观察一下 $n\_h$ 和 $F\_i$ 的关系。 $n\_0 = 1$ $n\_1 = 1 + n\_0 + n\_{-1} = 1 + 1 + 0 = 2$ $n\_2 = 1 + n\_1 + n\_0 = 1 + 2 + 1 = 4$

让我们尝试将 $n\_h$ 表示为 $F\_i$ 的形式。 令 $n\_h = F\_{k} - 1$，代入递推式： $F\_k - 1 = 1 + (F\_{k-1} - 1) + (F\_{k-2} - 1)$ $F\_k - 1 = F\_{k-1} + F\_{k-2} - 1$ $F\_k = F\_{k-1} + F\_{k-2}$ 这完全符合斐波那契数列的定义！

现在我们来确定 $k$ 和 $h$ 的关系。 $n\_0 = 1 \\implies F\_k - 1 = 1 \\implies F\_k = 2$。在斐波那契数列中，$F\_3 = 2$。所以 $k=3$。 $n\_1 = 2 \\implies F\_k - 1 = 2 \\implies F\_k = 3$。在斐波那契数列中，$F\_4 = 3$。所以 $k=4$。 我们可以得出结论：$$k = h + 3$$

所以，高度为 $h$ 的AVL树的最少节点数是： $$n\_h = F\_{h+3} - 1$$

斐波那契数列有一个近似的通项公式： $F\_i \\approx \\frac{\\phi^i}{\\sqrt{5}}$, 其中 $\\phi = \\frac{1+\\sqrt{5}}{2} \\approx 1.618$ (黄金分割比)。

因此，$$N = n\_h \\approx \\frac{\\phi^{h+3}}{\\sqrt{5}} - 1$$ 我们可以看到，节点数 `N` 是以指数形式随高度 `h` 增长的。反过来，我们可以解出 `h`： $$\\phi^{h+3} \\approx N\\sqrt{5}$$ $$(h+3)\\log\\phi \\approx \\log(N\\sqrt{5})$$ $$h \\approx \\frac{\\log N}{\\log\\phi} + C$$

因为 $\\log\\phi$ 是一个常数，所以我们得到了最终的结论： $$h = O(\\log N)$$

**证明**：对于一个包含N个节点的AVL树，其高度 $h$ 不会超过 $1.44 log₂(N)$。这是一个非常严格和优秀的界限，确保了所有操作的效率。

#### 1.5 AVL树的实现 (C++伪代码与代码)

**节点结构**

```cpp
struct AvlNode {
    int data;
    AvlNode *left;
    AvlNode *right;
    int height; // 存储高度比存储BF更方便

    AvlNode(int val) : data(val), left(nullptr), right(nullptr), height(0) {}
};
```

**核心函数**

```cpp
// 获取节点高度（处理空节点）
int height(AvlNode* node) {
    return node == nullptr ? -1 : node->height;
}

// 右单旋 (LL)
AvlNode* rotateWithLeftChild(AvlNode* k2) {
    AvlNode* k1 = k2->left;
    k2->left = k1->right;
    k1->right = k2;
    // 更新高度
    k2->height = max(height(k2->left), height(k2->right)) + 1;
    k1->height = max(height(k1->left), k2->height) + 1;
    return k1; // 新的根
}

// 左单旋 (RR)
AvlNode* rotateWithRightChild(AvlNode* k1) {
    AvlNode* k2 = k1->right;
    k1->right = k2->left;
    k2->left = k1;
    // 更新高度
    k1->height = max(height(k1->left), height(k1->right)) + 1;
    k2->height = max(height(k2->right), k1->height) + 1;
    return k2; // 新的根
}

// 双旋转 (LR)
AvlNode* doubleWithLeftChild(AvlNode* k3) {
    k3->left = rotateWithRightChild(k3->left);
    return rotateWithLeftChild(k3);
}

// 双旋转 (RL)
AvlNode* doubleWithRightChild(AvlNode* k1) {
    k1->right = rotateWithLeftChild(k1->right);
    return rotateWithRightChild(k1);
}

// 插入函数（递归）
AvlNode* insert(AvlNode* root, int data) {
    if (root == nullptr) {
        return new AvlNode(data);
    }

    if (data < root->data) {
        root->left = insert(root->left, data);
    } else if (data > root->data) {
        root->right = insert(root->right, data);
    }
    // else: data already exists, do nothing

    // --- 平衡调整 ---
    int balance = height(root->left) - height(root->right);

    // LL Case
    if (balance > 1 && data < root->left->data) {
        return rotateWithLeftChild(root);
    }
    // RR Case
    if (balance < -1 && data > root->right->data) {
        return rotateWithRightChild(root);
    }
    // LR Case
    if (balance > 1 && data > root->left->data) {
        return doubleWithLeftChild(root);
    }
    // RL Case
    if (balance < -1 && data < root->right->data) {
        return doubleWithRightChild(root);
    }

    // 更新当前节点高度
    root->height = max(height(root->left), height(root->right)) + 1;

    return root;
}
```

* * *

### 第二部分：Splay树 (伸展树)

AVL树提供了一种完美的平衡保证：任何时候，树都处于近乎完美平衡的状态，所有操作都是严格的 $O(\\log N)$。但这种保证是有代价的：实现复杂，需要存储额外的高度或平衡因子信息，并且在每次插入/删除后都需要从下到上检查和旋转，这在某些情况下可能效率不高。

Splay树提出了一种完全不同的哲学：**我们不追求时刻的平衡，而是追求长期的、摊还的效率**。

#### 2.1 Splay树的核心思想

*   **目标 (Target):** 任何 M 次连续的操作（从空树开始），总时间最多为 $O(M \\log N)$。这意味着，**平均**每次操作的摊还时间是 $O(\\log N)$。
*   **核心思想 (Idea):** 每当一个节点被访问时（无论是查找、插入还是删除），就通过一系列特殊的旋转操作，将这个节点**移动到树的根部**。这个过程称为**伸展 (Splaying)**。

**直觉：** 这个想法基于一种常见的“局部性原理”：如果一个节点最近被访问过，那么它很可能在不久的将来再次被访问。把它移动到根部，下一次访问它就会非常快 ($O(1)$)。

更重要的是，Splay操作不仅仅是将目标节点移到根部，它还有一个奇妙的副作用：**它会改善从根到访问节点的整条路径上其他节点的深度，使树趋向于平衡**。它能有效地“打散”那些过长的、倾斜的路径。

#### 2.2 错误的尝试：朴素的旋转

我们可能会想，把一个节点移到根部很简单，只要不断地对它和它的父节点进行单旋转，直到它成为根就行了。

**但这行不通！**

让我们看一个例子。假设我们有一棵倾斜的树，节点为 `k1, k2, ..., kN`。如果我们依次访问 `k1, k2, k3...`

```
  k_N
  /
k_{N-1}
  ...
  /
 k_2
 /
k_1
```

*   访问 `k1`：不断旋转 `k1`，它成为根。树的形态变化不大。
*   访问 `k2`：不断旋转 `k2`，它成为根。树的形态又变回去了，只是`k1`和`k2`换了位置。
*   访问 `k3`...

如果我们按顺序访问 `1, 2, 3, ..., N`，每次查找的成本都是 `O(N)`。N次操作的总成本将是 `O(N²)`，平均每次操作的成本是 `O(N)`。这个朴素的方法完全失败了，甚至比普通的BST还差。

#### 2.3 正确的Splay旋转策略

Splay树的精髓在于其独特的旋转规则，它比AVL树的旋转更复杂一些。对于要伸展的节点 `X`，我们看它的父节点 `P` 和祖父节点 `G`。

**Case 1: Zig (单旋)** 如果 `P` 是树的根节点，那么我们只需要对 `X` 和 `P` 进行一次单旋转。

graph TD subgraph "Before" P --> X P --> B X --> A end subgraph "After" X --> A X --> P P --> B end

**Case 2: P 不是根节点** 这时候我们需要看 `X, P, G` 的相对位置。

*   **Zig-Zig (之之型):** 如果 `X` 和 `P` 同为左孩子（或同为右孩子），即 `G-P-X` 形成一条直线。 **操作：** 先旋转 `P` 和 `G`，再旋转 `X` 和 `P`。
    
    graph TD subgraph "Before (Zig-Zig)" G --> P G --> D P --> X P --> C X --> A X --> B end subgraph "After" X --> A X --> P P --> B P --> G G --> C G --> D end
    
*   **Zig-Zag (之字型):** 如果 `X` 是右孩子而 `P` 是左孩子（或反之），即 `G-P-X` 形成一个“之”字形。 **操作：** 先旋转 `X` 和 `P`，然后 `X` 上升，再旋转 `X` 和 `G`。这和AVL的双旋转很像。
    
    graph TD subgraph "Before (Zig-Zag)" G --> P G --> D P --> A P --> X X --> B X --> C end subgraph "After" X --> P X --> G P --> A P --> B G --> C G --> D end
    

**关键区别：** Splay树的 **Zig-Zig** 操作与朴素方法中连续两次单旋转的顺序是**不同**的。正是这个区别，使得Splay树能够有效地缩短路径深度，避免退化。

#### 2.4 Splay树操作示例

**【示例】** 插入 1, 2, 3, 4, 5, 6, 7，然后查找 1。

1.  **插入1-7:** 由于每次插入的数都是当前最大的，所以会形成一个向右倾斜的链表。
    
    ```
    1 -> 2 -> 3 -> 4 -> 5 -> 6 -> 7 (root)
    ```
    
2.  **查找 1:** 找到节点 1，开始对 1 进行 Splay 操作。
    
    *   **Step 1:** `X=1, P=2, G=3` (Zig-Zig)。先旋`2-3`，再旋`1-2`。`1`成为`3`的左孩子。
    *   **Step 2:** `X=1, P=3, G=4` (Zig-Zig)。先旋`3-4`，再旋`1-3`。`1`成为`4`的左孩子。
    *   **Step 3:** `X=1, P=4, G=5` (Zig-Zig)。
    *   ...
    *   **最后一步:** `X=1, P=7` (Zig)。旋`1-7`。
    
    经过一系列 Zig-Zig 和最后的 Zig 操作，节点 1 最终成为根，并且树的结构变得更加平衡。原本很深的节点（如 2, 3, 4）都被提到了离根更近的位置。这就是Splay的魔力。
    

#### 2.5 Splay树的删除

删除操作也利用Splay。

1.  **Step 1:** 查找要删除的节点 `X`。这会自动将 `X` 伸展到根部。
2.  **Step 2:** 移除 `X`。此时树分裂成两棵独立的子树：左子树 `T_L` 和右子树 `T_R`。
3.  **Step 3:** 在左子树 `T_L` 中查找最大的元素（`FindMax`）。这个操作会将 `T_L` 中的最大元素伸展到 `T_L` 的根部。
4.  **Step 4:** 由于新的 `T_L` 的根是最大元素，所以它没有右孩子。直接将 `T_R` 作为这个新根的右孩子即可。

#### 2.6 Splay树的实现 (C++伪代码与代码)

```cpp
struct SplayNode {
    int data;
    SplayNode *left;
    SplayNode *right;
    SplayNode *parent; // Splay树通常需要父指针来简化旋转

    SplayNode(int val) : data(val), left(nullptr), right(nullptr), parent(nullptr) {}
};

class SplayTree {
private:
    SplayNode* root;

    // 左旋和右旋 (这里需要处理父指针)
    void rotateLeft(SplayNode* x) { ... }
    void rotateRight(SplayNode* x) { ... }

    // 核心Splay操作
    void splay(SplayNode* x) {
        while (x->parent != nullptr) {
            SplayNode* p = x->parent;
            SplayNode* g = p->parent;

            if (g == nullptr) { // Zig
                if (x == p->left) rotateRight(p);
                else rotateLeft(p);
            } else if (x == p->left && p == g->left) { // Zig-Zig
                rotateRight(g);
                rotateRight(p);
            } else if (x == p->right && p == g->right) { // Zig-Zig
                rotateLeft(g);
                rotateLeft(p);
            } else { // Zig-Zag
                if (x == p->left) rotateRight(p);
                else rotateLeft(p);
                if (x == g->left) rotateRight(g);
                else rotateLeft(g);
            }
        }
        this->root = x;
    }

public:
    // 查找
    SplayNode* find(int data) {
        SplayNode* curr = root;
        // ... 普通BST查找 ...
        // 找到后 splay(found_node)
        // 没找到 splay(last_visited_node)
        return root; // Splay后根就是查找的节点
    }

    // 插入
    void insert(int data) {
        // ... 普通BST插入 ...
        // 插入新节点后 splay(new_node)
    }

    // 删除
    void remove(int data) {
        find(data); // Step 1: Splay X to root
        SplayNode* X = root;
        // Step 2 & 3 & 4
        // ... 合并左右子树 ...
    }
};
```

_（注意：这里只展示核心逻辑）_

* * *

### 第三部分：摊还分析 (Amortized Analysis)

Splay树的性能保证是“摊还 $O(log N)$”，而不是“最坏情况 $O(log N)$”。这意味着单次操作可能很慢（比如对一个深层节点的Splay操作，成本可能是 $O(N)$），但一系列操作的**总成本**是可控的。摊还分析就是用来证明这一点的数学工具。

**核心概念**

摊还分析不是平均情况分析，它不涉及概率。它是一种对**数据结构操作序列**的**最坏情况**性能分析。

*   **最坏情况界 (Worst-case bound):** 保证**每次**操作的成本上限。 (AVL树)
*   **摊还界 (Amortized bound):** 保证**一系列**操作的**平均**成本上限。 (Splay树)
*   **平均情况界 (Average-case bound):** 在假设输入有某种概率分布的情况下，操作的**期望**成本。

通常有 `worst-case bound ≥ amortized bound ≥ average-case bound`。

摊还分析有三种主要方法：

1.  **聚合分析 (Aggregate Analysis)**
2.  **记账方法 (Accounting Method)**
3.  **势能方法 (Potential Method)**

我们用一个经典的例子——“带MultiPop的栈”——来理解这三种方法。

**【示例】带MultiPop的栈** 我们有一个栈，支持三种操作：

*   `Push(x)`: 将元素x入栈。实际成本为1。
*   `Pop()`: 弹出一个元素。实际成本为1。
*   `MultiPop(k)`: 连续弹出k个元素。实际成本为 `min(k, size)`。

**问题：** $n$ 次操作的序列，最坏情况总时间是多少？ 一个朴素的分析是：`MultiPop` 最坏情况下成本是 $O(n)$。如果 $n$ 次操作都是 `MultiPop`，总时间是 $O(n²)$ 吗？ 不对。因为你不能连续进行 $n$ 次 $O(n)$ 的`MultiPop`。要弹出元素，必须先推进去。

#### 3.1 聚合分析

**思想：** 计算 $n$ 次操作序列的**总**最坏情况成本 $T(n)$，然后摊还成本就是 $T(n) / n$。

对于栈的例子，我们观察到：**一个元素最多只能被压入栈一次，也最多只能被弹出一次**（无论是通过 `Pop` 还是 `MultiPop`）。

*   每次 `Push` 的成本是1。
*   每个元素一生中对 `Pop` 或 `MultiPop` 的总成本贡献最多是1。

假设在 $n$ 次操作序列中，有 $k$ 次是 `Push`。那么总的 `Push` 成本是 $k$。 被压入栈的元素总数是 $k$。因此，所有 `Pop` 和 `MultiPop` 操作的总成本之和，不可能超过 $k$。 总成本 `T(n) ≤ k (Push成本) + k (Pop总成本) ≤ 2k`。 因为 $k ≤ n$，所以总成本 $T(n) = O(n)$。 那么，$n$ 次操作的摊还成本是 $T(n) / n = O(n) / n = O(1)$。

#### 3.2 记账方法

**思想：** 这是一个更精细化的方法。我们为每个操作设定一个**摊还成本 (amortized cost, ĉ)**。

*   如果 `ĉ` > `实际成本(c)`，我们将差额 `ĉ - c` 存为“信用 (credit)”，把它“存”在数据结构的某个部分上。
*   如果 `ĉ` < `实际成本(c)`，我们用之前存下的信用，来支付 `c - ĉ` 这部分“赤字”。

**规则：** 任何时候，总信用都必须**非负**。

对于栈的例子，我们设定摊还成本：

*   `Push`: `ĉ = 2` (实际成本 `c = 1`)。每次Push，我们支付1单位的实际成本，然后存下1单位的信用。这个信用可以看作是“附着”在刚入栈的元素上，作为它未来被弹出的“预付款”。
*   `Pop`: `ĉ = 0` (实际成本 `c = 1`)。当这个元素被`Pop`时，它的实际成本1由它自己携带的信用支付。
*   `MultiPop(k)`: `ĉ = 0` (实际成本 `c = k'`)。弹出的 `k'` 个元素，每个都用自己携带的1单位信用支付自己的弹出成本。

因为每个元素入栈时都预付了它出栈的费用，所以任何操作序列的总摊还成本 `Σĉ` 总是大于或等于总实际成本 `Σc`。 一个 `Push` 的摊还成本是2，`Pop`和`MultiPop`是0。所以 `n` 次操作的总摊还成本是 `O(n)`，因此总实际成本也是 `O(n)`。平均每次操作的摊还成本是 `O(1)`。

#### 3.3 势能方法 (最强大、最数学化的方法)

**思想：** 我们定义一个**势能函数 (Potential Function) $\\Phi$**，它将数据结构的一个状态 $D$ 映射到一个实数 $\\Phi(D)$。这个势能代表了我们“储存”的“预付款”，类似于记账方法中的总信用。

我们规定初始状态 $D\_0$ 的势能$\\Phi(D\_0)=0$，并且对于任何状态$D\_i$，$\\Phi(D\_i)\\geq 0$。

第 $i$ 次操作的**摊还成本 ($\\hat{c\_i}$)** 定义为： $$\\hat{c\_i} = c\_i + \\Phi(D\_i) - \\Phi(D\_{i-1})$$ 其中 $c\_i$是实际成本，$D\_{i-1}$ 和 $D\_i$ 分别是操作前后的数据结构状态。$\\Phi(D\_i)-\\Phi(D\_{i-1})$是势能的变化量。

对一个包含 $n$ 次操作的序列，总摊还成本为： $$\\sum\_{i=1}^{n} \\hat{c}\_i = \\sum\_{i=1}^{n} (c\_i + \\Phi(D\_i) - \\Phi(D\_{i-1}))$$ 这是一个**伸缩和 (telescoping sum)**： $$\\sum \\hat{c}\_i = (\\sum c\_i) + (\\Phi(D\_1) - \\Phi(D\_0)) + (\\Phi(D\_2) - \\Phi(D\_1)) + ... + (\\Phi(D\_n) - \\Phi(D\_{n-1}))$$ $$\\sum \\hat{c}\_i = (\\sum c\_i) + \\Phi(D\_n) - \\Phi(D\_0)$$

因为我们要求 $\\Phi(D\_n)\\geq \\Phi(D\_0)$（通常是 $\\Phi(D\_n)\\geq 0$且 $\\Phi(D\_0)=0$），所以我们有： $$\\sum \\hat{c}\_i \\ge \\sum c\_i$$ 总摊还成本是总实际成本的一个上界。如果我们能证明每次操作的摊还成本 `ĉᵢ` 很小，就能证明总实际成本也很小。

**应用于栈的例子：**

*   **势能函数定义：** $\\Phi(D)=\\text{栈中元素的数量}$。
    *   初始空栈 $D\_0$，$\\Phi(D\_0)=0$。
    *   任何时候 $\\Phi(D\_i)\\geq 0$。满足条件。
*   **分析 Push:**
    *   $c\_i=1$。
    *   栈中元素数量加1，$\\Phi(D\_i)-\\Phi(D\_{i-1})=1$。
    *   $\\hat{c\_i}=c\_i+(\\Phi(D\_i)-\\Phi(D\_{i-1}))=1+1=2$。
*   **分析 Pop:**
    *   $c\_i=1$。
    *   栈中元素数量减1，$\\Phi(D\_i)-\\Phi(D\_{i-1})=-1$。
    *   $\\hat{c\_i}=c\_i+(\\Phi(D\_i)-\\Phi(D\_{i-1}))=1+(-1)=0$。
*   **分析 MultiPop(k):**
    *   实际成本 $c\_i=k'$ (弹出了 $k'$ 个)。
    *   栈中元素数量减 $k'$，$\\Phi(D\_i)-\\Phi(D\_{i-1})=-k'$。
    *   $\\hat{c\_i}=c\_i + (\\Phi(D\_i)-\\Phi(D\_{i-1}))=k'+(-k')=0$。

每次操作的摊还成本都是 $O(1)$。因此 $n$ 次操作的总摊还成本是 $O(n)$，总实际成本也是 $O(n)$。

#### 3.4 Splay树的势能分析

现在，我们用强大的势能方法来证明Splay树的摊还复杂度是 $O(log N)$。

*   **定义：**
    
    *   对于树中的任意节点 $i$，定义 $S(i)$ 为以 $i$ 为根的子树的大小（包括 $i$ 自己）。
    *   定义节点 $i$ 的**秩 (rank)** 为 $R(i) = \\log\_2(S(i))$。
    *   **势能函数定义：** 整棵树 `T` 的势能是所有节点的秩之和。 $$\\Phi(T) = \\sum\_{i \\in T} R(i) = \\sum\_{i \\in T} \\log\_2(S(i))$$
*   **分析：** 我们需要计算一次Splay操作中每个小步骤（Zig, Zig-Zig, Zig-Zag）的摊还成本。假设我们要伸展的节点是 `X`。一次Splay操作由 $d$ 次小步骤组成，总实际成本是 $d$ 次旋转的成本，约为 $O(d)$。
    
    `Amortized Cost = Actual Cost + ΔΦ`
    
    经过非常严谨和巧妙的数学推导（这里省略具体细节，但它基于 $\\log(a)+\\log(b)\\leq 2\\log (a+b)-2$的不等式），我们可以得到每个小步骤的摊还成本$\\hat{c\_i}$ 的上界：
    
    *   **Zig 步:** $\\hat{c\_i}\\leq 1 + 3\\left(R\_{\\text{after}}(X)-R\_{\\text{before}}(X)\\right)$
    *   **Zig-Zig 步:** $\\hat{c\_i}\\leq 3\\left(R\_{\\text{after}}(X)-R\_{\\text{before}}(X)\\right)$
    *   **Zig-Zag 步:** $\\hat{c\_i}\\leq 2\\left(R\_{\\text{after}}(X)-R\_{\\text{before}}(X)\\right)$
    
    这里 $R\_{\\text{before}}(X)$ 和 $R\_{\\text{after}}(X)$ 分别是这个小步骤执行前后 `X` 的秩。注意，在每个小步骤中，`X` 都在向上移动，所以它的子树大小 $S(X)$ 在增加，因此它的秩 $R(X)$ 也在增加。$R\_{\\text{after}}(X)-R\_{\\text{before}}(X)$ 是一个正数。
    
*   **求和：** 我们将一次完整的Splay操作（包含 $k$ 个小步骤）的摊还成本加起来。 `Total Amortized Cost = (k 次旋转的实际成本) + (总的势能变化)`
    
    幸运的是，当我们把所有步骤的摊还成本加起来时，中间项会相互抵消（伸缩和）： $$\\sum\_{i=1}^{k} \\hat{c}\_i \\le O(k) + 3 \\sum (R\_{i}(X) - R\_{i-1}(X))$$ $$\\sum \\hat{c}\_i \\le O(k) + 3(R\_{final}(X) - R\_{initial}(X))$$
    
    其中$R\_{\\text{initial}}(X)$ 是Splay操作开始时 `X` 的秩，$R\_{\\text{final}}(X)$ 是Splay结束后 `X` 的秩。
    
    *   Splay结束后，`X` 成为根节点，所以 $S\_{\\text{final}}(X)=N$（树的总节点数）。因此 $R\_{\\text{final}}(X)=\\log N$。
    *   $S\_{\\text{initial}}\\geq 1$，所以 $R\_{\\text{initial}}\\geq \\log 1 = 0$。
    
    因此，一次完整的Splay操作的摊还成本为： $$\\text{Amortized Cost (Splay)}\\leq O(k) + 3 (\\log N - R\_{\\text{initial}}(X))=O(\\log N)$$ _（$O(k)$项实际上被更精确的分析吸收了，最终结果就是$O(\\log N)$）_
    

**结论：** 一次Splay操作的摊还成本是 $O(\\log N)$。由于Splay树的所有操作（查找、插入、删除）都基于Splay，所以这些操作的摊还成本也都是 $O(\\log N)$。这就严格证明了Splay树的性能保证。

* * *

### 第四部分：总结与比较

现在，我们来回答那个终极问题：**Splay树真的比AVL树更好吗？**

答案是：**看情况 (It depends)。**

特性

普通BST

AVL树

Splay树

**单次操作最坏时间**

$O(N)$

$O(\\log N)$

$O(N)$

**M次操作总最坏时间**

$O(M\*N)$

$O(M \\log N)$

$O(M \\log N)$

**单次操作摊还时间**

$O(N)$

$O(\\log N)$

$O(\\log N)$

**空间复杂度**

$O(1)$ (额外)

$O(N)$ (存高度)

$O(N)$ (需父指针)

**实现复杂度**

简单

复杂

非常复杂

**局部性优势**

无

无

**强**

**选择建议：**

*   如果你需要**严格的、可预测的**单次操作性能保证，比如在实时系统中，**选择AVL树**（或红黑树）。
*   如果你的应用中存在明显的**访问局部性**（即某些元素被频繁访问），或者你对平均性能要求很高，而可以容忍偶尔的慢操作，**Splay树是绝佳的选择**。它的自优化特性会使常用数据自动靠近根部，长期运行下来性能会非常好。
*   如果数据是静态的，只构建一次，之后只有查询操作，那么一个静态的、完美平衡的BST是最好的。
*   在大多数通用场景下，标准库（如C++的`std::map`）通常使用**红黑树**，它是AVL树和Splay树之间的一种折衷：它不像AVL树那样严格平衡，但能保证$O(\\log N)$的最坏情况性能，同时旋转和调整的次数比AVL树少，实现也相对（只是相对）简单一些。