---
title: "数值分析13:龙格-库塔法与多步法"
description: ""
pubDate: "2025-08-27"
heroImage: ""
---

# 数值分析13:龙格-库塔法与多步法

### **引言：追求“完美”的ODE求解器**

在上一讲中，我们开启了求解常微分方程初值问题的大门，并学习了最基础的**欧拉方法**和**高阶泰勒方法**。我们的探索之路留下了一个关键性的挑战。

**“能否找到一种方法，它兼具所有已知方法的优点，而没有它们的缺点？”**

让我们回顾一下这些优缺点：

*   **显式欧拉法：** 简单，但精度仅为一阶。
*   **高阶泰勒法：** 精度高 ($O(h^n)$)，但需要手动计算并编程实现 $f(t,y)$ 的复杂高阶导数，这在实践中几乎是不可行的。
*   **隐式欧拉法：** 具有更好的稳定性（我们将在后续课程深入探讨），但每一步都需要求解一个（可能非线性的）方程，计算成本高。

这个“圣杯”式的追求，引导我们去寻找一种方法，它应该具备：

1.  **高阶精度：** 像泰勒方法一样，局部截断误差是 $O(h^p)$ 且 $p \\ge 2$。
2.  **无需导数：** 像欧拉方法一样，算法的实现应该只依赖于函数 $f(t,y)$ 本身的求值。
3.  **单步、显式：** 易于编程实现和启动。

这个看似不可能的任务，被两位杰出的数学家**龙格 (Runge)** 和**库塔 (Kutta)** 解决了。他们发明的**龙格-库塔方法**，通过在每个时间步内进行多次“内插”式的函数求值，巧妙地用这些值的线性组合来模拟高阶泰勒展开的效果，从而在不计算导数的情况下实现了高阶精度。

今天，我们将首先深入学习**龙格-库塔方法**，特别是最经典的四阶方法。然后，我们将探讨另一大类求解ODE的思路——**多步法 (Multistep Methods)**。与只利用上一步信息的单步法不同，多步法会利用**过去多步**的历史信息来预测下一步，这使得它们在达到同等精度时可能需要更少的函数求值。最后，我们将看到如何将显式的多步法（作为**预测**）和隐式的多步法（作为**校正**）结合起来，形成高效稳定的**预测-校正**算法。

### **第一部分：龙格-库塔 (Runge-Kutta) 方法**

#### **5.4.1 核心思想：用函数求值模拟泰勒展开**

"A single-step method with high-order local truncation error without evaluating the derivatives of f."

**基本思路：** 欧拉法 $w\_{i+1} = w\_i + h \\cdot f(t\_i, w\_i)$ 使用的是起始点的斜率 $f(t\_i, w\_i)$ 来进行一步外推。这个斜率在整个区间 $\[t\_i, t\_{i+1}\]$ 上可能并不是一个好的代表。 **"We can improve the result by finding a better slope."**

龙格-库塔法的核心就是去寻找一个**加权的平均斜率 $\\phi(t\_i, w\_i, h)$**，使得单步格式 $w\_{i+1} = w\_i + h \\cdot \\phi$ 在与 $y(t)$ 的泰勒展开式进行比较时，能够匹配到尽可能高的 $h$ 的幂次。

#### **5.4.2 二阶龙格-库塔方法 (RK2) 的推导**

让我们尝试构造一个二阶方法。我们寻求一个形式如下的平均斜率： $$\\phi = \\lambda\_1 K\_1 + \\lambda\_2 K\_2$$ 其中 $K\_1$ 和 $K\_2$ 是在区间内不同点计算的斜率。 迭代格式为： $$w\_{i+1} = w\_i + h(\\lambda\_1 K\_1 + \\lambda\_2 K\_2)$$

*   **$K\_1$ (起始点斜率):** 最自然的选择是在起始点 $(t\_i, w\_i)$ 计算斜率。 $K\_1 = f(t\_i, w\_i)$
*   **$K\_2$ (试探点斜率):** 我们向前“试探”一小步，在一个新的点 $(t\_i+ph, w\_i+qh K\_1)$ 计算斜率。为了简化，通常取 $p=q$。 $$K\_2 = f(t\_i+ph, w\_i+phK\_1)$$

现在我们有了一个包含三个待定参数 $\\lambda\_1, \\lambda\_2, p$ 的通用二阶方法族。

**Step 1-3: 泰勒展开与系数匹配** 我们的目标是选择这三个参数，使得 $w\_{i+1}$ 的表达式与 $y(t\_{i+1})$ 的二阶泰勒展开式尽可能匹配。

*   **真解的二阶泰勒展开:** $$y(t\_{i+1}) = y(t\_i) + h y'(t\_i) + \\frac{h^2}{2} y''(t\_i) + O(h^3)$$ 将 $y'=f$ 和 $y'' = f\_t + f\_y f$ 代入，得到： $$y(t\_{i+1}) = y\_i + h f(t\_i, y\_i) + \\frac{h^2}{2}(f\_t + f\_y f)(t\_i, y\_i) + O(h^3)$$
    
*   **数值解的展开:**
    
    1.  对 $K\_2 = f(t\_i+ph, y\_i+phK\_1)$ 进行二元泰勒展开： $$K\_2 = f(t\_i,y\_i) + ph \\frac{\\partial f}{\\partial t} + phK\_1 \\frac{\\partial f}{\\partial y} + O(h^2)$$ 代入 $K\_1=f(t\_i, y\_i)$，得到： $$K\_2 = f + ph(f\_t + f\_y f) + O(h^2) = y'(t\_i) + ph y''(t\_i) + O(h^2)$$
    2.  将 $K\_1, K\_2$ 的展开式代入 $w\_{i+1} = y\_i + h(\\lambda\_1 K\_1 + \\lambda\_2 K\_2)$： $$w\_{i+1} = y\_i + h(\\lambda\_1 f + \\lambda\_2\[f + ph(f\_t+f\_y f)\]) + O(h^3)$$ $$w\_{i+1} = y\_i + (\\lambda\_1+\\lambda\_2)h f + \\lambda\_2 p h^2 (f\_t+f\_y f) + O(h^3)$$
*   **系数匹配:** 比较 $y(t\_{i+1})$ 和 $w\_{i+1}$ 表达式中 $h$ 和 $h^2$ 的系数：
    
    *   $h$ 的系数: $\\lambda\_1 + \\lambda\_2 = 1$
    *   $h^2$ 的系数: $\\lambda\_2 p = 1/2$

我们得到了 **2 个方程**，但有 **3 个未知数**！ "Here are 3 unknowns and 2 equations."这意味着存在**无穷多组解**，形成一个二阶龙格-库塔方法的家族。

**RK2 家族的两个著名成员：**

1.  **中点法 (Midpoint Method):**
    
    *   选择 $\\lambda\_2 = 1$, 则 $\\lambda\_1 = 0, p=1/2$。
    *   $w\_{i+1} = w\_i + h \\cdot f(t\_i+\\frac{h}{2}, w\_i+\\frac{h}{2}K\_1)$
    *   几何意义：用区间**中点**的斜率来更新整步。
2.  **修正欧拉法 (Modified Euler's Method) 或 Heun 法:**
    
    *   选择 $\\lambda\_2=1/2$, 则 $\\lambda\_1=1/2, p=1$。
    *   $K\_1 = f(t\_i, w\_i)$
    *   $K\_2 = f(t\_i+h, w\_i+hK\_1)$
    *   $w\_{i+1} = w\_i + \\frac{h}{2}(K\_1 + K\_2)$
    *   几何意义：用**起始点斜率**和**用欧拉法预测的终点斜率**的**算术平均值**来更新整步。

#### **5.4.3 经典的四阶龙格-库塔方法 (RK4)**

"How to obtain higher-ordered accuracy?" 通过引入更多的“级”(stages)，即更多的中间斜率 $K\_i$，并匹配更高阶的泰勒展开，我们可以得到更高阶的RK方法。这个过程极其繁琐，被称为“屠夫表理论 (Butcher Tableau)”。

最著名、最流行、应用最广泛的是**经典的四阶龙ge-库塔方法 (RK4)**。

**RK4 格式：** $$w\_{i+1} = w\_i + \\frac{h}{6}(K\_1 + 2K\_2 + 2K\_3 + K\_4)$$ 其中： $K\_1 = f(t\_i, w\_i)$ (起始点斜率) $K\_2 = f(t\_i + \\frac{h}{2}, w\_i + \\frac{h}{2}K\_1)$ (用 $K\_1$ 预测的**中点**斜率) $K\_3 = f(t\_i + \\frac{h}{2}, w\_i + \\frac{h}{2}K\_2)$ (用 $K\_2$ 预测的**中点**斜率) $K\_4 = f(t\_i + h, w\_i + hK\_3)$ (用 $K\_3$ 预测的**终点**斜率)

**分析：**

*   **局部截断误差**为 $O(h^4)$，因此全局误差也是 $O(h^4)$。这是一个**四阶**方法，精度非常高。
*   加权平均斜率 $\\phi = \\frac{1}{6}(K\_1+2K\_2+2K\_3+K\_4)$ 的形式与数值积分中的**辛普森法则**惊人地相似！这并非巧合，RK方法可以从求积公式的角度来理解。
*   **计算成本：** 每一步需要**4次**函数 $f$ 的求值。这是它的主要计算开销。

RK4在精度、稳定性和实现复杂度之间达到了绝佳的平衡，是科学与工程计算中的“主力军”。

### **第二部分：多步法 (Multistep Methods)**

**动机：** RK4每步需要4次函数求值来实现四阶精度。有没有可能更“经济”一些？ **多步法的核心思想：** 单步法是“健忘”的，它只利用 $(t\_i, w\_i)$ 来计算 $w\_{i+1}$。而多步法会利用**过去的一系列点** $(t\_i, w\_i), (t\_{i-1}, w\_{i-1}), \\dots$ 的信息来计算 $w\_{i+1}$。如果这些历史信息可以被高效地复用，我们或许能用**少于4次**的函数求值（理想情况是1次）就达到四阶精度。

#### **5.6.1 基于积分的推导**

所有单步和多步法都可以从积分形式 $y(t\_{i+1}) = y(t\_i) + \\int\_{t\_i}^{t\_{i+1}} f(t, y(t)) dt$ 推导出来。 **"The key is to approximate the integral."**

**核心策略：** 用一个多项式 $P(t)$ 来**插值**函数 $f(t,y(t))$ 在过去一系列点 $(t\_j, f\_j)$ 上的值（其中 $f\_j = f(t\_j, w\_j)$），然后对这个插值多项式积分来近似 $\\int\_{t\_i}^{t\_{i+1}} f dt$。

#### **5.6.2 Adams-Bashforth (AB) 显式方法**

**思想：** 要计算 $w\_{i+1}$，我们用一个 $m-1$ 次的多项式来插值**过去 $m$ 个点**的斜率信息： $$(t\_i, f\_i), (t\_{i-1}, f\_{i-1}), \\dots, (t\_{i+1-m}, f\_{i+1-m})$$ 然后将这个多项式从 $t\_i$ 积分到 $t\_{i+1}$。

**推导 (AB二步法为例):**

1.  用 $(t\_i, f\_i)$ 和 $(t\_{i-1}, f\_{i-1})$ 构造一个线性插值多项式 $P\_1(t)$。使用牛顿后向差分形式更方便。
2.  积分 $\\int\_{t\_i}^{t\_{i+1}} P\_1(t) dt$。
3.  得到迭代格式： $w\_{i+1} = w\_i + \\frac{h}{2}\[3f(t\_i, w\_i) - f(t\_{i-1}, w\_{i-1})\]$ 这是一个二步方法，局部截断误差是 $O(h^2)$。

**通用 Adams-Bashforth m-步法：** $w\_{i+1} = w\_i + h \\sum\_{j=0}^{m-1} b\_j \\nabla^j f\_i$ (基于牛顿后向差分公式积分) 例如，**四阶 Adams-Bashforth (AB4) 方法：** $$w\_{i+1} = w\_i + \\frac{h}{24}\[55f\_i - 59f\_{i-1} + 37f\_{i-2} - 9f\_{i-3}\]$$

*   **优点：**
    *   这是一个**四阶**方法（LTE为 $O(h^4)$）。
    *   它是**显式**的。一旦启动，每一步**只需要1次**新的函数求值 $f\_i=f(t\_i, w\_i)$，因为过去的值 $f\_{i-1}, \\dots$ 都是已知的！这比RK4的4次求值高效得多。
*   **缺点：**
    *   **不是自启动的：** 为了计算 $w\_4$，我们需要 $w\_0, w\_1, w\_2, w\_3$。而 $w\_1, w\_2, w\_3$ 必须由其他方法（通常是同阶的RK4）来“预热”生成。
    *   **稳定性较差：** 相比于同阶的RK方法，AB方法的稳定性区间更小。
    *   **改变步长困难：** 由于公式依赖于等距的历史点，改变步长 $h$ 会变得非常麻烦。

#### **5.6.3 Adams-Moulton (AM) 隐式方法**

**思想：** AB方法只利用了过去的信息，是一种“外插”。如果我们把**未来的、未知的点 $(t\_{i+1}, f\_{i+1})$** 也包含进来进行插值，就能得到更精确的积分近似，这是一种“内插”。 我们用一个 $m-1$ 次多项式来插值点： $$(t\_{i+1}, f\_{i+1}), (t\_i, f\_i), \\dots, (t\_{i+2-m}, f\_{i+2-m})$$

**三阶 Adams-Moulton (AM3) 方法：** $$w\_{i+1} = w\_i + \\frac{h}{24}\[9f\_{i+1} + 19f\_i - 5f\_{i-1} + f\_{i-2}\]$$

*   **优点：**
    *   这是一个**四阶**方法。
    *   **精度更高：** 在同阶的情况下，AM方法的截断误差常数比AB方法小得多 (例如，四阶AM的误差常数约为AB的 $1/14$）。
    *   **稳定性更好：** 稳定性区间远大于同阶的AB方法。
*   **缺点：**
    *   **隐式：** 未知量 $w\_{i+1}$ 出现在 $f\_{i+1} = f(t\_{i+1}, w\_{i+1})$ 中，每一步都需要迭代求解。

#### **5.6.4 预测-校正 (Predictor-Corrector) 系统**

我们如何结合AB方法的**高效**和AM方法的**高精度与稳定性**，同时又避免求解非线性方程？ **Adams 预测-校正系统：** 这是一个绝妙的组合，构成了许多现代ODE求解器的核心。

**以四阶 Adams 系统为例 (PECE 模式):** 假设我们已经有了 $w\_i, f\_i, f\_{i-1}, f\_{i-2}$。

1.  **P (Predict):** 使用**显式**的 Adams-Bashforth (AB4) 方法，计算一个对 $w\_{i+1}$ 的初步**预测值** $w\_{i+1}^\*$。 $$w\_{i+1}^\* = w\_i + \\frac{h}{24}\[55f\_i - 59f\_{i-1} + 37f\_{i-2} - 9f\_{i-3}\]$$
    
2.  **E (Evaluate):** 使用这个预测值，计算一个对未来斜率的近似值 $f\_{i+1}^\*$。 $$f\_{i+1}^\* = f(t\_{i+1}, w\_{i+1}^\*)$$
    
3.  **C (Correct):** 使用**隐式**的 Adams-Moulton (AM3) 方法的公式，但用 $f\_{i+1}^\*$ 代替右端真正的 $f\_{i+1}$，来对预测值进行**校正**，得到最终的 $w\_{i+1}$。 $$w\_{i+1} = w\_i + \\frac{h}{24}\[9f\_{i+1}^\* + 19f\_i - 5f\_{i-1} + f\_{i-2}\]$$
    
4.  **E (Evaluate, optional):** 可以计算最终的 $f\_{i+1}=f(t\_{i+1}, w\_{i+1})$ 为下一步做准备。
    

**优点：**

*   **高阶精度：** 整个方法的局部截断误差是四阶的，与AM方法在同一水平。
*   **计算高效：** 每一步只需要**2次**函数求值（一次在E步，一次在C步之后），比RK4的4次求值更经济。
*   **全显式计算：** 巧妙地规避了求解隐式方程的难题。
*   **误差估计：** 预测值 $w\_{i+1}^\*$ 和校正值 $w\_{i+1}$ 之间的差异 $|w\_{i+1} - w\_{i+1}^\*|$ 可以用来**实时估计**该步的局部截断误差，这为实现**自适应步长控制**提供了基础。

### **课程总结**

**本讲核心要点：**

1.  **龙格-库塔 (RK) 方法**通过在每个步长内进行多次函数求值和加权平均，可以在不计算导数的情况下实现高阶精度。经典的**RK4**是应用最广泛的单步法。
2.  **多步法**利用历史信息来提高效率。**Adams-Bashforth (AB)** 是显式的，每步函数求值次数少但稳定性较差；**Adams-Moulton (AM)** 是隐式的，精度和稳定性更优但需要求解方程。
3.  **预测-校正 (PECE)** 是一种黄金组合，它使用AB法做预测，AM法做校正，兼具了高阶精度、高效率和全显式计算的优点，是现代ODE求解器中的主流技术。
4.  **单步法 vs 多步法：** RK方法是自启动的，易于改变步长，适合求解精度要求高、函数行为复杂的通用问题。多步法在启动和变步长上较为复杂，但在函数行为平滑、可以采用较固定步长的情况下，其函数求值效率更高。